{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# What is Spark and Why Should I Care?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SWBAT:\n",
    "\n",
    "- describe the basic structure of Spark distributed data systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "N.B. Unless otherwise marked, page references are to [Salloum, Dautov, et al., \"Big Data Analytics on Apache Spark\", 2016](https://link.springer.com/content/pdf/10.1007%2Fs41060-016-0027-9.pdf).\n",
    "\n",
    "Spark is a tool for the management of big data. Sometimes data science professionals will refer to the [five \"V\"s](https://blog.unbelievable-machine.com/en/what-is-big-data-definition-five-vs) of big data. Clearly, the availabilty and size of datasets are growing rapidly. What counts as \"big data\"? Roughly speaking, we're talking about datasets that are too large to be processed on a single machine.\n",
    "\n",
    "Many large companies are relying on big data these days, and Spark is a major player in the big data game. Examples can be found [here](https://www.icas.com/thought-leadership/technology/10-companies-using-big-data) and [here](https://enlyft.com/tech/products/apache-spark) and [here](https://www.quora.com/Which-are-the-companies-that-use-apache-spark).\n",
    "\n",
    "So ... how in the world *do* you process a dataset that's too large for a single machine? You use multiple machines linked together! Let's call each machine a *node*, and the group of all machines working in parallel a *cluster*.\n",
    "\n",
    "The origin story of Spark starts with [MapReduce](https://en.wikipedia.org/wiki/MapReduce), whose programs comprise (unsurprisingly) a \"map\" routine (for filtering and sorting) and a \"reduce\" routine (for performing some aggregate operation).\n",
    "\n",
    "Let's look at an [example](https://en.wikipedia.org/wiki/MapReduce#Logical_view):\n",
    "\n",
    "An early major player in big data that used MapReduce was [Hadoop](https://en.wikipedia.org/wiki/Apache_Hadoop). Hadoop was (and still is) a framework for distributed data processing. Its processing component used MapReduce, but it also had a storage component called the \"Hadoop Distributed File System\".\n",
    "\n",
    "From Wikipedia: \"Hadoop splits files into large blocks and distributes them across nodes in a cluster. It then transfers packaged code into nodes to process the data in parallel. This approach takes advantage of data locality, where nodes manipulate the data they have access to. This allows the dataset to be processed faster and more efficiently than it would be in a more conventional supercomputer architecture that relies on a parallel file system where computation and data are distributed via high-speed networking\".\n",
    "\n",
    "But Spark appeared as open source in 2010, and had some advantages over Hadoop MapReduce.\n",
    "\n",
    "Spark's advances over Hadoop MapReduce:\n",
    "\n",
    "- data processing in memory rather than on disks\n",
    "- a single framework for machine learning, graph analysis, and processing of streaming data (pp. 159-160)\n",
    "\n",
    "For more on the advantages of Spark over MapReduce, see [this piece](https://research.ijcaonline.org/volume113/number1/pxc3900531.pdf).\n",
    "\n",
    "Distributed computing can help enormously with speed. Check out [this website](http://sortbenchmark.org) for the latest in speed records.\n",
    "\n",
    "\"As a framework, it combines a core engine for distributed computing with an advanced programming model for in-memory processing. Although it has the same linear scalability and fault tolerance capabilities as those of MapReduce, it comes with a multistage in-memory programming model comparing to the rigid map-then-reduce disk-based model\" (146).\n",
    "\n",
    "Illustration, p. 148, of Spark guts.\n",
    "\n",
    "\"Running a Spark application involves five key entities ... a driver program, a cluster manager, workers, executors and tasks. A driver program is an application that uses Spark as a library and defines a high-level control flow of the target computation. While a worker provides CPU, memory and storage resources to a Spark application, an executer \\[sic\\] is a JVM (Java Virtual Machine) process that Spark creates on each worker for that application. A job is a set of computations (e.g., a data processing algorithm) that Spark performs on a cluster to get results to the driver program. A Spark application can launch multiple jobs. Spark splits a job into a directed acyclic graph (DAG) of stages where each stage is a collection of tasks. A task is the smallest unit of work that Spark sends to an executor. The main entry point for Spark functionalities is a SparkContext through which the driver program access \\[sic\\] Spark. A SparkContext represents a connection to a computing cluster\" (149).\n",
    "\n",
    "RDDs, Transformations, and Actions:\n",
    "\n",
    "Fault tolerance achieved by keeping a record of the RDD's lineage. There are *redundancies* in the data records, so that, in the event of node failure, the other nodes can provide for data recovery. This is what makes these RDDs *resilient*.\n",
    "\n",
    "- Transformations take one from an RDD to another RDD;\n",
    "- Actions take one from an RDD to an output value.\n",
    "\n",
    "Broadcast variables and accumulators act as global variables; the latter are for counters or sums.\n",
    "\n",
    "Surveys of Big Data tools [here](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-015-0032-1) and [here](https://ieeexplore.ieee.org/document/7300948).\n",
    "\n",
    "Debugging can be a challenge in Spark. [This project](https://sites.google.com/site/sparkbigdebug/) was started to help with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also check out Paco Nathan's [massive slide show presentation](http://stanford.edu/~rezab/sparkclass/slides/itas_workshop.pdf) on Spark. Let's just look at slides 66-7 and 82."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark context and concepts review\n",
    "![sparkler](https://images.pexels.com/photos/285173/pexels-photo-285173.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](https://towardsdatascience.com/a-neanderthals-guide-to-apache-spark-in-python-9ef1f156d427) is a great walkthrough of Spark basics!\n",
    "\n",
    "And [here](https://towardsdatascience.com/apache-spark-a-conceptual-orientation-e326f8c57a64)'s another from our very own alum, Alex Shropshire!\n",
    "\n",
    "Spark has APIs for Scala (this is ur-Spark), Java, Python, and R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The story of Spark (a timeline)\n",
    "\n",
    "|<p align=\"left justify\">Date</p>|<p align=\"left justify\">Product</p>|<p align=\"left justify\">Update</p>|\n",
    "|:----|:-----|:-----|\n",
    "| 2002 | Hadoop | <p align=\"left justify\">Doug Cutting starts `Apache Nutch` researching sort/merge processing</p> |\n",
    "| 2006 | Hadoop |  <p align=\"left justify\">Leaves `Nutch` and joins `Yahoo`, renaming the project `Hadoop` </p>|\n",
    "| 2008 | Hadoop |  <p align=\"left justify\">`Hadoop` was made `Apache’s` top level project </p> |\n",
    "| Jan 2008 | Hadoop |  <p align=\"left justify\">v 0.10.1 released </p>|\n",
    "| 2009 | Spark | <p align=\"left justify\">started as a research project at the UC Berkeley AMPLab  </p>|\n",
    "| 2010 | Spark |  <p align=\"left justify\">open sourced </p>|\n",
    "| Sept 2012 | Spark |  <p align=\"left justify\">0.6.0 released </p>|\n",
    "| 2013 | Spark |  <p align=\"left justify\">moved to the `Apache` Software Foundation </p>|\n",
    "| Feb 2013| Spark |  <p align=\"left justify\">Spark 0.7 adds a Python API called `PySpark` </p>|\n",
    "| Sept 2013 | Spark | <p align=\"left justify\">0.8.0 introduces `MLlib` </p>|\n",
    "| 2013 | Databricks |  <p align=\"left justify\">Original Spark research team at UC Berkeley found Databricks</p> |\n",
    "| May 2014 |Spark |  <p align=\"left justify\">v 1.0 introduces Spark SQL, for loading and manipulating structured data in Spark</p>|\n",
    "| Sept 2014 | Spark|  <p align=\"left justify\">v 1.1.0 provided support for registering Python lambda funtions as UDFs</p>|\n",
    "|Mar 2015 | Spark | <p align=\"left justify\"> v 1.3.0 brings a new DataFrame API</p> |\n",
    "| Jun 2015 | Spark | <p align=\"left justify\"> v 1.4.0 brings an R API to Spark</p> |\n",
    "| 2015 | Databricks | <p align=\"left justify\"> The Databricks Apache Spark cloud platform goes public</p> |\n",
    "| Jan 2016|  Spark | <p align=\"left justify\"> v 1.6.0 brings a new Dataset API <br> - A new Spark API, similar to RDDs, that allows users to work with custom objects and lambda functions while still gaining the benefits of the Spark SQL execution engine.</p> |\n",
    "| Jul 2016 | Spark | <p align=\"left justify\"> v 2.0.0 **big update**! <Br> - Unifying DataFrame and Dataset: In Scala and Java, DataFrame and Dataset have been unified, i.e. DataFrame is just a type alias for Dataset of Row. In Python and R, given the lack of type safety, DataFrame is the main programming interface. <br> - SparkSession: new entry point that replaces the old SQLContext<br>- Native CSV data source, based on Databricks’ spark-csv module<br>- MLlib - The DataFrame-based API is now the primary API. The RDD-based API is entering maintenance mode </p> |\n",
    "| 2016 | Databricks | <p align=\"left justify\"> Databricks Launches Free Community Edition As Companion To Free Online Spark Courses </p>|\n",
    "| Jul 2017| Spark | <p align=\"left justify\"> v 2.2.0 drops support for Python 2.6 |\n",
    "| Nov 2018 | Spark | <p align=\"left justify\"> v 2.4.0<br> - This release adds Barrier Execution Mode for better integration with deep learning frameworks<br> - more integration between pandas UDF and spark DataFrames </p>|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark data objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In Pyspark there are only RDD and DataFrames\n",
    "\n",
    "In other languages where \"compiling\" is done, there is the distinction between DataFrames and DataSet. \n",
    "\n",
    "![dataframe image](https://databricks.com/wp-content/uploads/2018/05/DataFrames.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Use an RDD when:\n",
    "[quoted from databricks](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\n",
    "\n",
    "> - you want low-level transformation and actions and control on your dataset;\n",
    "> - your data is unstructured, such as media streams or streams of text;\n",
    "> - you want to manipulate your data with functional programming constructs than domain specific expressions;\n",
    "> - you don’t care about imposing a schema, such as columnar format, while processing or accessing data attributes by name or column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Use a dataframe when:\n",
    "[also quoted from databricks](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\n",
    "\n",
    "\n",
    "> - you want rich semantics, high-level abstractions, and domain specific APIs, use DataFrame\n",
    "> - your processing demands high-level expressions, filters, maps, aggregation, averages, sum, SQL queries, columnar access and use of lambda functions on semi-structured data, use DataFrame\n",
    "> - you want higher degree of type-safety at compile time, want typed JVM objects, take advantage of Catalyst optimization, and benefit from Tungsten’s efficient code generation, use Dataset.\n",
    "> - you want unification and simplification of APIs across Spark Libraries, use DataFrame or Dataset.\n",
    "> - If you are a R user, use DataFrames.\n",
    "> - If you are a Python user, use DataFrames and resort back to RDDs if you need more control.\n",
    "\n",
    "**Note**: Machine learning algorithms are run on _DataFrames_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
