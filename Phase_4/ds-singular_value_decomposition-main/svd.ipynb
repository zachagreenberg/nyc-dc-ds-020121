{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On the Meaning and Use of Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SWBAT:\n",
    "\n",
    "- explain the notion of singular value decomposition;\n",
    "- describe the relationship between SVD and eigendecomposition;\n",
    "- describe the relationship between SVD and PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigendecomposition\n",
    "\n",
    "Let's start with eigendecomposition. Remember PCA?\n",
    "\n",
    "Any *non-defective* and *square* matrix $A$ has an eigendecomposition:\n",
    "\n",
    "Non-defective you have as many equations as are unknowns\n",
    "\n",
    "$\\large A = Q\\Lambda Q^{-1}$,\n",
    "\n",
    "where:\n",
    "\n",
    "- the columns of $Q$ are the eigenvectors of $A$, and\n",
    "- $\\Lambda$ is a diagonal matrix whose non-zero entries are the eigenvalues of $A$.\n",
    "-The upsidedown V is capital lambda\n",
    "\n",
    "Q is eigenvector (row) * Lamba (diagonal matrix) * Q-1 is flipped with eigenvect(column)\n",
    "\n",
    "Eigendecompositions have *many* practical applications.\n",
    "\n",
    "But since not all matrices are square, not all matrices have eigendecompositions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "If you multiply a matrix of by it's transpose, you are squaring it!\n",
    "\n",
    "However, given a non-square matrix $R$, we can construct a square matrix by simply calculating $RR^T$ or $R^TR$.\n",
    "\n",
    "(The 'T' superscript indicates that the relevant matrix is *transposed*, i.e. its rows and columns are switched. So for example the transpose of $\\begin{bmatrix} 0 & 1 & 2 \\\\ 3 & 4 & 5 \\\\ 6 & 7 & 8 \\end{bmatrix}$ is $\\begin{bmatrix} 0 & 3 & 6 \\\\ 1 & 4 & 7 \\\\ 2 & 5 & 8 \\end{bmatrix}$.)\n",
    "\n",
    "Moreover, _any_ matrix $A$ has a ***singular value decomposition***, i.e. a factorization in the form:\n",
    "\n",
    "$\\large A = U\\Sigma V^T$,\n",
    "\n",
    "where\n",
    "\n",
    "- $\\Sigma$ is diagonal if square and otherwise \"pseudo-diagonal\" (a diagonal matrix sitting on top of zeroes) with real, non-negative entries; and\n",
    "- $U$ and $V$ are orthogonal.\n",
    "\n",
    "A matrix $Q$ is orthogonal if its columns are mutually orthogonal and normalized to lengths of 1. This guarantees that, for orthogonal $Q$, $Q^TQ = I$. Thus, $Q^T = Q^T(QQ^{-1}) = (Q^TQ)Q^{-1}$, so $Q^T = Q^{-1}$.\n",
    "\n",
    "Note also that, if $V$ is orthogonal, then so is $V^{-1}$:\n",
    "\n",
    "$(V^{-1})^T = (V^T)^T$ <br/>\n",
    "$(V^{-1})^T = V$ <br/>\n",
    "$(V^{-1})^T = (V^{-1})^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now (this text adapted from [Inderjit Dhillon]( http://www.cs.utexas.edu/users/inderjit/courses/dm2009/LinearAlgebraBackground.pdf)):\n",
    "\n",
    "Using the singular value decomposition of $A$, we have:\n",
    "\n",
    "$\\large AA^T = U\\Sigma V^T\\times(U\\Sigma V^T)^T$ <br/>\n",
    "$\\large AA^T = U\\Sigma V^T\\times V\\Sigma^TU^T$ <br/>\n",
    "$\\large AA^T = U\\Sigma I\\Sigma U^T$ <br/>\n",
    "$\\large AA^T = U\\Sigma^2U^T$ <br/>\n",
    "Here we have an eigendecomposition of $AA^T$ in terms of the SVD of $A$!\n",
    "\n",
    "In particular:\n",
    "\n",
    "the eigenvectors of $AA^T$ are the columns of $U$; and <br/>\n",
    "the eigenvalues of $AA^T$ are the squares of the singular values of $A$. <br/>\n",
    "\n",
    "Similarly, $\\large A^TA = V\\Sigma^2V^T$.\n",
    "\n",
    "And so:\n",
    "\n",
    "the eigenvectors of $A^TA$ are the columns of $V$; and <br/>\n",
    "the eigenvalues of $A^TA$ are the squares of the singular values of $A$.<br/>\n",
    "\n",
    "Put another way: The singular values of A are the non-negative square roots of the eigenvalues of $AA^T$ or $A^TA$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again (see [this page](https://math.stackexchange.com/questions/2152751/why-does-the-eigenvalues-of-aat-are-the-squares-of-the-singular-values-of-a)):\n",
    "\n",
    "Since: <br/> $\\large AA^T = U\\Sigma^2U^T$, <br/> we have that <br/> $\\large AA^TU = U\\Sigma^2$ (since $U$ is orthogonal), <br/> which says that $AA^T$ multiplied by a vector (let's choose $U_i$, a column of $U$) yields $U$ multiplied by a scalar, namely $\\sigma^2_i$. I.e. the squares of the singular values of $A$ are the (non-zero) eigenvalues of $A^TA$ (or $AA^T$).\n",
    "\n",
    "### Eigenvalues of $AA^T$ and $A^TA$\n",
    "Why do $AA^T$ and $A^TA$ have the same eigenvalues?\n",
    "\n",
    "Let $\\lambda$ be a (non-zero) eigenvalue of $A^TA$.\n",
    "\n",
    "Then:\n",
    "\n",
    "$A^TAx = \\lambda x$.\n",
    "\n",
    "Now let $Ax = y$. So we have:\n",
    "\n",
    "$A^Ty = \\lambda x$. If we left-multiply by $A$, we have:\n",
    "\n",
    "$AA^Ty = A\\lambda x = \\lambda Ax = \\lambda y$, which is to say that $\\lambda$ is an eigenvalue of $AA^T$.\n",
    "\n",
    "(To show the reverse, just let $B = A^T$.)\n",
    "\n",
    "For more, see [this post](https://math.stackexchange.com/questions/1087064/non-zero-eigenvalues-of-aat-and-ata)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagonalization vs. SVD\n",
    "\n",
    "This shows that the eigen- and the singular value decompositions are intimately related!\n",
    "\n",
    "The SVD has many uses! Check out [this book chapter](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD and Dimensionality Reduction\n",
    "\n",
    "The SVD, much like PCA, can be used to reduce the dimensionality of your data.\n",
    "\n",
    "Recall how PCA works: We start with an eigendecomposition of our covariance matrix. We then order our eigenvectors by the size of their corresponding eigenvalues. Eigenvectors with large eigenvalues explain more of the variance in our dataset, and so we define our principal components accordingly.\n",
    "\n",
    "The situation is much the same with the SVD. The singular vectors that correspond to larger singular values capture more of the variance in our data. And, just as with PCA, we can often capture a large percentage of the variance by taking a relatively small number of singular vectors, throwing out the ones that correspond to small singular values. [Here](https://rpubs.com/Tanmay007/svd) is a good example of this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD in Python\n",
    "\n",
    "Let's show that the squares of the singular values of a non-square matrix $A$ are equal to the eigenvalues of $A^TA$ (or $AA^T$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37454012, 0.95071431, 0.73199394],\n",
       "       [0.59865848, 0.15601864, 0.15599452],\n",
       "       [0.05808361, 0.86617615, 0.60111501],\n",
       "       [0.70807258, 0.02058449, 0.96990985],\n",
       "       [0.83244264, 0.21233911, 0.18182497]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "A = np.random.rand(5, 3)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.5991048 , -0.38620771, -0.12988737, -0.68883081, -0.02363108],\n",
       "        [-0.25170251,  0.32375656, -0.38389036,  0.13776803, -0.81576694],\n",
       "        [-0.4495347 , -0.55516825,  0.01152904,  0.69900869,  0.03099514],\n",
       "        [-0.51180949,  0.4814656 ,  0.71001691,  0.04057048,  0.0217244 ],\n",
       "        [-0.33717783,  0.45387706, -0.57576083,  0.12756552,  0.57665694]]),\n",
       " array([1.99063285, 1.0096001 , 0.57767497]),\n",
       " array([[-0.52458829, -0.54271957, -0.65594405],\n",
       "        [ 0.72866708, -0.6846751 , -0.01625695],\n",
       "        [-0.44028559, -0.48649304,  0.75463443]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using np.linalg.svd()\n",
    "\n",
    "np.linalg.svd(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.linalg.svd()` returns a triple, unsurprisingly: The left singular vectors (\"U\"), the singular values, and the (transpose of the) right singular vectors (\"V\").\n",
    "\n",
    "We can re-create A by multiplying these components together. But we'll first have to turn the array of singular values into $\\Sigma$, so we'll use `np.diag()` together with `np.vstack()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vT = np.linalg.svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = np.vstack([np.diag(s), [[0, 0, 0], [0, 0, 0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.99063285, 0.        , 0.        ],\n",
       "       [0.        , 1.0096001 , 0.        ],\n",
       "       [0.        , 0.        , 0.57767497],\n",
       "       [0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37454012, 0.95071431, 0.73199394],\n",
       "       [0.59865848, 0.15601864, 0.15599452],\n",
       "       [0.05808361, 0.86617615, 0.60111501],\n",
       "       [0.70807258, 0.02058449, 0.96990985],\n",
       "       [0.83244264, 0.21233911, 0.18182497]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This should reproduce the matrix A\n",
    "\n",
    "u.dot(sigma).dot(vT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the eigendecomposition of $AA^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.96261914e+00,  1.01929237e+00,  3.33708373e-01, -3.85173654e-17,\n",
       "         6.67933316e-17]),\n",
       " array([[-0.5991048 , -0.38620771,  0.12988737, -0.56579353,  0.31590997],\n",
       "        [-0.25170251,  0.32375656,  0.38389036, -0.37821855, -0.77909222],\n",
       "        [-0.4495347 , -0.55516825, -0.01152904,  0.57835455, -0.31445698],\n",
       "        [-0.51180949,  0.4814656 , -0.71001691,  0.0455007 , -0.00086544],\n",
       "        [-0.33717783,  0.45387706,  0.57576083,  0.44750879,  0.44083133]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eig(A.dot(A.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.96261914e+00,  1.01929237e+00,  3.33708373e-01, -3.85173654e-17,\n",
       "        6.67933316e-17])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eig(A.dot(A.T))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last two eigenvalues are really equal to *zero*, which of course can often confuse computers. Squaring the singular values of $A$ should yield the same list (without the zeroes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.96261914, 1.01929237, 0.33370837])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.svd(A)[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">PCA take covariance and find the eigenvectors. Basically multiplying by it's transpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation to PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by **centering** the matrix, i.e. subtracting the mean of the relevant column from each entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "centered_A = np.vstack([A[:, col] - A.mean(axis=0)[col] for col in range(3)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.13981937,  0.50954777,  0.20382628],\n",
       "       [ 0.084299  , -0.2851479 , -0.37217314],\n",
       "       [-0.45627587,  0.42500961,  0.07294735],\n",
       "       [ 0.19371309, -0.42058205,  0.44174219],\n",
       "       [ 0.31808315, -0.22882743, -0.34634269]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centered_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column sums should now be 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.33066907e-16, 5.55111512e-16, 3.33066907e-16])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centered_A.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix of a centered matrix $A$ is equal to $A^TA/(n-1)$, where $n$ is the number of rows of $A$. See [this helpful post](https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(np.cov(centered_A.T), centered_A.T.dot(centered_A) / 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD of centered matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_c, s_c, vT_c = np.linalg.svd(centered_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA begins by diagonalizing the covariance matrix. And with these matrices generated by the SVD we can reproduce the covariance matrix of $A_{centered}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09338628, -0.11086559, -0.02943783],\n",
       "       [-0.11086559,  0.18770817,  0.0336127 ],\n",
       "       [-0.02943783,  0.0336127 ,  0.12511719]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vT_c.T.dot(np.diag(s_c)**2).dot(vT_c) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09338628, -0.11086559, -0.02943783],\n",
       "       [-0.11086559,  0.18770817,  0.0336127 ],\n",
       "       [-0.02943783,  0.0336127 ,  0.12511719]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(centered_A.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Least-Squares Problem\n",
    "\n",
    "The singular value decomposition can be used to solve a least-squares problem quickly. Let's create such a problem.\n",
    "\n",
    "### Comparison to the matrix-vector equation, $M\\vec{x} = \\vec{b}$\n",
    "\n",
    "Suppose we have an exact equation, $M\\vec{x} = \\vec{b}$.\n",
    "\n",
    "In that case $M$ is square, and the solution to the equation is $x = M^{-1}b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(43)\n",
    "M = np.random.rand(5, 5)\n",
    "b = np.random.rand(5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66972465],\n",
       "       [0.08250005],\n",
       "       [0.89709858],\n",
       "       [0.2980035 ],\n",
       "       [0.26230482]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linalg.inv(M).dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66972465],\n",
       "       [0.08250005],\n",
       "       [0.89709858],\n",
       "       [0.2980035 ],\n",
       "       [0.26230482]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reproducing the vector b\n",
    "\n",
    "M.dot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Problem\n",
    "\n",
    "But of course the typical DS situation is that we have not an exact equation to solve but rather an optimization to perform. So let's now imagine that $A$ has more rows than columns.\n",
    "\n",
    "If we need some warm and fuzzy familiarity, we could throw this all into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred1</th>\n",
       "      <th>pred2</th>\n",
       "      <th>pred3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.374540</td>\n",
       "      <td>0.950714</td>\n",
       "      <td>0.731994</td>\n",
       "      <td>0.669725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.598658</td>\n",
       "      <td>0.156019</td>\n",
       "      <td>0.155995</td>\n",
       "      <td>0.082500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.058084</td>\n",
       "      <td>0.866176</td>\n",
       "      <td>0.601115</td>\n",
       "      <td>0.897099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.708073</td>\n",
       "      <td>0.020584</td>\n",
       "      <td>0.969910</td>\n",
       "      <td>0.298004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.832443</td>\n",
       "      <td>0.212339</td>\n",
       "      <td>0.181825</td>\n",
       "      <td>0.262305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pred1     pred2     pred3    target\n",
       "0  0.374540  0.950714  0.731994  0.669725\n",
       "1  0.598658  0.156019  0.155995  0.082500\n",
       "2  0.058084  0.866176  0.601115  0.897099\n",
       "3  0.708073  0.020584  0.969910  0.298004\n",
       "4  0.832443  0.212339  0.181825  0.262305"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.hstack([A, b]),\n",
    "             columns=['pred1', 'pred2', 'pred3', 'target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treating the columns of $A$ as our predictors and $b$ as our target, the answer to this least-squares problem turns out to be $A^+\\vec{b}$, where $A^+$ is the *pseudo-inverse* of $A$. The formula for the pseudo-inverse if $(A^TA)^{-1}A^T$, and the idea behind it is to generalize the notion of an inverse to non-square matrices. The pseudo-inverse reduces to the inverse in the case of square matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = np.random.rand(100, 100)\n",
    "np.allclose(np.linalg.inv(mat), np.linalg.pinv(mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have $A = U\\Sigma V^T$, then $A^+ = V\\Sigma^+U^T$. Numpy has a pseudo-inverse function, `np.linalg.pinv()`, which we could use directly, rather than first constructing the SVD. But because the decomposed equation involves only the pseudo-inverse of a (pseudo-) diagonal matrix, the SVD route can be much *faster*. **This is the real point of using the SVD in calculating least-squares solutions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [this site](https://math.stackexchange.com/questions/974193/why-does-svd-provide-the-least-squares-and-least-norm-solution-to-a-x-b) for a proof of the least-squares solution. For more on the pseudo-inverse, see [here](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01999533],\n",
       "       [ 0.64792212],\n",
       "       [ 0.29648717]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's calculate the least-squares solution using our SVD components:\n",
    "\n",
    "vT.T.dot(np.linalg.pinv(sigma)).dot(u.T).dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01999533,  0.64792212,  0.29648717]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking against sklearn's LinearRegression():\n",
    "\n",
    "LinearRegression(fit_intercept=False).fit(A, b).coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, `LinearRegression()` uses SVD under the hood!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.3 µs ± 1.36 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit vT.T.dot(np.linalg.pinv(sigma)).dot(u.T).dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237 µs ± 9.37 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit LinearRegression(fit_intercept=False).fit(A, b).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 µs ± 3.13 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.linalg.pinv(A).dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our small sample matrix, `sklearn`'s version actually takes longer. But let's try a much larger matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.rand(10000, 100)\n",
    "target = np.random.rand(10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.4 ms ± 1.36 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.linalg.pinv(X).dot(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.1 ms ± 905 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit LinearRegression(fit_intercept=False).fit(X, target).coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going through the SVD (as `sklearn`'s model-fitting procedure does) makes noticeable savings in time here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "- The [MovieLens](https://grouplens.org/datasets/movielens/) datset(s).\n",
    "- [This article](https://analyticsindiamag.com/singular-value-decomposition-svd-application-recommender-system/) that illustrates the SVD in a recommender system.\n",
    "- The [`surprise`](https://surprise.readthedocs.io/en/stable/) library, which you'll see in the Learn labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
