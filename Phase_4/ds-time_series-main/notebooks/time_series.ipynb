{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "import os, sys\n",
    "path_add = os.path.abspath(os.pardir)\n",
    "if path_add not in sys.path:\n",
    "    sys.path.append(path_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "SWBAT:\n",
    "\n",
    "- Manipulate datetime objects;\n",
    "- Use rolling and weighted averages to inspect time series;\n",
    "- Conduct Dickey-Fuller Tests for stationarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "The study of time series has arisen because certain sorts of data streams are heavily dependent on the flow of time. Of course, we have not totally ignored time as a feature up to this point. The selling price of a house probably *does* have some relation to the season or the year as real estate markets grow and decline with certain temporally-indexed economic changes etc. But surely time is not the most important predictor of house price. Square footage would likely be more strongly correlated with price than would date of sale.\n",
    "\n",
    "But there are other sorts of data that more readily lend themselves to a temporal analysis. One canonical example is numbers from a stock exchange: First, data from stock tickers often arrive as numbers anchored to consecutive units of time. I get the selling price for some stock on January 1, say, and the next bit of information I gain will be the selling price for that stock on January 2. (We'll explore this feature of time series below.) Second, and more important, if I'm interested in actually *predicting* the selling price of a stock for, say, tomorrow, then very likely one piece of very salient (i.e. *correlated*) information would be the selling price of that stock *today*.\n",
    "\n",
    "Let's look at some time series data plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will help us load and\n",
    "# clean up a dataset.\n",
    "\n",
    "def load_trend(trend_name='football', country_code='us'):\n",
    "    df = pd.read_csv('../data/google-trends_'\n",
    "                     + trend_name + '_'\n",
    "                     + country_code\n",
    "                     + '.csv').iloc[1:, :]\n",
    "    df.columns = ['counts']\n",
    "    df['counts'] = df['counts'].str.replace('<1', '0').astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_trend(**{'trend_name': 'data-science', 'country_code': 'us'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends = [\n",
    "    {'trend_name': 'data-science', 'country_code': 'us'},\n",
    "    {'trend_name': 'football', 'country_code': 'us'},\n",
    "    {'trend_name': 'football', 'country_code': 'uk'},\n",
    "    {'trend_name': 'coronavirus', 'country_code': 'us'},\n",
    "    {'trend_name': 'trump', 'country_code': 'us'},\n",
    "    {'trend_name': 'taxes', 'country_code': 'us'},\n",
    "    {'trend_name': 'avengers', 'country_code': 'us'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_dfs = [load_trend(**trend) for trend in trends]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if we can guess which is which just by looking\n",
    "# at their graphs.\n",
    "\n",
    "fig, axs = plt.subplots(len(trend_dfs), 1, figsize=(8, 10))\n",
    "plt.tight_layout()\n",
    "for i, trend_df in enumerate(trend_dfs):\n",
    "    ax = axs[i]\n",
    "    #ax.set_title(str(trends[i]))\n",
    "    ax.plot(np.array(trend_df.index), trend_df['counts'])\n",
    "    ticks = ax.get_xticks()\n",
    "    ax.set_ylim((0, 100))\n",
    "    ax.set_xticks([tick for tick in ticks if tick%24 == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series as Both Predictor and Target?\n",
    "\n",
    "Often, the phenomenon we want to capture with a time series is a dataset being correlated with *itself*.\n",
    "\n",
    "Well, of course every dataset is perfectly correlated with itself. But what we're after now is the idea that a series is correlated with *earlier versions* of itself.\n",
    "\n",
    "Consider the problem of trying to predict tomorrow's closing price for some stock on the market. One may consider lots of features, like what sort of company it is to which the stock belongs or whether that company has been in the news recently.\n",
    "\n",
    "But it is very often the case that one of the most helpful predictors of tomorrow's price is *today's* price. And so we want to build a model where one of our predictors is an earlier version of our target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "Let's import some data on **gun violence in Chicago**.\n",
    "\n",
    "[source](https://data.cityofchicago.org/Public-Safety/Gun-Crimes-Heat-Map/iinq-m3rg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.read_csv('../data/Gun_Crimes_Heat_Map.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some summary stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {ts.shape[0]} records in our timeseries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitely some messy input of our Desciption data\n",
    "ts['Description'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = ts['Description'].value_counts()[:10]\n",
    "offense_names = ts['Description'].value_counts()[:10].index\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.barplot(height, offense_names, color='r', ax=ax)\n",
    "ax.set_title('Mostly Handgun offenses');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostly non-domestic offenses\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.barplot( ts['Domestic'].value_counts().index, \n",
    "             ts['Domestic'].value_counts(),  \n",
    "             palette=[ 'r', 'b'], ax=ax\n",
    "           )\n",
    "\n",
    "ax.set_title(\"Overwhelmingly Non-Domestic Offenses\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostly non-domestic offenses\n",
    "\n",
    "arrest_rate = ts['Arrest'].value_counts()[1]/len(ts)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.barplot( ts['Arrest'].value_counts().index, \n",
    "             ts['Arrest'].value_counts(), \n",
    "             palette=['r', 'g'], ax=ax\n",
    "           )\n",
    "\n",
    "ax.set_title(f'{arrest_rate: 0.2%} of Total Cases\\n Result in Arrest');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.barplot( ts['Year'].value_counts().index, \n",
    "             ts['Year'].value_counts(),  \n",
    "             color= 'r', ax=ax\n",
    "           )\n",
    "\n",
    "ax.set_title(\"Offenses By Year\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this does show some interesting information that will be relevant to our time series analysis, we are going to get more granular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datetime Objects\n",
    "\n",
    "Datetime objects make our time series modeling lives easier.  They will allow us to perform essential data prep tasks with a few lines of code.  \n",
    "\n",
    "We need our timeseries **index** to be datetime objects, since our models will rely on being able to identify the previous chronological value.\n",
    "\n",
    "There is a `datetime` [library](https://docs.python.org/2/library/datetime.html), and inside `pandas` there is a datetime module as well as a to_datetime() function.\n",
    "\n",
    "For time series modeling, the first step often is to make sure that the index is a datetime object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few ways to **reindex** our series to datetime. \n",
    "\n",
    "We can use `pandas.to_datetime()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.set_index(pd.to_datetime(ts['Date']), drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can parse the dates directly on import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.read_csv('../data/Gun_Crimes_Heat_Map.csv', index_col='Date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Now our index is a {type(ts.index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datetime objects include aspects of the date as attributes, like month and year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.index[0].month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.index[0].year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily see now whether offenses happen, for example, during business hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ts['hour'] = ts.index\n",
    "ts['hour'] = ts.hour.apply(lambda x: x.hour)\n",
    "ts['business_hours'] = ts.hour.apply(lambda x: 9 <= x <= 17)\n",
    "\n",
    "bh_ratio = ts.business_hours.value_counts()[1]/len(ts)\n",
    "\n",
    "x = ts.business_hours.value_counts().index\n",
    "y = ts.business_hours.value_counts()\n",
    "sns.barplot(x=x, y=y)\n",
    "\n",
    "ax.set_title(f'{bh_ratio: 0.2%} of Offenses\\n Happen Btwn 9 and 5');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling and Downsampling\n",
    "\n",
    "With a Datetime index, we also have new abilities, such as **resampling**.\n",
    "\n",
    "To create our timeseries, we will count the number of gun offenses reported per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.resample('D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many possible units for resampling, each with its own alias:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"display: inline-block\">\n",
    "    <caption style=\"text-align: center\"><strong>TIME SERIES OFFSET ALIASES</strong></caption>\n",
    "<tr><th>ALIAS</th><th>DESCRIPTION</th></tr>\n",
    "<tr><td>B</td><td>business day frequency</td></tr>\n",
    "<tr><td>C</td><td>custom business day frequency (experimental)</td></tr>\n",
    "<tr><td>D</td><td>calendar day frequency</td></tr>\n",
    "<tr><td>W</td><td>weekly frequency</td></tr>\n",
    "<tr><td>M</td><td>month end frequency</td></tr>\n",
    "<tr><td>SM</td><td>semi-month end frequency (15th and end of month)</td></tr>\n",
    "<tr><td>BM</td><td>business month end frequency</td></tr>\n",
    "<tr><td>CBM</td><td>custom business month end frequency</td></tr>\n",
    "<tr><td>MS</td><td>month start frequency</td></tr>\n",
    "<tr><td>SMS</td><td>semi-month start frequency (1st and 15th)</td></tr>\n",
    "<tr><td>BMS</td><td>business month start frequency</td></tr>\n",
    "<tr><td>CBMS</td><td>custom business month start frequency</td></tr>\n",
    "<tr><td>Q</td><td>quarter end frequency</td></tr>\n",
    "<tr><td></td><td><font color=white>intentionally left blank</font></td></tr></table>\n",
    "\n",
    "<table style=\"display: inline-block; margin-left: 40px\">\n",
    "<caption style=\"text-align: center\"></caption>\n",
    "<tr><th>ALIAS</th><th>DESCRIPTION</th></tr>\n",
    "<tr><td>BQ</td><td>business quarter endfrequency</td></tr>\n",
    "<tr><td>QS</td><td>quarter start frequency</td></tr>\n",
    "<tr><td>BQS</td><td>business quarter start frequency</td></tr>\n",
    "<tr><td>A</td><td>year end frequency</td></tr>\n",
    "<tr><td>BA</td><td>business year end frequency</td></tr>\n",
    "<tr><td>AS</td><td>year start frequency</td></tr>\n",
    "<tr><td>BAS</td><td>business year start frequency</td></tr>\n",
    "<tr><td>BH</td><td>business hour frequency</td></tr>\n",
    "<tr><td>H</td><td>hourly frequency</td></tr>\n",
    "<tr><td>T, min</td><td>minutely frequency</td></tr>\n",
    "<tr><td>S</td><td>secondly frequency</td></tr>\n",
    "<tr><td>L, ms</td><td>milliseconds</td></tr>\n",
    "<tr><td>U, us</td><td>microseconds</td></tr>\n",
    "<tr><td>N</td><td>nanoseconds</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When resampling, we have to provide a rule to resample by, and an **aggregate function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To upsample** is to increase the frequency of the data of interest.  \n",
    "**To downsample** is to decrease the frequency of the data of interest.\n",
    "\n",
    "For our purposes, we will downsample, and  count the number of occurences per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.resample('D').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our time series will consist of a series of counts of gun reports per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID is unimportant. We could choose any column, since the counts are the same.\n",
    "ts = ts.resample('D').count()['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our timeseries with a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(ts.index, ts.values)\n",
    "ax.set_title('Gun Crimes per day in Chicago')\n",
    "ax.set_ylabel('Reported Gun Crimes');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be some abnormal activity happening towards the end of our series.\n",
    "\n",
    "**[sun-times](https://chicago.suntimes.com/crime/2020/6/8/21281998/chicago-deadliest-day-violence-murder-history-police-crime)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's treat the span of days from 5-31 to 6-03 as outliers. \n",
    "\n",
    "There are several ways to do this, but let's first remove the outliers, and populate an an empty array with the original date range. That will introduce us to the `pandas.date_range()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_count = ts[ts < 90]\n",
    "ts_dr = pd.date_range(daily_count.index[0], daily_count.index[-1])\n",
    "ts_daily = np.empty(shape=len(ts_dr))\n",
    "ts_daily = pd.Series(ts_daily)\n",
    "ts_daily = ts_daily.reindex(ts_dr)\n",
    "ts = ts_daily.fillna(daily_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ts.plot(ax=ax)\n",
    "ax.set_title('Gun Crimes in Chicago with Deadliest Days Removed');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's zoom in on that week again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(ts[(ts.index > '2020-05-20') \n",
    "                 & (ts.index < '2020-06-07')]\n",
    "       )\n",
    "ax.tick_params(rotation=45)\n",
    "ax.set_title('We have some gaps now');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datetime object allows us several options of how to fill those gaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .ffill()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))\n",
    "ax1.plot(ts.ffill()[(ts.index > '2020-05-20') \n",
    "                 & (ts.index < '2020-06-07')]\n",
    "       )\n",
    "ax1.tick_params(rotation=45)\n",
    "ax1.set_title('Forward Fill')\n",
    "\n",
    "ax2.plot(ts[(ts.index > '2020-05-20') \n",
    "                 & (ts.index < '2020-06-07')]\n",
    "       )\n",
    "ax2.tick_params(rotation=45)\n",
    "ax2.set_title('Original');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .bfill()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))\n",
    "ax1.plot(ts.bfill()[(ts.index > '2020-05-20') \n",
    "                 & (ts.index < '2020-06-07')]\n",
    "       )\n",
    "ax1.tick_params(rotation=45)\n",
    "ax1.set_title('Back Fill')\n",
    "\n",
    "ax2.plot(ts[(ts.index > '2020-05-20') \n",
    "                 & (ts.index < '2020-06-07')]\n",
    "       )\n",
    "ax2.tick_params(rotation=45)\n",
    "ax2.set_title('Original');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .interpolate()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))\n",
    "ax1.plot(ts.interpolate()[(ts.index > '2020-05-20') \n",
    "                 & (ts.index < '2020-06-07')]\n",
    "       )\n",
    "ax1.tick_params(rotation=45)\n",
    "ax1.set_title('Interpolation')\n",
    "\n",
    "ax2.plot(ts[(ts.index > '2020-05-20') \n",
    "                 & (ts.index < '2020-06-07')]\n",
    "       )\n",
    "ax2.tick_params(rotation=45)\n",
    "ax2.set_title('Original');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed with the interpolated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = ts.interpolate()\n",
    "ts.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cleaned up a few data points, let's downsample to the week level.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_weekly = ts.resample('W').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_weekly.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Diagnostics with Moving Averages\n",
    "\n",
    "Let's begin considering some models for our data.\n",
    "\n",
    "These are not useful for prediction just yet, but they will lead us towards our prediction models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Moving Average\n",
    "\n",
    "A simple moving average consists of an average across a specified window of time. \n",
    "\n",
    "The datetime index allows us to calculate simple moving averages via the `rolling()` function.\n",
    "\n",
    "The rolling function calculates a statistic across a moving **window**, which we can change with the window parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_weekly.rolling(window=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate a month long moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_weekly.rolling(4).mean()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is simply the avarage of a datapoint and the previous three data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_weekly[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_weekly[:4].mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_weekly[:4].mean() == ts_weekly.rolling(4).mean()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sma_week = ts_weekly.rolling(4).mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ts_weekly.plot(ax=ax, label='raw')\n",
    "sma_week.plot(ax=ax, label='rolling average')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plot below, the simple moving average **smooths** out the series. Smoothing can help visualize the underlying pattern. It can also be a very simple predictive model, where we just project the mean out into the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's zoom in\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ts_weekly[-100:].plot(ax=ax, c='r', label='raw')\n",
    "sma_week[-100:].plot(ax=ax, c='b', label='rolling average')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple moving avereage tracks fairly well, but does not reach to the peaks and valleys of the original distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot the moving average across 52 weeeks, we can see a smooth trend across a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sma_week = ts_weekly.rolling(52).mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ts_weekly.plot(ax=ax, label='raw')\n",
    "sma_week.plot(ax=ax, label='rolling average')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponentially Weighted Moving Average \n",
    "\n",
    "An alternative to SMA is the [EWMA](https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average). The exponentially weighted average gives more weight to the points closer to the date in question.  With EWMA, the average will track more closely to the peaks and valleys. If there are extreme historical values in the dataset, the EWMA will be less skewed than the SMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ts_ex_ewm = ts_weekly.ewm(alpha=0.5).mean()[:10]\n",
    "ts_ex_ewm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the $\\alpha$ parameter, the closer the EWMA will be to the actual value of the point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_ex_ewm = ts_weekly.ewm(alpha=0.99).mean()[:10]\n",
    "ts_ex_ewm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_weekly[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our rolling statistics with some different windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ts_weekly[-100:].plot(ax=ax, c='r', label='Original')\n",
    "ts_weekly.rolling(4).mean().dropna()[-100:].plot(ax=ax, c='g', label='SMA')\n",
    "ts_weekly.ewm(span=4).mean().dropna()[-100:].plot(ax=ax, c='b', label='EWMA')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, if we zoom in to the year level, we can see peaks and valleys according to the seasons.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ts_weekly.plot(ax=ax, c='r', label='Original')\n",
    "ts_weekly.rolling(52).mean().dropna().plot(ax=ax, c='g', label='SMA')\n",
    "ts_weekly.ewm(span=52).mean().dropna().plot(ax=ax, c='b', label='EWMA')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot rolling averages for the variance and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ts_weekly.plot(ax=ax, c='r', label='Original')\n",
    "ts_weekly.rolling(4).var().dropna().plot(ax=ax, c='g', label='SMA')\n",
    "ts_weekly.ewm(span=4).var().dropna().plot(ax=ax, c='b', label='EWMA')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ts_weekly.rolling(52).var().dropna().plot(ax=ax, c='g', label='SMA')\n",
    "ts_weekly.ewm(span=52).var().dropna().plot(ax=ax, c='b', label='EWMA')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we zoom in on our standard deviation, we can see that the variance of our data has quite a fluctuation at different moments in time.  When we are building our models, we will want to remove this variability, or our models will have different performance at different times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of Time Series Data\n",
    "\n",
    "A time series in general is supposed to be affected by four main components, which can be separated from the observed data. These components are: *Trend, Cyclical, Seasonal and Irregular* components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Trend** : The long term movement of a time series. For example, series relating to population growth, number of houses in a city etc. show upward trend.\n",
    "- **Seasonality** : Fluctuation in the data set that follow a regular pattern due to outside influences. For example sales of ice-cream increase in summer, or daily web traffic.\n",
    "- **Cyclical** : When data exhibit rises and falls that are not of fixed period.  Think of business cycles which usually last several years, but where the length of the current cycle is unknown beforehand.\n",
    "- **Irregular**: Are caused by unpredictable influences, which are not regular and also do not repeat in a particular pattern. These variations are caused by incidences such as war, strike, earthquake, flood, revolution, etc. There is no defined statistical technique for measuring random fluctuations in a time series.\n",
    "\n",
    "\n",
    "*Note: Many people confuse cyclic behaviour with seasonal behaviour, but they are really quite different. If the fluctuations are not of fixed period then they are cyclic; if the period is unchanging and associated with some aspect of the calendar, then the pattern is seasonal.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statsmodels seasonal decompose can also help show us the trends in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition = seasonal_decompose(ts_weekly)\n",
    "fig = plt.figure()\n",
    "fig = decomposition.plot()\n",
    "fig.set_size_inches(15, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical stationarity: \n",
    "\n",
    "When building our models, we will want to account for these trends somehow.  Time series whose mean and variance have trends across time will be difficult to predict out into the future. \n",
    "\n",
    "A **stationary time series** is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time. Most statistical forecasting methods are based on the assumption that the time series can be rendered approximately stationary (i.e., \"stationarized\") through the use of mathematical transformations. A stationarized series is relatively easy to predict: you simply predict that its statistical properties will be the same in the future as they have been in the past!\n",
    "\n",
    "It may seem counterintuitive that, for modeling purposes, we want our time series not to be a function of time! But the basic idea is the familiar one that we want our datapoints to be mutually *independent*. For more on this topic, see [here](https://stats.stackexchange.com/questions/19715/why-does-a-time-series-have-to-be-stationary).\n",
    "\n",
    "One way of testing for stationarity is to use the Dickey-Fuller Test. The statsmodels version returns the test statistic and a p-value, relative to the null hypothesis that the series in question is NOT stationary. For more, see [this Wikipedia page](https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 style=\"text-align: center;\">Constant Mean</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../img/mean_nonstationary.webp'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\">Constant Variance</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../img/variance_nonstationary.webp'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\">Constant Covariance</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../img/covariance_nonstationary.webp'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can get a sense of how stationary our data is with visuals, the Dickey-Fuller test gives us a quantitatitive measure.\n",
    "\n",
    "Here the null hypothesis is that the TS is non-stationary. If the test statistic is less than the critical value, we can reject the null hypothesis and say that the series is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that will help us to quickly test stationarity\n",
    "def test_stationarity(timeseries, window):\n",
    "    \n",
    "    #Determing rolling statistics\n",
    "    rolmean = timeseries.rolling(window=window).mean()\n",
    "    rolstd = timeseries.rolling(window=window).std()\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    orig = plt.plot(timeseries.iloc[window:], color='blue', label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label='Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show()\n",
    "    \n",
    "    #Perform Dickey-Fuller test:\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic',\n",
    "                                             'p-value', '#Lags Used',\n",
    "                                             'Number of Observations Used'])\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)' %key] = value\n",
    "    print (dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(ts_weekly, 52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time series does not pass the test of stationarity for $\\alpha=0.05$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to stationarize time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A series of steps can be taken to stationarize your data - also known -  as removing trends (linear trends, seasonaility/periodicity, etc - more details on transformations <a href='http://people.duke.edu/~rnau/whatuse.htm'>here</a>).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to remove trends is to difference our data.  \n",
    "Differencing is performed by subtracting the previous observation (lag=1) from the current observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_weekly.diff().dropna()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_weekly.diff().dropna().plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_weekly[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we have to difference the differenced data (known as a second difference) to achieve stationary data. <b>The number of times we have to difference our data is the order of differencing</b> - we will use this information when building our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second order difference:\n",
    "\n",
    "ts_weekly.diff().diff().dropna()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also apply seasonal differences:\n",
    "    \n",
    "ts_weekly.diff(52).dropna()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's difference our data and see if it improves Dickey-Fuller Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ts_weekly.diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_stationarity(ts_weekly.diff().dropna(), 52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
