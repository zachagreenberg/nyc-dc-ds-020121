{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "SWBAT:\n",
    "- describe the basic concepts of NLP\n",
    "- use pre-processing methods for NLP \n",
    "    - tokenization\n",
    "    - stopword removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of NLP\n",
    "\n",
    "NLP allows computers to interact with text data in a structured and sensible way. In short, we will be breaking up series of texts into individual words (or groups of words), and isolating the words with **semantic value**.  We will then compare texts with similar distributions of these words, and group them together.\n",
    "\n",
    "In this section, we will discuss some steps and approaches to common text data analytic procedures. Some of the applications of natural language processing are:\n",
    "- Chatbots \n",
    "- Speech recognition and audio processing \n",
    "- Classifying documents \n",
    "\n",
    "Here is an example that uses some of the tools we use in this notebook.  \n",
    "  -[chi_justice_project](https://chicagojustice.org/research/justice-media-project/)  \n",
    "  -[chicago_justice classifier](https://github.com/chicago-justice-project/article-tagging/blob/master/lib/notebooks/bag-of-words-count-stemmed-binary.ipynb)\n",
    "\n",
    "We will introduce you to the preprocessing steps, feature engineering, and other steps you need to take in order to format text data for machine learning tasks. \n",
    "\n",
    "We will also introduce you to [**NLTK**](https://www.nltk.org/) (Natural Language Toolkit), which will be our main tool for engaging with textual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP process\n",
    "\n",
    "<img src=\"img/nlp_process.png\" style=\"width:1000px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !conda install -c anaconda nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with a dataset which includes both **satirical** (The Onion) and real news (Reuters) articles. \n",
    "\n",
    "We refer to the entire set of articles as the **corpus**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![the_onion](img/the_onion.jpeg) ![reuters](img/reuters.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('../data/satire_nosatire.csv')\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to detect satire, so our target class of 1 is associated with The Onion articles.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.loc[10].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.loc[10].target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.loc[502].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.loc[502].target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each article in the corpus is refered to as a **document**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a balanced dataset with 500 documents of each category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about our types of error and the use cases of being able to correctly separate satirical from authentic news. What type of error should we decide to optimize our models for?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thoughts here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization \n",
    "\n",
    "In order to convert the texts into data suitable for machine learning, we need to break down the documents into smaller parts. \n",
    "\n",
    "The first step in doing that is **tokenization**.\n",
    "\n",
    "Tokenization is the process of splitting documents into units of observations. We usually represent the tokens as __n-grams__, where n represent the number of consecutive words occuring in a document that we will consider a unit. In the case of unigrams (one-word tokens), the sentence \"David works here\" would be tokenized into:\n",
    "\n",
    "- \"David\", \"works\", \"here\";\n",
    "\n",
    "If we want (also) to consider bigrams, we would (also) consider:\n",
    "\n",
    "- \"David works\" and \"works here\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the first document in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_document = corpus.iloc[0].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to tokenize our document. \n",
    "\n",
    "It is a long string, so the first way we might consider is to split it by spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is not ideal. We are trying to create a set of tokens with **high semantic value**.  In other words, we want to isolate text which best represents the meaning in each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common text cleaning tasks:  \n",
    "  1. remove capitalization  \n",
    "  2. remove punctuation  \n",
    "  3. remove stopwords  \n",
    "  4. remove numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could manually perform all of these tasks with string operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capitalization\n",
    "\n",
    "When we create our matrix of words associated with our corpus, **capital letters** will mess things up.  The semantic value of a word used at the beginning of a sentence is the same as that same word in the middle of the sentence.  In the two sentences:\n",
    "\n",
    "sentence_one =  \"Excessive gerrymandering in small counties suppresses turnout.\"   \n",
    "sentence_two =  \"Turnout is suppressed in small counties by excessive gerrymandering.\"  \n",
    "\n",
    "'excessive' has the same semantic value, but will be treated as different tokens because of capitals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_one =  \"Excessive gerrymandering in small counties suppresses turnout.\" \n",
    "sentence_two =  \"Turnout is suppressed in small counties by excessive gerrymandering.\"\n",
    "\n",
    "Excessive = sentence_one.split(' ')[0]\n",
    "excessive = sentence_two.split(' ')[-2]\n",
    "print(excessive, Excessive)\n",
    "excessive == Excessive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_cleanup = [word.lower() for word in first_document.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Our initial token set for our first document is {len(manual_cleanup)} words long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Our initial token set for our first document has \\\n",
    "{len(set(first_document.split()))} unique words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"After removing capitals, our first document has \\\n",
    "{len(set(manual_cleanup))} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punctuation\n",
    "\n",
    "Like capitals, splitting on white space will create tokens which include punctuation that will muck up our semantics.  \n",
    "\n",
    "Returning to the above example, 'gerrymandering' and 'gerrymandering.' will be treated as different tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_punct = sentence_one.split(' ')[1]\n",
    "punct = sentence_two.split(' ')[-1]\n",
    "print(no_punct, punct)\n",
    "no_punct == punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manual removal of punctuation\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_cleanup = [s.translate(str.maketrans('', '', string.punctuation))\\\n",
    "                  for s in manual_cleanup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"After removing punctuation, our first document has \\\n",
    "{len(set(manual_cleanup))} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are the **filler** words in a language: prepositions, articles, conjunctions. They have low semantic value, and almost always need to be removed.  \n",
    "\n",
    "Luckily, NLTK has lists of stopwords ready for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('greek')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which stopwords are present in our first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = [token for token in manual_cleanup if token in stopwords.words('english')]\n",
    "stops[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(stops)} stopwords in the first document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'That is {len(stops)/len(manual_cleanup): 0.2%} of our text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also use the **FreqDist** tool to look at the makeup of our text before and after removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(manual_cleanup)\n",
    "plt.figure(figsize=(10, 10))\n",
    "fdist.plot(30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_cleanup = [token for token in manual_cleanup if\\\n",
    "                  token not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also customize our stopwords list\n",
    "\n",
    "custom_sw = stopwords.words('english')\n",
    "custom_sw.extend([\"i'd\",\"say\"] )\n",
    "custom_sw[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_cleanup = [token for token in manual_cleanup if token not in custom_sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'After removing stopwords, there are {len(set(manual_cleanup))} unique words left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(manual_cleanup)\n",
    "plt.figure(figsize=(10, 10))\n",
    "fdist.plot(30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numbers\n",
    "\n",
    "Numbers also usually have low semantic value. Their removal can help improve our models. \n",
    "\n",
    "To remove them, we will use regular expressions, a powerful tool which you may already have some familiarity with.\n",
    "\n",
    "Regex allows us to match strings based on a pattern.  This pattern comes from a language of identifiers, which we can begin exploring on the cheatsheet found here:\n",
    "  -   https://regexr.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few key symbols:\n",
    "  - . : matches any character\n",
    "  - \\d, \\w, \\s : represent digit, word, whitespace  \n",
    "  - *, ?, +: matches 0 or more, 0 or 1, 1 or more of the preceding character  \n",
    "  - [A-Z]: matches any capital letter  \n",
    "  - [a-z]: matches lowercase letter  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other helpful resources:\n",
    "  - https://regexcrossword.com/\n",
    "  - https://www.regular-expressions.info/tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use regex to isolate numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '[0-9]'\n",
    "number = re.findall(pattern, first_document)\n",
    "number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern2 = '[0-9]+'\n",
    "number2 = re.findall(pattern2, first_document)\n",
    "number2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn and NLTK provide us with a suite of **tokenizers** for our text preprocessing convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember that the '?' indicates 0 or 1 of what follows!\n",
    "\n",
    "re.findall(r\"([a-zA-Z]+(?:'[a-z]+)?)\", \"I'd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "tokenizer = RegexpTokenizer(r\"([a-zA-Z]+(?:[â€™'][a-z]+)?)\")\n",
    "first_doc = tokenizer.tokenize(first_document)\n",
    "first_doc = [token.lower() for token in first_doc]\n",
    "first_doc = [token for token in first_doc if token not in custom_sw]\n",
    "first_doc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We are down to {len(set(first_doc))} unique words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
