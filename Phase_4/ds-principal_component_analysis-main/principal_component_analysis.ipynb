{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explain the concepts behind principal component analysis (PCA)\n",
    "- Explain how PCA addresses the problem of multicollinearity\n",
    "- Explain the idea of eigendecomposition\n",
    "- Implement PCA using `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "So far, you have dealt with datasets with a moderate number of predictors. What would happen if you had thousands of predictors? A few problems can arise:\n",
    "\n",
    "* Requires a ton of computing power/time\n",
    "* Computational problems caused by multicollinearity\n",
    "* Can overfit your data\n",
    "\n",
    "How could we address this problem?\n",
    "\n",
    "* You could drop a bunch of predictors at random, but you would potentially lose useful information that way \n",
    "* You could drop predictors that have weak correlations with your target, but they may still be useful in combination with other features in non-linear models (e.g. interaction terms, decision trees) \n",
    "* You could combine a bunch of features together, such as by multiplying them, but it's not clear how you would do this to best preseve information\n",
    "\n",
    "Principal Component Analysis (PCA) is a tool for reducing the dimensionality of our data in a way that tries to preserve information. It does this by projecting our data from a higher-dimensional space onto a lower-dimensional space. The PCA algorithm chooses a lower-dimensional space to project to that will preserve as much variance as possible from our original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario: Shipping Costs\n",
    "\n",
    "Let's say that we want to predict the cost to ship a package based on its properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = pd.read_csv('packages.csv')\n",
    "packages.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality\n",
    "\n",
    "You can think about each variable as a dimension, and thus each package as a data point. If we take just one feature, we can easily visualize this in 2 dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages.plot(kind='scatter', y='Shipping Cost ($)', x='Length (in)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of each package as a point in six-dimensional space - 5 dimensions for our features and 1 for our target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation and Covariance Matrices\n",
    "\n",
    "The first four features in this dataset all relate to package size, so we might expect them to be strongly related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(packages.corr(),\n",
    "            annot=True,\n",
    "            fmt='0.2g',\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            center=0,\n",
    "            cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA does not use this **correlation matrix**, which is conveniently scaled between -1 and 1. Rather, it uses the **covariance matrix**, which is scaled in square units of the original variables. This makes PCA very sensitive to the scale of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(packages.cov(),\n",
    "            annot=True,\n",
    "            fmt='0.2g',\n",
    "            center=0,\n",
    "            cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's normalize our variables to mean = 0 & SD = 1, which will make our covariance matrix equal the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_scaled = (packages - packages.mean())/packages.std()\n",
    "sns.heatmap(packages_scaled.cov(),\n",
    "            annot=True,\n",
    "            fmt='0.2g',\n",
    "            center=0,\n",
    "            cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, for the centered data matrix $X$, the covariance matrix $C$ is equal to $\\frac{1}{n-1}X^TX$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_scaled.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_scaled.T.dot(packages_scaled) / (len(packages_scaled)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means that the covariance matrix captures preserves the information about the spread of our dataset. What we want to do now is to explain that spread, one linear transformation (one **eigenvector**) at at a time. For more see [this useful blog post](https://datascienceplus.com/understanding-the-covariance-matrix/).\n",
    "\n",
    "Let's try to reduce the dimensionality of our dataset. Since the features capturing size are strongly correlated, we might expect to be able to reduce our feature dimensions down to two without losing much information (i.e. variance in our features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigendecomposition\n",
    "\n",
    "We will use an **eigendecomposition** of the covariance matrix to create a new set of dimensions. We can then decide how many of these dimensions to keep based on how much variance is captured by each dimension.\n",
    "\n",
    "Here, we show you how to do this using the NumPy `.eig()` function, but we will learn how to do PCA more easily in `sklearn` later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_packages_scaled = packages_scaled['Shipping Cost ($)']\n",
    "X_packages_scaled = packages_scaled.drop('Shipping Cost ($)', axis=1)\n",
    "\n",
    "cov_mat = X_packages_scaled.cov().values\n",
    "eigvals, eigvecs = np.linalg.eig(cov_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This decomposition gives us two things: eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues\n",
    "\n",
    "**Eigenvalues** represent the relative amount of variance captured by each new dimension. The average eigenvalue will be 1, so we look for values over 1 to identify dimensions that capture more variance than average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have one great dimension capturing 3.4x more variance than average, one OK dimension capturing an average amount of variance, and three other dimensions that don't capture much variance. This is in line with what we were expecting! It means that we can just use the first two dimensions - and drop the last three - without losing much variance/information from our predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportion of Variance\n",
    "\n",
    "You can also divide your eigenvalues by the number of features and then interpret them as the _propotion of variance in the features_ captured by each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvectors (aka Principal Components)\n",
    "\n",
    "**Eigenvectors** represent the new dimensions, which we call **principal components** when doing PCA. There is one eigenvector for each dimension, and they are all combined together into one matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PCA, the values in our eigenvectors are called **component weights**, and they tell us how much variance of each feature is captured by that dimension. These weights range from -1 to 1, but the relative sizes are what matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonality\n",
    "\n",
    "These eigenvectors are **orthogonal**, meaning their dot product is zero. Think of it like being at right angles, like the x and y axes of a graph, but in higher-dimensional space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvec1 = eigvecs[:, 0]\n",
    "eigvec2 = eigvecs[:, 1]\n",
    "eigvec1.dot(eigvec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Principal Component\n",
    "\n",
    "The first column of `eigvecs` is our first eigenvector, corresponding to the eigenvalue of 3.4. Let's look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvec1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first four numbers are relatively large, while the fifth is near zero. This means that this first dimension is almost entirely capturing the shared variance in our four size features, as we hoped! It's also interesting to note that the weights for the four features are almost equal, so they are equally represented in this dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Principal Component\n",
    "\n",
    "Let's look at our second eigenvector and see what features it seems to be capturing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvec2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it is almost entirely capturing the distance dimension, which makes sense, since that is not related to the package size at all. It has an eigenvalue of 1, which is appropriate, since the eigenvector only captures one feature, which wasn't captured at all in the first principal component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaining Principal Components\n",
    "\n",
    "Since the remaining eigenvalues were all much less than 1, we can ignore the eigenvectors associated with them. We will not include components corresponding to them in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidebar: Properties of Eigenvectors\n",
    "\n",
    "These eigenvectors have **unit length** (length 1) in multi-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(eigvec1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvectors are related to eigenvalues by the following property: $\\vec{x}$ is an eigenvector of the matrix $A$ if $A\\vec{x} = \\lambda\\vec{x}$, for some eigenvalue $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat.dot(eigvec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigval1 = eigvals[0]\n",
    "eigval1*eigvec1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Data\n",
    "\n",
    "We will now use these principal components to create new features. These features will be weighted sums (aka **linear combinations**) of existing features, using the component weights from the eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Component\n",
    "\n",
    "We will now create a new feature using the first principal component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvec1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first feature will be calculated as follows:\n",
    "\n",
    "**PC1** = 0.492 * Length + 0.508 * Width + 0.508 * Height + 0.492 * Weight - 0.003 * Distance\n",
    "\n",
    "We use a dot product between the data and the eigenvector to do the arithmetic for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = X_packages_scaled.values\n",
    "pc1 = data_array.dot(eigvec1)\n",
    "X_packages_pca = pd.DataFrame(data = pc1, columns=['PC1'])\n",
    "X_packages_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Components\n",
    "\n",
    "You can calculate all the new features at once using a dot product with the `eigvecs` matrix, which has all the eigenvectors in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = data_array.dot(eigvecs)\n",
    "X_packages_pca = pd.DataFrame(data = pcs, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5'])\n",
    "X_packages_pca.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Correlations\n",
    "\n",
    "Because we used eigenvectors to construct our new features, we have completely solved any multicollinearity issues. This is because the eigenvectors define new, uncorrelated dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X_packages_pca.corr(),\n",
    "            annot=True,\n",
    "            fmt='0.2g',\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            center=0,\n",
    "            cmap= 'coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Now let's compare linear regression models with...\n",
    "\n",
    "* All five original features \n",
    "* All five new features\n",
    "* Only 2 best new features\n",
    "* Only 1 best new feature\n",
    "\n",
    "Note that we are skipping the validation process with these models - we'll do that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm1 = OLS(exog=X_packages_scaled, endog=y_packages_scaled).fit()\n",
    "sm1.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm2 = OLS(exog=X_packages_pca, endog=y_packages_scaled).fit()\n",
    "sm2.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm3 = OLS(exog=X_packages_pca[['PC1', 'PC2']], endog=y_packages_scaled).fit()\n",
    "sm3.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm4 = OLS(exog=X_packages_pca[['PC1']], endog=y_packages_scaled).fit()\n",
    "sm4.rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA in `sklearn`\n",
    "\n",
    "As always, `sklearn` makes this all much easier, this time with the `PCA()` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2) # Check out how `n_components` works\n",
    "\n",
    "X_packages_pca2 = pca.fit_transform(X_packages_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the eigenvalues and eigenvectors out, too "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the signs get flipped on the eigenvectors - don't worry about it. Think of \"up\" and \"down\" as both representing the same dimension, just in opposite directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can use your transformed data as you would in any model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pca = LinearRegression()\n",
    "lr_pca.fit(X_packages_pca2, y_packages_scaled)\n",
    "lr_pca.score(X_packages_pca2, y_packages_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario: Car Properties\n",
    "\n",
    "Use PCA to reduce the dimensionality of features in the example below: Predict car mpg using car properties. We've done the data prep. Now you practice the modeling, including scoring on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cars = pd.read_csv('cars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars[' cubicinches'].replace(' ', np.nan, inplace=True)\n",
    "cars[' cubicinches'] = cars[' cubicinches'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars[' weightlbs'].replace(' ', np.nan, inplace=True)\n",
    "cars[' weightlbs'] = cars[' weightlbs'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cars.drop('mpg', axis=1),\n",
    "                                                    cars['mpg'],\n",
    "                                                   random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct1 = ColumnTransformer(transformers=[\n",
    "    ('imputer', SimpleImputer(), [1, 3])],\n",
    "    remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct2 = ColumnTransformer(transformers=[\n",
    "    ('scaler', StandardScaler(), [0, 1, 2, 3, 4, 5]),\n",
    "    ('ohe', OneHotEncoder(), [6])],\n",
    "    remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('ct1', ct1),\n",
    "    ('ct2', ct2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_pp = pipe.transform(X_train)\n",
    "X_te_pp = pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Code\n",
    "\n",
    "<details>\n",
    "    <summary>\n",
    "        No peeking until you've tried it out first!\n",
    "    </summary>\n",
    "<code>\n",
    "## Let's start with a linear regression\n",
    "lr = LinearRegression().fit(X_tr_pp, y_train)\n",
    "## Score on train\n",
    "lr.score(X_tr_pp, y_train)\n",
    "## Score on test\n",
    "lr.score(X_te_pp, y_test)\n",
    "    </code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients of the best-fit hyperplane\n",
    "\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, our best-fit hyperplane is given by:\n",
    "\n",
    "$2.177\\times in^3\\_sd - 4.645\\times lbs.\\_sd - 1.555\\times cyl\\_sd - 1.154\\times hp\\_sd -  0.267\\times time_{60}\\_sd + 2.604\\times yr\\_sd + 0.708\\times brand_{Europe} + 0.912\\times brand_{Japan} - 1.620\\times brand_{US}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_pca = PCA(n_components=3) \n",
    "\n",
    "X_train_new = cars_pca.fit_transform(X_tr_pp)\n",
    "X_test_new = cars_pca.transform(X_te_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of our PCA are as follows:\n",
    "\n",
    "**PC1** = 0.465 * cubicinches_sd + 0.435 * weightlbs_sd + 0.449 * cylinders_sd + 0.454 * hp_sd - 0.349 * time-to-60_sd - 0.187 * year_sd - 0.068 * Europe - 0.073 * Japan + 0.140 * US\n",
    "\n",
    "**PC2** = -0.099 * cubicinches_sd - 0.196 * weightlbs_sd - 0.131 * cylinders_sd + 0.006 * hp_sd - 0.125 * time-to-60_sd - 0.937 * year_sd + 0.129 * Europe + 0.022 * Japan - 0.152 * US\n",
    "\n",
    "**PC3** = 0.141 * cubicinches_sd + 0.342 * weightlbs_sd + 0.187 * cylinders_sd - 0.144 * hp_sd + 0.851 * time-to-60_sd - 0.239 * year_sd + 0.043 * Europe - 0.132 * Japan + 0.089 * US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling with New Dimensions\n",
    "\n",
    "Now that we have optimized our features, we can build a new model with them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pca = LinearRegression()\n",
    "lr_pca.fit(X_train_new, y_train)\n",
    "lr_pca.score(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new = cars_pca.transform(X_te_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pca.score(X_test_new, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pca.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, our best-fit hyperplane is given by:\n",
    "\n",
    "$-2.967\\times PC1 - 1.162\\times PC2 -2.486\\times PC3$\n",
    "\n",
    "Of course, since the principal components are just linear combinations of our original predictors, we could re-express this hyperplane in terms of those original predictors!\n",
    "\n",
    "And if the PCA was worth anything, we should expect the new linear model to be *different from* the first!\n",
    "\n",
    "Recall that we had:\n",
    "\n",
    "**PC1** = 0.465 * cubicinches_sd + 0.435 * weightlbs_sd + 0.449 * cylinders_sd + 0.454 * hp_sd - 0.349 * time-to-60_sd - 0.187 * year_sd - 0.068 * Europe - 0.073 * Japan + 0.140 * US\n",
    "\n",
    "**PC2** = -0.099 * cubicinches_sd - 0.196 * weightlbs_sd - 0.131 * cylinders_sd + 0.006 * hp_sd - 0.125 * time-to-60_sd - 0.937 * year_sd + 0.129 * Europe + 0.022 * Japan - 0.152 * US\n",
    "\n",
    "**PC3** = 0.141 * cubicinches_sd + 0.342 * weightlbs_sd + 0.187 * cylinders_sd - 0.144 * hp_sd + 0.851 * time-to-60_sd - 0.239 * year_sd + 0.043 * Europe - 0.132 * Japan + 0.089 * US\n",
    "\n",
    "Therefore, our new PCA-made hyperplane can be expressed as:\n",
    "\n",
    "$-2.967\\times(0.465\\times in^3\\_sd + 0.435\\times lbs.\\_sd + 0.449\\times cyl\\_sd + 0.454\\times hp\\_sd - 0.349\\times time_{60}\\_sd - 0.187\\times yr\\_sd - 0.068\\times brand_{Europe} - 0.073\\times brand_{Japan} + 0.140\\times brand_{US})$ <br/> $- 1.162\\times(-0.099\\times in^3\\_sd - 0.196\\times lbs.\\_sd - 0.131\\times cyl\\_sd + 0.006\\times hp\\_sd - 0.125\\times time_{60}\\_sd - 0.937\\times yr\\_sd + 0.129\\times brand_{Europe} + 0.022\\times brand_{Japan} - 0.152\\times brand_{US})$ <br/> $- 2.486\\times(0.141\\times in^3\\_sd + 0.342\\times lbs.\\_sd + 0.187\\times cyl\\_sd -0.144\\times hp\\_sd + 0.851\\times time_{60}\\_sd - 0.239\\times yr\\_sd + 0.043\\times brand_{Europe} - 0.132\\times brand_{Japan} + 0.089\\times brand_{US})$\n",
    "\n",
    "Let's make these calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'cubicinches_sd coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 0], 3)}')\n",
    "print(f'weightlbs_sd coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 1], 3)}')\n",
    "print(f'cylinders_sd coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 2], 3)}')\n",
    "print(f'horsepower_sd coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 3], 3)}')\n",
    "print(f'timeto60_sd coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 4], 3)}')\n",
    "print(f'year_sd coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 5], 3)}')\n",
    "print(f'Europe coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 6], 3)}')\n",
    "print(f'Japan coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 7], 3)}')\n",
    "print(f'US coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 8], 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our best-fit hyperplane using PCA is:\n",
    "\n",
    "$-1.616\\times in^3\\_sd - 1.913\\times lbs.\\_sd - 1.646\\times cyl\\_sd - 0.996\\times hp\\_sd - 0.933\\times time_{60}\\_sd + 2.237\\times yr\\_sd - 0.055\\times brand_{Europe} + 0.517\\times brand_{Japan} - 0.462\\times brand_{US}$\n",
    "\n",
    "Recall that our first linear regression model had:\n",
    "\n",
    "$2.177\\times in^3\\_sd - 4.645\\times lbs.\\_sd - 1.555\\times cyl\\_sd - 1.154\\times hp\\_sd -  0.267\\times time_{60}\\_sd + 2.604\\times yr\\_sd + 0.708\\times brand_{Europe} + 0.912\\times brand_{Japan} - 1.620\\times brand_{US}$\n",
    "\n",
    "which is clearly a different hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassembling the whole dataset for the sake of visualization\n",
    "X_transformed = np.vstack([X_train_new, X_test_new])\n",
    "y_new = np.concatenate([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots()\n",
    "a.plot(X_transformed[:, 0], y_new, 'r.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots()\n",
    "a.plot(X_transformed[:, 1], y_new, 'g.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots()\n",
    "a.plot(X_transformed[:, 2], y_new, 'k.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.hstack([X_transformed, y_new[:, np.newaxis]]),\n",
    "                  columns=['PC1', 'PC2', 'PC3', 'y'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.relplot(data=df,\n",
    "            x='PC1',\n",
    "            y='PC2',\n",
    "           hue='y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Up: Relation to Linear Regression\n",
    "\n",
    "Question: Is the first principal component the same line we would get if we constructed an ordinary least-squares regression line?\n",
    "\n",
    "Answer: No. The best-fit line minimizes the sum of squared errors, i.e. the minimum sum of (\"vertical\") distances between the predictions and the real values of the dependent variable. Principal Component Analysis, by contrast, is not a modeling procedure and so has no target. The first principal component thus cannot minimize the sum of distances between predictions and real values; instead, it minimizes the sum of (\"perpendicular\") distances between the data points and *it (the line) itself*.\n",
    "\n",
    "Suppose we look at MPG vs. z-scores of weight in lbs. Let's make a scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots()\n",
    "\n",
    "a.scatter(X_tr_pp[:, 1], y_train)\n",
    "a.set_xlabel('weight z-scores (lbs.)')\n",
    "a.set_ylabel('efficiency (MPG)')\n",
    "a.set_title('MPG vs. Weight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the best-fit line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1 = LinearRegression().fit(X_tr_pp[:, 1].reshape(-1, 1),\n",
    "                               y_train).coef_\n",
    "beta0 = LinearRegression().fit(X_tr_pp[:, 1].reshape(-1, 1),\n",
    "                               y_train).intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots()\n",
    "\n",
    "a.scatter(X_tr_pp[:, 1], y_train)\n",
    "a.plot(X_tr_pp[:, 1],\n",
    "       beta1[0] * X_tr_pp[:, 1] + beta0,\n",
    "      c='r', label='best-fit line')\n",
    "a.set_xlabel('weight z-scores (lbs.)')\n",
    "a.set_ylabel('efficiency (MPG)')\n",
    "a.set_title('MPG vs. Weight')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what the principal component looks like. We'll make use of the `inverse_transform()` method of `PCA()` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1 = PCA(n_components=1).fit(np.concatenate((X_tr_pp[:, 1].reshape(-1, 1),\n",
    "                                 y_train.values.reshape(-1, 1)),\n",
    "                                axis=1))\n",
    "\n",
    "pc = pc1.transform(np.concatenate((X_tr_pp[:, 1].reshape(-1, 1),\n",
    "                                 y_train.values.reshape(-1, 1)),\n",
    "                                axis=1))\n",
    "\n",
    "pc_inv = pc1.inverse_transform(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots()\n",
    "\n",
    "a.scatter(X_tr_pp[:, 1], y_train)\n",
    "a.plot(X_tr_pp[:, 1],\n",
    "       beta1[0] * X_tr_pp[:, 1] + beta0,\n",
    "      c='r', label='best-fit line')\n",
    "a.plot(pc_inv[:, 0],\n",
    "       pc_inv[:, 1],\n",
    "      c='b', label='principal component')\n",
    "a.set_xlabel('weight z-scores (lbs.)')\n",
    "a.set_ylabel('efficiency (MPG)')\n",
    "a.set_title('MPG vs. Weight')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out this post, to which the preceding is indebted, for more on this subtle point: https://shankarmsy.github.io/posts/pca-vs-lr.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Up: Diagonalization\n",
    "\n",
    "The key idea is to diagonalize (i.e. find the eigendecomposition of) the covariance matrix. The decomposition will produce a set of orthogonal vectors that explain as much of the remaining variance as possible. These are our [principal components](https://math.stackexchange.com/questions/23596/why-is-the-eigenvector-of-a-covariance-matrix-equal-to-a-principal-component).\n",
    "\n",
    "Let's say a word about eigenvalues and eigenvectors. It turns out that eigenvalues and -vectors have a dizzying number of applications. But the basic idea is that, if we can split a bunch of vectors (i.e. a matrix) into a set of mutually orthogonal vectors, then we can isolate the force of the bunch into discrete bits, each of which by itself acts like a simple linear transformation.\n",
    "\n",
    " That is, the vector is oriented in just such a direction that multiplying the matrix by it serves only to lengthen or shorten it.\n",
    "\n",
    "The diagonalization looks like this:\n",
    "\n",
    "$A = Q\\Lambda Q^{-1}$, where $Q$ is a matrix comprising the **eigenvectors** of $A$ and $\\Lambda$ has non-zero elements only along its main diagonal (hence the \"diagonalization\" of $A$). These non-zero elements are the **eigenvalues** of $A$. We'll return to eigendecomposition later when we discuss recommendation systems and the singular value decomposition, which is a related matrix factorization.\n",
    "\n",
    "Suppose we have the matrix\n",
    "$A =\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "$.\n",
    "\n",
    "Let's calculate the eigendecomposition of this matrix.\n",
    "\n",
    "In order to do this, we set $(A - \\lambda I)\\vec{x} = 0$. One trivial solution is $\\vec{x} = \\vec{0}$, but if there are more interesting solutions, then it must be that $|A - \\lambda I| = 0$, which is to say that some column vector in $A - \\lambda I$ must be expressible as a linear combination of the other columns. (Otherwise, there would be no way to \"undo\" the multiplicative effect of a column vector on $\\vec{x}$!) For more on this point, see [this page](http://www2.math.uconn.edu/~troby/math2210f16/LT/sec1_7.pdf).\n",
    "\n",
    "So we have:\n",
    "\n",
    "$\\begin{vmatrix}\n",
    "a_{11} - \\lambda & a_{12} \\\\\n",
    "a_{21} & a_{22} - \\lambda\n",
    "\\end{vmatrix} = 0$\n",
    "\n",
    "$(a_{11} - \\lambda)(a_{22} - \\lambda) - a_{12}a_{21} = 0$\n",
    "\n",
    "$\\lambda^2 - (a_{11} + a_{22})\\lambda + a_{11}a_{22} - a_{12}a_{21}$\n",
    "\n",
    "$\\lambda = \\frac{a_{11} + a_{22}\\pm\\sqrt{(a_{11} + a_{22})^2 + 4(a_{12}a_{21} - a_{11}a_{22})}}{2}$\n",
    "\n",
    "Suppose e.g. we had\n",
    "\n",
    "$A = \\begin{bmatrix}\n",
    "5 & 3 \\\\\n",
    "3 & 5\n",
    "\\end{bmatrix}$.\n",
    "\n",
    "We can use the equation we just derived to solve for the eigenvalues of this matrix. Then we can plug *those* into our eigenvector definition to solve for the eigenvectors:\n",
    "\n",
    "So:\n",
    "\n",
    "### Eigenvalues\n",
    "\n",
    "$\\lambda = \\frac{5+5\\pm\\sqrt{(5+5)^2+4(3\\times 3 - 5\\times 5)}}{2} = 5\\pm\\frac{\\sqrt{36}}{2} = 2, 8$.\n",
    "\n",
    "### Eigenvectors\n",
    "\n",
    "Now we can plug those in. If we plug in $\\lambda = 8$, then we get:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "5-8 & 3 \\\\\n",
    "3 & 5-8\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-3 & 3 \\\\\n",
    "3 & -3\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix} = 0.$\n",
    "\n",
    "So:\n",
    "\n",
    "$-3x_1 + 3x_2 = 0$ (or $3x_1 - 3x_2 = 0$)\n",
    "\n",
    "$x_1 = x_2$.\n",
    "\n",
    "It is standard to scale eigenvectors to a magnitude of 1, and so we would write this eigenvector as\n",
    "$\\begin{bmatrix}\n",
    "\\frac{\\sqrt{2}}{2} \\\\\n",
    "\\frac{\\sqrt{2}}{2}\n",
    "\\end{bmatrix}$.\n",
    "\n",
    "If we plug in $\\lambda = 2$, we find a second eigenvector equal to\n",
    "$\\begin{bmatrix}\n",
    "-\\frac{\\sqrt{2}}{2} \\\\\n",
    "\\frac{\\sqrt{2}}{2}\n",
    "\\end{bmatrix}$. (I'll leave this as an exercise.)\n",
    "\n",
    "**Thus we can express the full diagonalization of our matrix as follows**:\n",
    "\n",
    "$A = \\begin{bmatrix}\n",
    "5 & 3 \\\\\n",
    "3 & 5\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2} \\\\\n",
    "\\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{2}}{2}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "8 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{2}}{2} \\\\\n",
    "-\\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{2}}{2}\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level Up: Diagonalization In Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use np.linalg.eig()\n",
    "\n",
    "A = np.array([[5, 3], [3, 5]])\n",
    "np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.linalg.eig(X) returns a double of NumPy arrays, the first containing\n",
    "# the eigenvalues of X and the second containing the eigenvectors of X.\n",
    "\n",
    "v, q = np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.diag()\n",
    "\n",
    "np.diag(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct A by multiplication\n",
    "\n",
    "q.dot(np.diag(v)).dot(q.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration that the columns of q\n",
    "# are eigenvectors of A, where\n",
    "# multiplication by A scales them by\n",
    "# the eigenvalues\n",
    "\n",
    "print(np.allclose(A.dot(q.T[0]), v[0]*q.T[0]))\n",
    "print(np.allclose(A.dot(q.T[1]), v[1]*q.T[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Resource\n",
    "\n",
    "[Here's](https://www.youtube.com/watch?v=_UVHneBUBW0) a helpful video introduction to PCA if you're itching for more!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
