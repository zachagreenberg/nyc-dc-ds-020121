{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#Motivating--The-Bayesian-Classifier-üßê\" data-toc-modified-id=\"Motivating--The-Bayesian-Classifier-üßê-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Motivating  The Bayesian Classifier üßê</a></span><ul class=\"toc-item\"><li><span><a href=\"#Naive-Bayes-Setup\" data-toc-modified-id=\"Naive-Bayes-Setup-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Naive Bayes Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#What's-so-great-about-this?\" data-toc-modified-id=\"What's-so-great-about-this?-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>What's so great about this?</a></span></li></ul></li><li><span><a href=\"#The-Naive-Assumption\" data-toc-modified-id=\"The-Naive-Assumption-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>The Naive Assumption</a></span></li><li><span><a href=\"#The-formula\" data-toc-modified-id=\"The-formula-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>The formula</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-Parts-Can-We-Find?\" data-toc-modified-id=\"What-Parts-Can-We-Find?-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>What Parts Can We Find?</a></span></li></ul></li><li><span><a href=\"#Calculating-That-Our-Email-Is-Spam\" data-toc-modified-id=\"Calculating-That-Our-Email-Is-Spam-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Calculating That Our Email Is Spam</a></span></li><li><span><a href=\"#Extending-It-With-Multiple-Words\" data-toc-modified-id=\"Extending-It-With-Multiple-Words-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Extending It With Multiple Words</a></span></li></ul></li><li><span><a href=\"#Naive-Bayes-Modeling-Example\" data-toc-modified-id=\"Naive-Bayes-Modeling-Example-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Naive Bayes Modeling Example</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-Bayes's-Theorem-for-Classification\" data-toc-modified-id=\"Using-Bayes's-Theorem-for-Classification-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Using Bayes's Theorem for Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Does-this-look-like-a-classification-problem?\" data-toc-modified-id=\"Does-this-look-like-a-classification-problem?-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Does this look like a classification problem?</a></span></li></ul></li><li><span><a href=\"#Elephant-Example\" data-toc-modified-id=\"Elephant-Example-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Elephant Example</a></span><ul class=\"toc-item\"><li><span><a href=\"#Naive-Bayes-by-Hand\" data-toc-modified-id=\"Naive-Bayes-by-Hand-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Naive Bayes by Hand</a></span></li><li><span><a href=\"#Calculation-of-Likelihoods\" data-toc-modified-id=\"Calculation-of-Likelihoods-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Calculation of Likelihoods</a></span></li><li><span><a href=\"#Posteriors\" data-toc-modified-id=\"Posteriors-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Posteriors</a></span></li><li><span><a href=\"#More-Dimensions\" data-toc-modified-id=\"More-Dimensions-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>More Dimensions</a></span><ul class=\"toc-item\"><li><span><a href=\"#What's-&quot;Naive&quot;-about-This?\" data-toc-modified-id=\"What's-&quot;Naive&quot;-about-This?-3.2.4.1\"><span class=\"toc-item-num\">3.2.4.1&nbsp;&nbsp;</span>What's \"Naive\" about This?</a></span></li><li><span><a href=\"#Posteriors\" data-toc-modified-id=\"Posteriors-3.2.4.2\"><span class=\"toc-item-num\">3.2.4.2&nbsp;&nbsp;</span>Posteriors</a></span></li></ul></li><li><span><a href=\"#GaussianNB\" data-toc-modified-id=\"GaussianNB-3.2.5\"><span class=\"toc-item-num\">3.2.5&nbsp;&nbsp;</span><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\" target=\"_blank\"><code>GaussianNB</code></a></a></span></li></ul></li><li><span><a href=\"#Comma-Survey-Example\" data-toc-modified-id=\"Comma-Survey-Example-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Comma Survey Example</a></span><ul class=\"toc-item\"><li><span><a href=\"#Calculating-Priors-and-Likelihoods\" data-toc-modified-id=\"Calculating-Priors-and-Likelihoods-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Calculating Priors and Likelihoods</a></span></li><li><span><a href=\"#Calculating-Posteriors\" data-toc-modified-id=\"Calculating-Posteriors-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Calculating Posteriors</a></span></li><li><span><a href=\"#Comparison-with-MultinomialNB\" data-toc-modified-id=\"Comparison-with-MultinomialNB-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Comparison with <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\" target=\"_blank\"><code>MultinomialNB</code></a></a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "    # There is also a BernoulliNB for a dataset with binary predictors\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "SWBAT:\n",
    "\n",
    "- Describe how Bayes's Theorem can be used to make predictions of a target\n",
    "- Identify the appropriate variant of Naive Bayes models for a particular business problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Motivating  The Bayesian Classifier üßê"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Let's take a second to go through an example to get a feel for how Bayes' Theorem can help us with classification. Specifically about document classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Spam, Spam, Spam, Spam, Spam..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![Many cans of spam](images/wall_of_spam.jpeg)\n",
    "\n",
    "> This is the classic example: detecting email spam!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**The Problem Setup**\n",
    "\n",
    "> We get emails that can be either emails we care about (***ham*** üê∑) or emails we don't care about (***spam*** ü•´). \n",
    ">\n",
    "> We can probably look at the words in the email and get an idea of whether they are spam or not just by observing if they contain red-flag words üö©\n",
    "> \n",
    "> We won't always be right, but if we see an email that uses word(s) that are more often associated with spam, then we can feel more confident as labeling that email as spam!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Naive Bayes Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What we gotta do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Look at spam and not spam (ham) emails\n",
    "2. Identify words that suggest classification\n",
    "3. Determine probability that words occur in each classification\n",
    "4. Profit (classify new emails as \"spam\" or \"ham\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### What's so great about this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- We can keep updating our belief based on the email we detect\n",
    "- Relatively simple\n",
    "- Can expand to multiple words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## The Naive Assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$P(A,B) = P(A\\cap B) = P(A)\\ P(B)$ only if independent \n",
    "\n",
    "In practice, makes sense & is usually pretty good assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## The formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's say the word that occurs is \"cash\":\n",
    "\n",
    "$$ P(ü•´ | \"cash\") = \\frac{P(\"cash\" | ü•´)P(ü•´)}{P(\"cash\")}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### What Parts Can We Find?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- $P(\"cash\")$\n",
    "    * That's just the probability of finding the word \"cash\"! Frequency of the word!\n",
    "- $P(ü•´)$\n",
    "    * Well, we start with some data (_prior knowledge_). So frequency of the spam occurring!\n",
    "- $P(\"cash\" | ü•´)$\n",
    "    * How frequently is \"cash\" used in a known spam emails. Count the frequency across all spam emails\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Calculating That Our Email Is Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let's just say 2% of all the words are the word \"cash\"\n",
    "p_cash = 0.02\n",
    "\n",
    "# We normally would measure this from our data, but we'll take \n",
    "# it that 10% of all emails we collected were spam\n",
    "p_spam = 0.10\n",
    "\n",
    "# 12% of all words in a spam email have the word \"cash\"\n",
    "p_cash_given_its_spam = 0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p_spam_given_cash = p_cash_given_its_spam * p_spam / p_cash\n",
    "print(f'If the email has the word \"cash\" in it, there is a {p_spam_given_cash*100}% chance the email is spam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Check it**: Does this make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Extending It With Multiple Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> With more words, the more certain we can be if it is/isn't spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Spam:\n",
    "\n",
    "$$ P(ü•´\\ |\"buy\",\\ \"cash\") \\propto P(\"buy\",\\ \"cash\"|\\ ü•´)\\ P(ü•´)$$\n",
    "\n",
    "\n",
    "But because of independence: \n",
    "    \n",
    "$$ P(\"buy\",\\ \"cash\"|\\ ü•´) = P(\"buy\"|\\ ü•´)\\ P(\"cash\"|\\ ü•´)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Normalize by dividing!\n",
    "\n",
    "$$\n",
    "P(ü•´\\ |\"buy\",\\ \"cash\")  =\n",
    "    \\frac\n",
    "        {P(\"buy\"|\\ ü•´)P(\"cash\"|\\ ü•´)\\ P(ü•´)}\n",
    "        {P(\"buy\"|\\ ü•´)P(\"cash\"|\\ ü•´)\\ P(ü•´) + P(\"buy\"|\\ üê∑)P(\"cash\"|\\ üê∑)\\ P(üê∑)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Note:** If we wanted to find the most probable class (especially useful for _multiclass_), we find the maximum numerator for the given criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Naive Bayes Modeling Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Using Bayes's Theorem for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's recall Bayes's Theorem:\n",
    "\n",
    "$\\large P(h|e) = \\frac{P(h)P(e|h)}{P(e)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Does this look like a classification problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Suppose we have three competing hypotheses $\\{h_1, h_2, h_3\\}$ that would explain our evidence $e$.\n",
    "    - Then we could use Bayes's Theorem to calculate the posterior probabilities for each of these three:\n",
    "        - $P(h_1|e) = \\frac{P(h_1)P(e|h_1)}{P(e)}$\n",
    "        - $P(h_2|e) = \\frac{P(h_2)P(e|h_2)}{P(e)}$\n",
    "        - $P(h_3|e) = \\frac{P(h_3)P(e|h_3)}{P(e)}$\n",
    "        \n",
    "- Suppose the evidence is a collection of elephant weights.\n",
    "- Suppose each of the three hypotheses claims that the elephant whose measurements we have belongs to one of the three extant elephant species (*L. africana*, *L. cyclotis*, and *E. maximus*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In that case the left-hand sides of these equations represent the probability that the elephant in question belongs to a given species.\n",
    "\n",
    "If we think of the species as our target, then **this is just an ordinary classification problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What about the right-hand sides of the equations? **These other probabilities we can calculate from our dataset.**\n",
    "\n",
    "- The priors can simply be taken to be the percentages of the different classes in the dataset.\n",
    "- What about the likelihoods?\n",
    "    - If the relevant features are **categorical**, we can simply count the numbers of each category in the dataset. For example, if the features are whether the elephant has tusks or not, then, to calculate the likelihoods, we'll just count the tusked and non-tuksed elephants per species.\n",
    "    - If the relevant features are **numerical**, we'll have to do something else. A good way of proceeding is to rely on (presumed) underlying distributions of the data. [Here](https://medium.com/analytics-vidhya/use-naive-bayes-algorithm-for-categorical-and-numerical-data-classification-935d90ab273f) is an example of using the normal distribution to calculate likelihoods. We'll follow this idea below for our elephant data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Elephant Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose we have a dataset that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "elephs = pd.read_csv('data/elephants.csv', usecols=['height (cm)',\n",
    "                                                   'species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "elephs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.kdeplot(data=elephs[elephs['species'] == 'maximus']['height (cm)'],\n",
    "            ax=ax, label='maximus')\n",
    "sns.kdeplot(data=elephs[elephs['species'] == 'africana']['height (cm)'],\n",
    "            ax=ax, label='africana')\n",
    "sns.kdeplot(data=elephs[elephs['species'] == 'cyclotis']['height (cm)'],\n",
    "            ax=ax, label='cyclotis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Naive Bayes by Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose we want to make prediction of species for some new elephant whose weight we've just recorded. We'll suppose the new elephant has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "new_ht = 263"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What we want to calculate is the mean and standard deviation for height for each elephant species. We'll use these to calculate the relevant likelihoods.\n",
    "\n",
    "So:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_stats = elephs[elephs['species'] == 'maximus'].describe().loc[['mean', 'std'], :]\n",
    "max_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cyc_stats = elephs[elephs['species'] == 'cyclotis'].describe().loc[['mean', 'std'], :]\n",
    "cyc_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "afr_stats = elephs[elephs['species'] == 'africana'].describe().loc[['mean', 'std'], :]\n",
    "afr_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "elephs['species'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Calculation of Likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll use the PDFs of the normal distributions with the discovered means and standard deviations to calculate likelihoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stats.norm(loc=max_stats['height (cm)'][0],\n",
    "           scale=max_stats['height (cm)'][1]).pdf(263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stats.norm(loc=cyc_stats['height (cm)'][0],\n",
    "          scale=cyc_stats['height (cm)'][1]).pdf(263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stats.norm(loc=afr_stats['height (cm)'][0],\n",
    "          scale=afr_stats['height (cm)'][1]).pdf(263)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What we have just calculated are the likelihoods, i.e.:\n",
    "\n",
    "- $P(height=263 | species=maximus) = 2.04\\%$\n",
    "- $P(height=263 | species=cyclotis) = 1.50\\%$\n",
    "- $P(height=263 | species=africana) = 0.90\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "(Notice that they do NOT sum to 1!) But what we'd really like to know are the posteriors. I.e. what are:\n",
    "\n",
    "- $P(species=maximus | height=263)$?\n",
    "- $P(species=cyclotis | height=263)$?\n",
    "- $P(species=africana | height=263)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since we have equal numbers of each species, every prior is equal to $\\frac{1}{3}$. Thus we can calculate the probability of the evidence:\n",
    "\n",
    "$P(height=263) = \\frac{1}{3}(0.0204 + 0.0150 + 0.0090) = 0.0148$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And therefore calculate the posteriors using Bayes's Theorem:\n",
    "\n",
    "- $P(species=maximus | height=263) = \\frac{1}{3}\\frac{0.0204}{0.0148} = 45.9\\%$;\n",
    "- $P(species=cyclotis | height=263) = \\frac{1}{3}\\frac{0.0150}{0.0148} = 33.8\\%$;\n",
    "- $P(species=africana | height=263) = \\frac{1}{3}\\frac{0.0090}{0.0148} = 20.3\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Bayes's Theorem shows us that the largest posterior belongs to the *maximus* species. (Note also that, since the priors are all the same, the largest posterior will necessarily belong to the species with the largest likelihood!)\n",
    "\n",
    "Therefore, the *maximus* species will be our prediction for an elephant of this height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### More Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In fact, we also have elephant *weight* data available in addition to their heights. To accommodate multiple features we can make use of **multivariate normal** distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![multivariate-normal](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/MultivariateNormal.png/440px-MultivariateNormal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### What's \"Naive\" about This?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For multiple predictors, we make the simplifying assumption that **our predictors are probablistically independent**. This will often be unrealistic, but it simplifies our calculations a great deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "elephants = pd.read_csv('data/elephants.csv',\n",
    "                       usecols=['height (cm)', 'weight (lbs)', 'species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "elephants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "maximus = elephants[elephants['species'] == 'maximus']\n",
    "cyclotis = elephants[elephants['species'] == 'cyclotis']\n",
    "africana = elephants[elephants['species'] == 'africana']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose our new elephant with a height of 263 cm also has a weight of 7009 lbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "likeli_max = stats.multivariate_normal(mean=maximus.mean(),\n",
    "                          cov=maximus.cov()).pdf([263, 7009])\n",
    "likeli_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "likeli_cyc = stats.multivariate_normal(mean=cyclotis.mean(),\n",
    "                         cov=cyclotis.cov()).pdf([263, 7009])\n",
    "likeli_cyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "likeli_afr = stats.multivariate_normal(mean=africana.mean(),\n",
    "                         cov=africana.cov()).pdf([263, 7009])\n",
    "likeli_afr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "post_max = likeli_max / sum([likeli_max, likeli_cyc, likeli_afr])\n",
    "post_cyc = likeli_cyc / sum([likeli_max, likeli_cyc, likeli_afr])\n",
    "post_afr = likeli_afr / sum([likeli_max, likeli_cyc, likeli_afr])\n",
    "\n",
    "print(post_max)\n",
    "print(post_cyc)\n",
    "print(post_afr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### [`GaussianNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gnb = GaussianNB(priors=[1/3, 1/3, 1/3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = elephants.drop('species', axis=1)\n",
    "y = elephants['species']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gnb.predict_proba(np.array([263, 7009]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(gnb, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Comma Survey Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "commas = pd.read_csv('data/comma-survey.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "commas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first question on the survey was about the Oxford comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "commas.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll go ahead and drop the NaNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "commas = commas.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "commas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commas['In your opinion, which sentence is more gramatically correct?'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Personally, I like the Oxford comma, since it can help eliminate ambiguities, such as:\n",
    "\n",
    "\"This book is dedicated to my parents, Ayn Rand, and God\" <br/> vs. <br/>\n",
    "\"This book is dedicated to my parents, Ayn Rand and God\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's see how a Naive Bayes model would make a prediction here. We'll think of the comma preference as our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "commas['Age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose we want to make a prediction about Oxford comma usage for a new person who falls into the **45-60 age group**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Calculating Priors and Likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The following code makes a table of values that count up the number of survey respondents who fall into each of eight bins (the four age groups and the two answers to the first comma question). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "table = np.zeros((2, 4))\n",
    "\n",
    "for idx, value in enumerate(commas['Age'].value_counts().index):\n",
    "    table[0, idx] = len(commas[(commas['In your opinion, which sentence is '\\\n",
    "                                       'more gramatically correct?'] ==\\\n",
    "                                        'It\\'s important for a person to be '\\\n",
    "                                'honest, kind, and loyal.') & (commas['Age'] == value)])\n",
    "    table[1, idx] = len(commas[(commas['In your opinion, which sentence is '\\\n",
    "                                       'more gramatically correct?'] ==\\\n",
    "                                        'It\\'s important for a person to be '\\\n",
    "                                'honest, kind and loyal.') & (commas['Age'] == value)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(table, columns=['Age45-60',\n",
    "                            'Age>60',\n",
    "                            'Age30-44',\n",
    "                            'Age18-29'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['Oxford'] = [True, False]\n",
    "df = df[['Age>60', 'Age45-60', 'Age30-44', 'Age18-29', 'Oxford']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since all we have is a single categorical feature here we can just read our likelihoods and priors right off of this table:\n",
    "\n",
    "Likelihoods:\n",
    "\n",
    "- Age45-60:\n",
    "    - P(Age45-60 | Oxford=True) = $\\frac{123}{470} = 0.2617$;\n",
    "    - P(Age45-60 | Oxford=False) = $\\frac{125}{355} = 0.3521$.\n",
    "\n",
    "Priors:\n",
    "\n",
    "- P(Oxford=True) = $\\frac{470}{825} = 0.5697$;\n",
    "- P(Oxford=False) = $\\frac{355}{825} = 0.4303$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Calculating Posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First we'll calculate the probability of the evidence:\n",
    "\n",
    "$$\\begin{align} \n",
    "    P(Age45-60) &= P(Age45-60 | Oxford=True) \\cdot P(Oxford=True) \\\\\n",
    "                & \\hspace{1cm} + P(Age45-60 | Oxford=False) \\cdot P(Oxford=False)\\\\ \n",
    "                &= 0.2617 \\cdot 0.5697 + 0.3521 \\cdot 0.4303 \\\\\n",
    "                &= 0.3006\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(123+125)/825"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now use Bayes's Theorem to calculate the posteriors:\n",
    "\n",
    "$$\\begin{align}\n",
    "P(Oxford=True | Age45-60) &= P(Oxford=True) \\cdot P(Age45-60 | Oxford=True) / P(Age45-60) \\\\\n",
    "                          &= 0.5697 \\cdot 0.2617 / 0.3006 \\\\\n",
    "                          &= 0.4960 \\\\\n",
    "                          \\\\\n",
    "P(Oxford=False | Age45-60) &= P(Oxford=False) \\cdot P(Age45-60 | Oxford=False) / P(Age45-60) \\\\ \n",
    "                          &= 0.4303 \\cdot 0.3521 / 0.3006 \\\\\n",
    "                          &= 0.5040\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Close! But our prediction for someone in the 45-60 age group will be that they **do not** favor the Oxford comma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Comparison with [`MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comma_model = MultinomialNB()\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(commas['Age'].values.reshape(-1, 1))\n",
    "\n",
    "X = ohe.transform(commas['Age'].values.reshape(-1, 1)).todense()\n",
    "y = commas['In your opinion, which sentence is more gramatically correct?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comma_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comma_model.predict_proba(np.array([0, 0, 1, 0]).reshape(1, -1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "221px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
