{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helpful link: https://docs.google.com/presentation/d/1KkH7XixkMg0Kmnm-upzOHmrvEPcGznsGyiwhzJ9n-wo/edit#slide=id.g569e94c9c1_0_44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor,\\\n",
    "GradientBoostingClassifier #there is also a GBRegressor\n",
    "import xgboost  # You may need to install this!\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "SWBAT:\n",
    "\n",
    "- describe boosting algorithms;\n",
    "- implement boosting models with `sklearn` and with `XGBoost`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "One of the problems with using single decision trees and random forests is that, once I make a split, I can't go back and consider how another feature varies across the whole dataset. But suppose I were to consider **my tree's errors**. The fundamental idea of ***boosting*** is to start with a weak learner and then to use information about its errors to build a new model that can supplement the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Types\n",
    "\n",
    "The two main types of boosting available in Scikit-Learn are adaptive boosting (AdaBoostClassifier, AdaBoostRegressor) and gradient boosting (GradientBoostingClassifier, GradientBoostingRegressor).\n",
    "\n",
    "--gradient is the one that is typically used\n",
    "\n",
    "Again, the fundamental idea of boosting is to use a sequence of **weak** learners to build a model. Though the individual learners are weak, the idea is to train iteratively in order to produce a better predictor. More specifically, the first learner will be trained on the data as it stands, but future learners will be trained on modified versions of the data. The point of the modifications is to highlight the \"hard-to-predict-accurately\" portions of the data.\n",
    "\n",
    "\n",
    "--adaptive will weight the errors so that in the next sequence the error is hopefully lowered. Trying to get the model to focus on the previous mistakes.\n",
    "\n",
    "- **AdaBoost** works by iteratively adapting two related series of weights, one attached to the datapoints and the other attached to the learners themselves. Datapoints that are incorrectly classified receive greater weights for the next learner in the sequence. That way, future learners will be more likely to focus on those datapoints. At the end of the sequence, the learners that make better predictions, especially on the datapoints that are more resistant to correct classification, receive more weight in the final \"vote\" that determines the ensemble's prediction. <br/> Suppose we have a binary classification problem and we represent the two classes with 1 and -1. (This is standard for describing the algorithm of AdaBoost.) <br/>\n",
    "\n",
    "--When error is small, Theta is large\n",
    "\n",
    "--When error is large Theta approaches neg infinity\n",
    "\n",
    "--If Theta is 1/2, the weight is 0.\n",
    "\n",
    "Then, in a nutshell: <br/>\n",
    "    1. Train a weak learner. <br/>\n",
    "    2. Calculate its error $\\epsilon$. <br/>\n",
    "    3. Use that error as a weight on the classifier: $\\theta \n",
    "    = \\frac{1}{2}ln\\left(\\frac{1-\\epsilon}{\\epsilon}\\right)$. <br/>\n",
    "    Note that $\\theta$ CAN be negative. This represents a classifier whose accuracy is _worse_ than chance. <br/>\n",
    "    4. Use _that_ to adjust the data points' weights: $w_{n+1} = w_n\\left(\\frac{e^{\\pm\\theta}}{scaler}\\right)$. Use $+\\theta$ for incorrect predictions, $-\\theta$ for correct predictions. <br/>  $\\rightarrow$ For more detail on AdaBoost, see [here](https://towardsdatascience.com/boosting-algorithm-adaboost-b6737a9ee60c).\n",
    "\n",
    "\n",
    "-- start with simple model, predicting mean of target, the next model will take the predictions of 0 and takes the residuals of it into consiteration, and it gets cyclical and adds more models.\n",
    "\n",
    "- **Gradient Boosting** works instead by training each new learner on the residuals of the model built with the learners that have so far been constructed. That is, Model $n+1$ (with $n+1$ learners) will focus on the predictions of Model $n$ (with only $n$ learners) that were **most off the mark**. As the training process repeats, the learners learn and the residuals get smaller. I would get a sequence going: <br/> Model 0 is very simple. Perhaps it merely predicts the mean: <br/>\n",
    "$\\hat{y}_0 = \\bar{y}$; <br/>\n",
    "Model 1's predictions would then be the sum of (i) Model 0's predictions and (ii) the predictions of the model fitted to Model 0's residuals: <br/> $\\hat{y}_1 = \\hat{y}_0 + \\hat{(y - \\hat{y})}_{err0}$; <br/>\n",
    "Now iterate: Model 2's predictions will be the sum of (i) Model 0's predictions, (ii) the predictions of the model fitted to Model 0's residuals, and (iii) the predictions of the model fitted to Model 1's residuals: <br/> $\\hat{y}_2 = \\hat{y}_0 + \\hat{(y - \\hat{y})}_{err0} + \\hat{(y - \\hat{y})}_{err1}$<br/>\n",
    "Etc.\n",
    "<br/>\n",
    "\n",
    "$\\rightarrow$ How does gradient boosting work for a classification problem? How do we even make sense of the notion of a gradient in that context? The short answer is that we appeal to the probabilities associated with the predictions for the various classes. See more on this topic [here](https://sefiks.com/2018/10/29/a-step-by-step-gradient-boosting-example-for-classification/). <br/> $\\rightarrow$ Why is this called \"_gradient_ boosting\"? The short answer is that fitting a learner to a model's residuals comes to the same thing as fitting it to the derivative of that model's loss function. See more on this topic [here](https://www.ritchievink.com/blog/2018/11/19/algorithm-breakdown-why-do-we-call-it-gradient-boosting/).\n",
    "\n",
    "Let's illustrate gradient boosting now!\n",
    "\n",
    "## AdaBoost in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nr</th>\n",
       "      <th>Rmag</th>\n",
       "      <th>e.Rmag</th>\n",
       "      <th>ApDRmag</th>\n",
       "      <th>mumax</th>\n",
       "      <th>Mcz</th>\n",
       "      <th>e.Mcz</th>\n",
       "      <th>MCzml</th>\n",
       "      <th>chi2red</th>\n",
       "      <th>UjMAG</th>\n",
       "      <th>...</th>\n",
       "      <th>UFS</th>\n",
       "      <th>e.UFS</th>\n",
       "      <th>BFS</th>\n",
       "      <th>e.BFS</th>\n",
       "      <th>VFD</th>\n",
       "      <th>e.VFD</th>\n",
       "      <th>RFS</th>\n",
       "      <th>e.RFS</th>\n",
       "      <th>IFD</th>\n",
       "      <th>e.IFD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>24.995</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.935</td>\n",
       "      <td>24.214</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.036</td>\n",
       "      <td>1.400</td>\n",
       "      <td>0.64</td>\n",
       "      <td>-17.67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01870</td>\n",
       "      <td>0.00239</td>\n",
       "      <td>0.01630</td>\n",
       "      <td>0.00129</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.00141</td>\n",
       "      <td>0.01650</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.02470</td>\n",
       "      <td>0.00483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>25.013</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>25.303</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-18.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00706</td>\n",
       "      <td>0.00238</td>\n",
       "      <td>0.00420</td>\n",
       "      <td>0.00115</td>\n",
       "      <td>0.003930</td>\n",
       "      <td>0.00182</td>\n",
       "      <td>0.00723</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.00973</td>\n",
       "      <td>0.00460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>24.246</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.821</td>\n",
       "      <td>23.511</td>\n",
       "      <td>1.202</td>\n",
       "      <td>0.037</td>\n",
       "      <td>1.217</td>\n",
       "      <td>0.92</td>\n",
       "      <td>-19.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01260</td>\n",
       "      <td>0.00184</td>\n",
       "      <td>0.01830</td>\n",
       "      <td>0.00115</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.00167</td>\n",
       "      <td>0.02880</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.05700</td>\n",
       "      <td>0.00465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>25.203</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.639</td>\n",
       "      <td>24.948</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-17.83</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01410</td>\n",
       "      <td>0.00186</td>\n",
       "      <td>0.01180</td>\n",
       "      <td>0.00110</td>\n",
       "      <td>0.009670</td>\n",
       "      <td>0.00204</td>\n",
       "      <td>0.01050</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.00330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>25.504</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-1.588</td>\n",
       "      <td>24.934</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.067</td>\n",
       "      <td>1.330</td>\n",
       "      <td>1.45</td>\n",
       "      <td>-17.69</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00514</td>\n",
       "      <td>0.00170</td>\n",
       "      <td>0.00102</td>\n",
       "      <td>0.00127</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.00160</td>\n",
       "      <td>0.00139</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.00590</td>\n",
       "      <td>0.00444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Nr    Rmag  e.Rmag  ApDRmag   mumax    Mcz  e.Mcz  MCzml  chi2red  UjMAG  \\\n",
       "0   6  24.995   0.097    0.935  24.214  0.832  0.036  1.400     0.64 -17.67   \n",
       "1   9  25.013   0.181   -0.135  25.303  0.927  0.122  0.864     0.41 -18.28   \n",
       "2  16  24.246   0.054    0.821  23.511  1.202  0.037  1.217     0.92 -19.75   \n",
       "3  21  25.203   0.128    0.639  24.948  0.912  0.177  0.776     0.39 -17.83   \n",
       "4  26  25.504   0.112   -1.588  24.934  0.848  0.067  1.330     1.45 -17.69   \n",
       "\n",
       "   ...      UFS    e.UFS      BFS    e.BFS       VFD    e.VFD      RFS  \\\n",
       "0  ...  0.01870  0.00239  0.01630  0.00129  0.017300  0.00141  0.01650   \n",
       "1  ...  0.00706  0.00238  0.00420  0.00115  0.003930  0.00182  0.00723   \n",
       "2  ...  0.01260  0.00184  0.01830  0.00115  0.018800  0.00167  0.02880   \n",
       "3  ...  0.01410  0.00186  0.01180  0.00110  0.009670  0.00204  0.01050   \n",
       "4  ...  0.00514  0.00170  0.00102  0.00127  0.000039  0.00160  0.00139   \n",
       "\n",
       "      e.RFS      IFD    e.IFD  \n",
       "0  0.000434  0.02470  0.00483  \n",
       "1  0.000500  0.00973  0.00460  \n",
       "2  0.000655  0.05700  0.00465  \n",
       "3  0.000416  0.01340  0.00330  \n",
       "4  0.000499  0.00590  0.00444  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "galaxies = pd.read_csv('COMBO17.csv')\n",
    "galaxies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a dataset about galaxies. The Mcz and MCzml columns are measures of redshift, which is our target. Mcz is usually understood to be a better measure, so that will be our target column. Many of the other columns have to do with various measures of galaxies' magnitudes. For more on the dataset, see [here](https://astrostatistics.psu.edu/datasets/COMBO17.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Nr', 'Rmag', 'e.Rmag', 'ApDRmag', 'mumax', 'Mcz', 'e.Mcz', 'MCzml',\n",
       "       'chi2red', 'UjMAG', 'e.UjMAG', 'BjMAG', 'e.BjMAG', 'VjMAG', 'e.VjMAG',\n",
       "       'usMAG', 'e.usMAG', 'gsMAG', 'e.gsMAG', 'rsMAG', 'e.rsMAG', 'UbMAG',\n",
       "       'e.UbMAG', 'BbMAG', 'e.BbMAG', 'VnMAG', 'e.VbMAG', 'S280MAG',\n",
       "       'e.S280MA', 'W420FE', 'e.W420FE', 'W462FE', 'e.W462FE', 'W485FD',\n",
       "       'e.W485FD', 'W518FE', 'e.W518FE', 'W571FS', 'e.W571FS', 'W604FE',\n",
       "       'e.W604FE', 'W646FD', 'e.W646FD', 'W696FE', 'e.W696FE', 'W753FE',\n",
       "       'e.W753FE', 'W815FS', 'e.W815FS', 'W856FD', 'e.W856FD', 'W914FD',\n",
       "       'e.W914FD', 'W914FE', 'e.W914FE', 'UFS', 'e.UFS', 'BFS', 'e.BFS', 'VFD',\n",
       "       'e.VFD', 'RFS', 'e.RFS', 'IFD', 'e.IFD'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "galaxies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "galaxies.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3462 entries, 0 to 3461\n",
      "Data columns (total 65 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Nr        3462 non-null   int64  \n",
      " 1   Rmag      3462 non-null   float64\n",
      " 2   e.Rmag    3462 non-null   float64\n",
      " 3   ApDRmag   3462 non-null   float64\n",
      " 4   mumax     3462 non-null   float64\n",
      " 5   Mcz       3462 non-null   float64\n",
      " 6   e.Mcz     3462 non-null   float64\n",
      " 7   MCzml     3462 non-null   float64\n",
      " 8   chi2red   3462 non-null   float64\n",
      " 9   UjMAG     3462 non-null   float64\n",
      " 10  e.UjMAG   3462 non-null   float64\n",
      " 11  BjMAG     3462 non-null   float64\n",
      " 12  e.BjMAG   3462 non-null   float64\n",
      " 13  VjMAG     3462 non-null   float64\n",
      " 14  e.VjMAG   3462 non-null   float64\n",
      " 15  usMAG     3462 non-null   float64\n",
      " 16  e.usMAG   3462 non-null   float64\n",
      " 17  gsMAG     3462 non-null   float64\n",
      " 18  e.gsMAG   3462 non-null   float64\n",
      " 19  rsMAG     3462 non-null   float64\n",
      " 20  e.rsMAG   3462 non-null   float64\n",
      " 21  UbMAG     3462 non-null   float64\n",
      " 22  e.UbMAG   3462 non-null   float64\n",
      " 23  BbMAG     3462 non-null   float64\n",
      " 24  e.BbMAG   3462 non-null   float64\n",
      " 25  VnMAG     3461 non-null   float64\n",
      " 26  e.VbMAG   3461 non-null   float64\n",
      " 27  S280MAG   3438 non-null   float64\n",
      " 28  e.S280MA  3438 non-null   float64\n",
      " 29  W420FE    3462 non-null   float64\n",
      " 30  e.W420FE  3462 non-null   object \n",
      " 31  W462FE    3462 non-null   float64\n",
      " 32  e.W462FE  3462 non-null   float64\n",
      " 33  W485FD    3462 non-null   float64\n",
      " 34  e.W485FD  3462 non-null   float64\n",
      " 35  W518FE    3462 non-null   float64\n",
      " 36  e.W518FE  3462 non-null   float64\n",
      " 37  W571FS    3462 non-null   float64\n",
      " 38  e.W571FS  3462 non-null   float64\n",
      " 39  W604FE    3462 non-null   float64\n",
      " 40  e.W604FE  3462 non-null   float64\n",
      " 41  W646FD    3462 non-null   float64\n",
      " 42  e.W646FD  3462 non-null   float64\n",
      " 43  W696FE    3462 non-null   float64\n",
      " 44  e.W696FE  3462 non-null   float64\n",
      " 45  W753FE    3462 non-null   float64\n",
      " 46  e.W753FE  3462 non-null   float64\n",
      " 47  W815FS    3462 non-null   float64\n",
      " 48  e.W815FS  3462 non-null   float64\n",
      " 49  W856FD    3462 non-null   float64\n",
      " 50  e.W856FD  3462 non-null   float64\n",
      " 51  W914FD    3462 non-null   float64\n",
      " 52  e.W914FD  3462 non-null   float64\n",
      " 53  W914FE    3462 non-null   float64\n",
      " 54  e.W914FE  3462 non-null   float64\n",
      " 55  UFS       3462 non-null   float64\n",
      " 56  e.UFS     3462 non-null   float64\n",
      " 57  BFS       3462 non-null   float64\n",
      " 58  e.BFS     3462 non-null   float64\n",
      " 59  VFD       3462 non-null   float64\n",
      " 60  e.VFD     3462 non-null   float64\n",
      " 61  RFS       3462 non-null   float64\n",
      " 62  e.RFS     3462 non-null   float64\n",
      " 63  IFD       3462 non-null   float64\n",
      " 64  e.IFD     3462 non-null   float64\n",
      "dtypes: float64(63), int64(1), object(1)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "galaxies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies = galaxies.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect together the columns that have high correlation with Mcz, our target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for ind in galaxies.corr()['Mcz'].index:\n",
    "    if abs(galaxies.corr()['Mcz'][ind]) > 0.5:\n",
    "        preds.append(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mcz</th>\n",
       "      <th>e.Mcz</th>\n",
       "      <th>MCzml</th>\n",
       "      <th>UjMAG</th>\n",
       "      <th>BjMAG</th>\n",
       "      <th>VjMAG</th>\n",
       "      <th>usMAG</th>\n",
       "      <th>gsMAG</th>\n",
       "      <th>rsMAG</th>\n",
       "      <th>UbMAG</th>\n",
       "      <th>BbMAG</th>\n",
       "      <th>VnMAG</th>\n",
       "      <th>S280MAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mcz</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.613071</td>\n",
       "      <td>0.872016</td>\n",
       "      <td>-0.699550</td>\n",
       "      <td>-0.628226</td>\n",
       "      <td>-0.641983</td>\n",
       "      <td>-0.698304</td>\n",
       "      <td>-0.652170</td>\n",
       "      <td>-0.631398</td>\n",
       "      <td>-0.699731</td>\n",
       "      <td>-0.659506</td>\n",
       "      <td>-0.641870</td>\n",
       "      <td>-0.700453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e.Mcz</th>\n",
       "      <td>0.613071</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.590179</td>\n",
       "      <td>-0.170296</td>\n",
       "      <td>-0.122309</td>\n",
       "      <td>-0.122616</td>\n",
       "      <td>-0.168326</td>\n",
       "      <td>-0.126491</td>\n",
       "      <td>-0.114022</td>\n",
       "      <td>-0.170208</td>\n",
       "      <td>-0.130680</td>\n",
       "      <td>-0.122546</td>\n",
       "      <td>-0.169344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCzml</th>\n",
       "      <td>0.872016</td>\n",
       "      <td>0.590179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.565663</td>\n",
       "      <td>-0.520006</td>\n",
       "      <td>-0.532880</td>\n",
       "      <td>-0.563591</td>\n",
       "      <td>-0.536750</td>\n",
       "      <td>-0.525495</td>\n",
       "      <td>-0.565406</td>\n",
       "      <td>-0.544325</td>\n",
       "      <td>-0.532837</td>\n",
       "      <td>-0.562474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UjMAG</th>\n",
       "      <td>-0.699550</td>\n",
       "      <td>-0.170296</td>\n",
       "      <td>-0.565663</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933252</td>\n",
       "      <td>0.961088</td>\n",
       "      <td>0.999865</td>\n",
       "      <td>0.970124</td>\n",
       "      <td>0.954853</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>0.975476</td>\n",
       "      <td>0.960981</td>\n",
       "      <td>0.963415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BjMAG</th>\n",
       "      <td>-0.628226</td>\n",
       "      <td>-0.122309</td>\n",
       "      <td>-0.520006</td>\n",
       "      <td>0.933252</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950728</td>\n",
       "      <td>0.932328</td>\n",
       "      <td>0.954006</td>\n",
       "      <td>0.947597</td>\n",
       "      <td>0.932831</td>\n",
       "      <td>0.957836</td>\n",
       "      <td>0.950694</td>\n",
       "      <td>0.885382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VjMAG</th>\n",
       "      <td>-0.641983</td>\n",
       "      <td>-0.122616</td>\n",
       "      <td>-0.532880</td>\n",
       "      <td>0.961088</td>\n",
       "      <td>0.950728</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959432</td>\n",
       "      <td>0.997956</td>\n",
       "      <td>0.999324</td>\n",
       "      <td>0.960265</td>\n",
       "      <td>0.992178</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.894171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usMAG</th>\n",
       "      <td>-0.698304</td>\n",
       "      <td>-0.168326</td>\n",
       "      <td>-0.563591</td>\n",
       "      <td>0.999865</td>\n",
       "      <td>0.932328</td>\n",
       "      <td>0.959432</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968971</td>\n",
       "      <td>0.953027</td>\n",
       "      <td>0.999903</td>\n",
       "      <td>0.974515</td>\n",
       "      <td>0.959320</td>\n",
       "      <td>0.964972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gsMAG</th>\n",
       "      <td>-0.652170</td>\n",
       "      <td>-0.126491</td>\n",
       "      <td>-0.536750</td>\n",
       "      <td>0.970124</td>\n",
       "      <td>0.954006</td>\n",
       "      <td>0.997956</td>\n",
       "      <td>0.968971</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995436</td>\n",
       "      <td>0.969562</td>\n",
       "      <td>0.995887</td>\n",
       "      <td>0.997923</td>\n",
       "      <td>0.914118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rsMAG</th>\n",
       "      <td>-0.631398</td>\n",
       "      <td>-0.114022</td>\n",
       "      <td>-0.525495</td>\n",
       "      <td>0.954853</td>\n",
       "      <td>0.947597</td>\n",
       "      <td>0.999324</td>\n",
       "      <td>0.953027</td>\n",
       "      <td>0.995436</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953909</td>\n",
       "      <td>0.988695</td>\n",
       "      <td>0.999341</td>\n",
       "      <td>0.882782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UbMAG</th>\n",
       "      <td>-0.699731</td>\n",
       "      <td>-0.170208</td>\n",
       "      <td>-0.565406</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>0.932831</td>\n",
       "      <td>0.960265</td>\n",
       "      <td>0.999903</td>\n",
       "      <td>0.969562</td>\n",
       "      <td>0.953909</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975035</td>\n",
       "      <td>0.960155</td>\n",
       "      <td>0.964189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BbMAG</th>\n",
       "      <td>-0.659506</td>\n",
       "      <td>-0.130680</td>\n",
       "      <td>-0.544325</td>\n",
       "      <td>0.975476</td>\n",
       "      <td>0.957836</td>\n",
       "      <td>0.992178</td>\n",
       "      <td>0.974515</td>\n",
       "      <td>0.995887</td>\n",
       "      <td>0.988695</td>\n",
       "      <td>0.975035</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992127</td>\n",
       "      <td>0.925047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VnMAG</th>\n",
       "      <td>-0.641870</td>\n",
       "      <td>-0.122546</td>\n",
       "      <td>-0.532837</td>\n",
       "      <td>0.960981</td>\n",
       "      <td>0.950694</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.959320</td>\n",
       "      <td>0.997923</td>\n",
       "      <td>0.999341</td>\n",
       "      <td>0.960155</td>\n",
       "      <td>0.992127</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.893972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S280MAG</th>\n",
       "      <td>-0.700453</td>\n",
       "      <td>-0.169344</td>\n",
       "      <td>-0.562474</td>\n",
       "      <td>0.963415</td>\n",
       "      <td>0.885382</td>\n",
       "      <td>0.894171</td>\n",
       "      <td>0.964972</td>\n",
       "      <td>0.914118</td>\n",
       "      <td>0.882782</td>\n",
       "      <td>0.964189</td>\n",
       "      <td>0.925047</td>\n",
       "      <td>0.893972</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Mcz     e.Mcz     MCzml     UjMAG     BjMAG     VjMAG     usMAG  \\\n",
       "Mcz      1.000000  0.613071  0.872016 -0.699550 -0.628226 -0.641983 -0.698304   \n",
       "e.Mcz    0.613071  1.000000  0.590179 -0.170296 -0.122309 -0.122616 -0.168326   \n",
       "MCzml    0.872016  0.590179  1.000000 -0.565663 -0.520006 -0.532880 -0.563591   \n",
       "UjMAG   -0.699550 -0.170296 -0.565663  1.000000  0.933252  0.961088  0.999865   \n",
       "BjMAG   -0.628226 -0.122309 -0.520006  0.933252  1.000000  0.950728  0.932328   \n",
       "VjMAG   -0.641983 -0.122616 -0.532880  0.961088  0.950728  1.000000  0.959432   \n",
       "usMAG   -0.698304 -0.168326 -0.563591  0.999865  0.932328  0.959432  1.000000   \n",
       "gsMAG   -0.652170 -0.126491 -0.536750  0.970124  0.954006  0.997956  0.968971   \n",
       "rsMAG   -0.631398 -0.114022 -0.525495  0.954853  0.947597  0.999324  0.953027   \n",
       "UbMAG   -0.699731 -0.170208 -0.565406  0.999986  0.932831  0.960265  0.999903   \n",
       "BbMAG   -0.659506 -0.130680 -0.544325  0.975476  0.957836  0.992178  0.974515   \n",
       "VnMAG   -0.641870 -0.122546 -0.532837  0.960981  0.950694  0.999997  0.959320   \n",
       "S280MAG -0.700453 -0.169344 -0.562474  0.963415  0.885382  0.894171  0.964972   \n",
       "\n",
       "            gsMAG     rsMAG     UbMAG     BbMAG     VnMAG   S280MAG  \n",
       "Mcz     -0.652170 -0.631398 -0.699731 -0.659506 -0.641870 -0.700453  \n",
       "e.Mcz   -0.126491 -0.114022 -0.170208 -0.130680 -0.122546 -0.169344  \n",
       "MCzml   -0.536750 -0.525495 -0.565406 -0.544325 -0.532837 -0.562474  \n",
       "UjMAG    0.970124  0.954853  0.999986  0.975476  0.960981  0.963415  \n",
       "BjMAG    0.954006  0.947597  0.932831  0.957836  0.950694  0.885382  \n",
       "VjMAG    0.997956  0.999324  0.960265  0.992178  0.999997  0.894171  \n",
       "usMAG    0.968971  0.953027  0.999903  0.974515  0.959320  0.964972  \n",
       "gsMAG    1.000000  0.995436  0.969562  0.995887  0.997923  0.914118  \n",
       "rsMAG    0.995436  1.000000  0.953909  0.988695  0.999341  0.882782  \n",
       "UbMAG    0.969562  0.953909  1.000000  0.975035  0.960155  0.964189  \n",
       "BbMAG    0.995887  0.988695  0.975035  1.000000  0.992127  0.925047  \n",
       "VnMAG    0.997923  0.999341  0.960155  0.992127  1.000000  0.893972  \n",
       "S280MAG  0.914118  0.882782  0.964189  0.925047  0.893972  1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "galaxies[preds].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These various magnitude columns all have high correlations **with one another**! Let's try a simple model with just the S280MAG column, since it has the highest correlation with Mcz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = galaxies['S280MAG']\n",
    "y = galaxies['Mcz']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only have one predictor, we can visualize the correlation with the target! We can also reshape it for modeling purposes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rev = x.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mpl.pyplot.scatter(x_rev, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_rev, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostRegressor(random_state=42)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abr = AdaBoostRegressor(random_state=42)\n",
    "\n",
    "abr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> These are r2 scores. We are back into linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50416011, 0.51277109, 0.58225908, 0.47066606, 0.58004547])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(abr, x_test, y_test, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Let's see if we can do better by trying different hyperparameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator=abr,\n",
    "                 param_grid={\n",
    "                     'n_estimators': [25, 50, 100],\n",
    "                     'loss': ['linear', 'square']\n",
    "                 }, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=AdaBoostRegressor(random_state=42),\n",
       "             param_grid={'loss': ['linear', 'square'],\n",
       "                         'n_estimators': [25, 50, 100]})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 'linear', 'n_estimators': 25}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=42,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_boost = xgboost.XGBRegressor(random_state=42, objective='reg:squarederror')\n",
    "\n",
    "grad_boost.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.38073713, 0.31875796, 0.43157919, 0.39840844, 0.52101417])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(grad_boost, x_test, y_test, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression or Classification?\n",
    "\n",
    "What does my target look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "galaxies['Mcz'].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a bit of a bimodal shape here. We might therefore try predicting whether the redshift factor is likely to be greater or less than 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies['bool'] = galaxies['Mcz'] > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nr</th>\n",
       "      <th>Rmag</th>\n",
       "      <th>e.Rmag</th>\n",
       "      <th>ApDRmag</th>\n",
       "      <th>mumax</th>\n",
       "      <th>Mcz</th>\n",
       "      <th>e.Mcz</th>\n",
       "      <th>MCzml</th>\n",
       "      <th>chi2red</th>\n",
       "      <th>UjMAG</th>\n",
       "      <th>...</th>\n",
       "      <th>e.UFS</th>\n",
       "      <th>BFS</th>\n",
       "      <th>e.BFS</th>\n",
       "      <th>VFD</th>\n",
       "      <th>e.VFD</th>\n",
       "      <th>RFS</th>\n",
       "      <th>e.RFS</th>\n",
       "      <th>IFD</th>\n",
       "      <th>e.IFD</th>\n",
       "      <th>bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3457</th>\n",
       "      <td>9990</td>\n",
       "      <td>24.962</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>25.189</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.89</td>\n",
       "      <td>-18.21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00170</td>\n",
       "      <td>0.00361</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.00489</td>\n",
       "      <td>0.00147</td>\n",
       "      <td>0.00625</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.00987</td>\n",
       "      <td>0.00323</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3458</th>\n",
       "      <td>9992</td>\n",
       "      <td>21.918</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>23.063</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-20.47</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00218</td>\n",
       "      <td>0.04500</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.05130</td>\n",
       "      <td>0.00173</td>\n",
       "      <td>0.07210</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.10200</td>\n",
       "      <td>0.00477</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3459</th>\n",
       "      <td>9995</td>\n",
       "      <td>23.701</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>24.053</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.121</td>\n",
       "      <td>1.330</td>\n",
       "      <td>0.60</td>\n",
       "      <td>-18.76</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00223</td>\n",
       "      <td>0.01850</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.01450</td>\n",
       "      <td>0.00182</td>\n",
       "      <td>0.01580</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.00484</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3460</th>\n",
       "      <td>9996</td>\n",
       "      <td>23.473</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-1.114</td>\n",
       "      <td>25.075</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.870</td>\n",
       "      <td>1.01</td>\n",
       "      <td>-19.67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00225</td>\n",
       "      <td>0.00809</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>0.01140</td>\n",
       "      <td>0.00166</td>\n",
       "      <td>0.01070</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.01930</td>\n",
       "      <td>0.00390</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3461</th>\n",
       "      <td>9997</td>\n",
       "      <td>25.621</td>\n",
       "      <td>0.298</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>25.488</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.957</td>\n",
       "      <td>1.13</td>\n",
       "      <td>-17.70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00165</td>\n",
       "      <td>0.00315</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.00247</td>\n",
       "      <td>0.00131</td>\n",
       "      <td>0.00317</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.00746</td>\n",
       "      <td>0.00415</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Nr    Rmag  e.Rmag  ApDRmag   mumax    Mcz  e.Mcz  MCzml  chi2red  \\\n",
       "3457  9990  24.962   0.186   -0.113  25.189  0.960  0.190  0.951     0.89   \n",
       "3458  9992  21.918   0.017   -0.562  23.063  0.770  0.031  0.766     0.90   \n",
       "3459  9995  23.701   0.051   -0.437  24.053  0.775  0.121  1.330     0.60   \n",
       "3460  9996  23.473   0.098   -1.114  25.075  0.926  0.087  0.870     1.01   \n",
       "3461  9997  25.621   0.298   -0.224  25.488  0.968  0.139  0.957     1.13   \n",
       "\n",
       "      UjMAG  ...    e.UFS      BFS     e.BFS      VFD    e.VFD      RFS  \\\n",
       "3457 -18.21  ...  0.00170  0.00361  0.001150  0.00489  0.00147  0.00625   \n",
       "3458 -20.47  ...  0.00218  0.04500  0.001310  0.05130  0.00173  0.07210   \n",
       "3459 -18.76  ...  0.00223  0.01850  0.001090  0.01450  0.00182  0.01580   \n",
       "3460 -19.67  ...  0.00225  0.00809  0.001190  0.01140  0.00166  0.01070   \n",
       "3461 -17.70  ...  0.00165  0.00315  0.000949  0.00247  0.00131  0.00317   \n",
       "\n",
       "         e.RFS      IFD    e.IFD  bool  \n",
       "3457  0.000413  0.00987  0.00323  True  \n",
       "3458  0.000542  0.10200  0.00477  True  \n",
       "3459  0.000468  0.01860  0.00484  True  \n",
       "3460  0.000454  0.01930  0.00390  True  \n",
       "3461  0.000426  0.00746  0.00415  True  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "galaxies.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(x_rev, galaxies['bool'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(random_state=42)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "abc.fit(x_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9081395348837209"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc.score(x_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9250720461095101"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test2, abc.predict(x_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9596412556053812"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test2, abc.predict(x_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(random_state=42)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "gbc.fit(x_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9011627906976745"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc.score(x_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9089635854341737"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test2, gbc.predict(x_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9701046337817638"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test2, gbc.predict(x_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[126,  65],\n",
       "       [ 20, 649]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test2, gbc.predict(x_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_boost_class = xgboost.XGBClassifier(random_state=42, objective='binary:logistic')\n",
    "\n",
    "grad_boost_class.fit(x_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9034883720930232"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_boost_class.score(x_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
