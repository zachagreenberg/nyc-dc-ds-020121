{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Agenda\" data-toc-modified-id=\"Agenda-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Agenda</a></span></li><li><span><a href=\"#Preparing-Data\" data-toc-modified-id=\"Preparing-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Preparing Data</a></span></li><li><span><a href=\"#Fitting-a-Line-to-$Logit(target)$\" data-toc-modified-id=\"Fitting-a-Line-to-$Logit(target)$-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Fitting a Line to $Logit(target)$</a></span><ul class=\"toc-item\"><li><span><a href=\"#sklearn.linear_model.LogisticRegression()\" data-toc-modified-id=\"sklearn.linear_model.LogisticRegression()-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span><code>sklearn.linear_model.LogisticRegression()</code></a></span></li><li><span><a href=\"#.predict()-vs.-.predict_proba()\" data-toc-modified-id=\".predict()-vs.-.predict_proba()-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span><code>.predict()</code> vs. <code>.predict_proba()</code></a></span></li><li><span><a href=\"#Cost-Functions-and-Solutions-to-the-Optimization-Problem\" data-toc-modified-id=\"Cost-Functions-and-Solutions-to-the-Optimization-Problem-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Cost Functions and Solutions to the Optimization Problem</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-bad-news:\" data-toc-modified-id=\"The-bad-news:-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>The bad news:</a></span></li><li><span><a href=\"#The-good-news:\" data-toc-modified-id=\"The-good-news:-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>The good news:</a></span></li></ul></li><li><span><a href=\"#Interpreting-Logistic-Regression-Coefficients\" data-toc-modified-id=\"Interpreting-Logistic-Regression-Coefficients-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Interpreting Logistic Regression Coefficients</a></span><ul class=\"toc-item\"><li><span><a href=\"#Use-Coefficients-to-Generate-Prediction\" data-toc-modified-id=\"Use-Coefficients-to-Generate-Prediction-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Use Coefficients to Generate Prediction</a></span></li></ul></li></ul></li><li><span><a href=\"#Exercise\" data-toc-modified-id=\"Exercise-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Exercise</a></span></li><li><span><a href=\"#Level-Up\" data-toc-modified-id=\"Level-Up-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Level Up</a></span><ul class=\"toc-item\"><li><span><a href=\"#More-Generalizations:-Other-Link-Functions,-Other-Models\" data-toc-modified-id=\"More-Generalizations:-Other-Link-Functions,-Other-Models-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>More Generalizations: Other Link Functions, Other Models</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For our modeling steps\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# For demonstrative pruposes\n",
    "from scipy.special import logit, expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "SWBAT:\n",
    "\n",
    "- Explain the form of logistic regression\n",
    "- Explain how to interpret logistic regression coefficients\n",
    "- Use logistic regression to perform a classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ri</th>\n",
       "      <th>na</th>\n",
       "      <th>mg</th>\n",
       "      <th>al</th>\n",
       "      <th>si</th>\n",
       "      <th>k</th>\n",
       "      <th>ca</th>\n",
       "      <th>ba</th>\n",
       "      <th>fe</th>\n",
       "      <th>glass_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.51966</td>\n",
       "      <td>14.77</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.29</td>\n",
       "      <td>72.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1.51115</td>\n",
       "      <td>17.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>75.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.52213</td>\n",
       "      <td>14.21</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0.47</td>\n",
       "      <td>71.77</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.52213</td>\n",
       "      <td>14.21</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0.47</td>\n",
       "      <td>71.77</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.52320</td>\n",
       "      <td>13.72</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.51</td>\n",
       "      <td>71.75</td>\n",
       "      <td>0.09</td>\n",
       "      <td>10.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ri     na    mg    al     si     k     ca   ba    fe  glass_type\n",
       "id                                                                        \n",
       "22   1.51966  14.77  3.75  0.29  72.02  0.03   9.00  0.0  0.00           1\n",
       "185  1.51115  17.38  0.00  0.34  75.41  0.00   6.65  0.0  0.00           6\n",
       "40   1.52213  14.21  3.82  0.47  71.77  0.11   9.57  0.0  0.00           1\n",
       "39   1.52213  14.21  3.82  0.47  71.77  0.11   9.57  0.0  0.00           1\n",
       "51   1.52320  13.72  3.72  0.51  71.75  0.09  10.06  0.0  0.16           1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glass identification dataset\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data'\n",
    "col_names = ['id','ri','na','mg','al','si','k','ca','ba','fe','glass_type']\n",
    "glass = pd.read_csv(url, names=col_names, index_col='id')\n",
    "glass.sort_values('al', inplace=True)\n",
    "glass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ri</th>\n",
       "      <th>na</th>\n",
       "      <th>mg</th>\n",
       "      <th>al</th>\n",
       "      <th>si</th>\n",
       "      <th>k</th>\n",
       "      <th>ca</th>\n",
       "      <th>ba</th>\n",
       "      <th>fe</th>\n",
       "      <th>glass_type</th>\n",
       "      <th>household</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.51966</td>\n",
       "      <td>14.77</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.29</td>\n",
       "      <td>72.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1.51115</td>\n",
       "      <td>17.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>75.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.52213</td>\n",
       "      <td>14.21</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0.47</td>\n",
       "      <td>71.77</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.52213</td>\n",
       "      <td>14.21</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0.47</td>\n",
       "      <td>71.77</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.52320</td>\n",
       "      <td>13.72</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.51</td>\n",
       "      <td>71.75</td>\n",
       "      <td>0.09</td>\n",
       "      <td>10.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ri     na    mg    al     si     k     ca   ba    fe  glass_type  \\\n",
       "id                                                                           \n",
       "22   1.51966  14.77  3.75  0.29  72.02  0.03   9.00  0.0  0.00           1   \n",
       "185  1.51115  17.38  0.00  0.34  75.41  0.00   6.65  0.0  0.00           6   \n",
       "40   1.52213  14.21  3.82  0.47  71.77  0.11   9.57  0.0  0.00           1   \n",
       "39   1.52213  14.21  3.82  0.47  71.77  0.11   9.57  0.0  0.00           1   \n",
       "51   1.52320  13.72  3.72  0.51  71.75  0.09  10.06  0.0  0.16           1   \n",
       "\n",
       "     household  \n",
       "id              \n",
       "22           0  \n",
       "185          1  \n",
       "40           0  \n",
       "39           0  \n",
       "51           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# types 1, 2, 3 are window glass\n",
    "# types 5, 6, 7 are household glass\n",
    "glass['household'] = glass.glass_type.map({1:0, 2:0, 3:0, 5:1, 6:1, 7:1})\n",
    "glass.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Fitting a Line to $Logit(target)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's try applying the logit function to our target and then fitting a linear regression to that. Since the model will be trained not on whether the glass is household but rather on *the logit of this label*, it will also make predictions of the logit of that label. But we can simply apply the sigmoid function to the model's output to get its predictions of whether the glass is household.\n",
    "\n",
    "We can't use the target as is, because the logit of 1 is $\\infty$ and the logit of 0 is $-\\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glass['household'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-inf,  inf])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit(glass['household']).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So we'll make a small adjustment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target_approx = np.where(glass['household'] == 0, 1e-9, 1-1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_to_logit = LinearRegression()\n",
    "\n",
    "X = glass[['al']]\n",
    "y = logit(target_approx)\n",
    "\n",
    "line_to_logit.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "final_preds = expit(line_to_logit.predict(X))\n",
    "ax.scatter(X, glass['household'])\n",
    "ax.plot(X, final_preds, 'm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## `sklearn.linear_model.LogisticRegression()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In general, we should always scale our data when using this class. Scaling is always important for models that include regularization, and scikit-learn's `LogisticRegression()` objects have regularization by default.\n",
    "\n",
    "Here we've forgone the scaling since we only have a single predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# fit a logistic regression model and store the class predictions\n",
    "\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "feature_cols = ['al']\n",
    "X = glass[feature_cols]\n",
    "y = glass.household\n",
    "logreg.fit(X, y)\n",
    "glass['household_pred_class'] = logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plot the class predictions\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(glass.al, glass.household)\n",
    "ax.plot(glass.al, glass.household_pred_class, color='red')\n",
    "ax.set_xlabel('al')\n",
    "ax.set_ylabel('household');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## `.predict()` vs. `.predict_proba()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's checkout some specific examples to make predictions with. We'll use both `predict()` and `predict_proba()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glass.al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examine some example predictions\n",
    "\n",
    "print(logreg.predict(glass['al'][22].reshape(1, -1)))\n",
    "print(logreg.predict(glass['al'][185].reshape(1, -1)))\n",
    "print(logreg.predict(glass['al'][164].reshape(1, -1)))\n",
    "print('\\n')\n",
    "print(logreg.predict_proba(glass['al'][22].reshape(1, -1))[0])\n",
    "print(logreg.predict_proba(glass['al'][185].reshape(1, -1))[0])\n",
    "print(logreg.predict_proba(glass['al'][164].reshape(1, -1))[0])\n",
    "first_row = glass['al'][22].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# store the predicted probabilites of class 1\n",
    "glass['household_pred_prob'] = logreg.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plot the predicted probabilities\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(glass.al, glass.household)\n",
    "ax.plot(glass.al, glass.household_pred_prob, color='red')\n",
    "ax.set_xlabel('al')\n",
    "ax.set_ylabel('household');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first column indicates the predicted probability of **class 0**, and the second column indicates the predicted probability of **class 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_loss(glass.household, logreg.predict_proba(X)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The above is a pretty good score. A baseline classifier that is fit on data with equal numbers of data points in the two target classes should be right about 50% of the time, and the log loss for such a classifier would be $-ln(0.5) = 0.693$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "-np.log(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Cost Functions and Solutions to the Optimization Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Unlike the least-squares problem for linear regression, no one has yet found a closed-form solution to the optimization problem presented by logistic regression. But even if one exists, the computation would no doubt be so complex that we'd be better off using some sort of approximation method instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "But there's still a problem.\n",
    "\n",
    "Recall the cost function for linear regression: <br/><br/>\n",
    "$SSE = \\Sigma_i(y_i - \\hat{y}_i)^2 = \\Sigma_i(y_i - (\\beta_0 + \\beta_1x_{i1} + ... + \\beta_nx_{in}))^2$.\n",
    "\n",
    "This function, $SSE(\\vec{\\beta})$, is convex.\n",
    "\n",
    "If we plug in our new logistic equation for $\\hat{y}$, we get: <br/><br/>\n",
    "$SSE_{log} = \\Sigma_i(y_i - \\hat{y}_i)^2 = \\Sigma_i\\left(y_i - \\left(\\frac{1}{1+e^{-(\\beta_0 + \\beta_1x_{i1} + ... + \\beta_nx_{in})}}\\right)\\right)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### The bad news:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "*This* function, $SSE_{log}(\\vec{\\beta})$, is [**not** convex](https://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c).\n",
    "\n",
    "That means that, if we tried to use gradient descent or some other approximation method that looks for the minimum of this function, we could easily find a local rather than a global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note that the scikit-learn class *expects the user to specify the solver* to be used in calculating the coefficients. The default solver, [lbfgs](https://en.wikipedia.org/wiki/Limited-memory_BFGS), works well for many applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### The good news:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can use **log-loss** instead:\n",
    "\n",
    "$\\mathcal{L}(\\vec{y}, \\hat{\\vec{y}}) = -\\frac{1}{N}\\Sigma^N_{i=1}\\left(y_iln(\\hat{y}_i)+(1-y_i)ln(1-\\hat{y}_i)\\right)$,\n",
    "\n",
    "where $\\hat{y}_i$ is the probability that $(x_{i1}, ... , x_{in})$ belongs to **class 1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**More resources on the log-loss function**:\n",
    "\n",
    "https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11\n",
    "\n",
    "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
    "\n",
    "http://wiki.fast.ai/index.php/Log_Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Interpreting Logistic Regression Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How do we interpret the coefficients of a logistic regression? For a linear regression, the situaton was like this:\n",
    "\n",
    "- Linear Regression: We construct the best-fit line and get a set of coefficients. Suppose $\\beta_1 = k$. In that case we would expect a 1-unit change in $x_1$ to produce a $k$-unit change in $y$.\n",
    "\n",
    "- Logistic Regression: We find the coefficients of the best-fit line by some approximation method. Suppose $\\beta_1 = k$. In that case we would expect a 1-unit change in $x_1$ to produce a $k$-unit change (not in $y$ but) in $ln\\left(\\frac{y}{1-y}\\right)$.\n",
    "\n",
    "We have:\n",
    "\n",
    "$\\ln\\left(\\frac{y(x_1+1, ... , x_n)}{1-y(x_1+1, ... , x_n)}\\right) = \\ln\\left(\\frac{y(x_1, ... , x_n)}{1-y(x_1, ... , x_n)}\\right) + k$.\n",
    "\n",
    "Exponentiating both sides:\n",
    "\n",
    "$\\frac{y(x_1+1, ... , x_n)}{1-y(x_1+1, ... , x_n)} = e^{\\ln\\left(\\frac{y(x_1, ... , x_n)}{1-y(x_1, ... , x_n)}\\right) + k}$ <br/><br/> $\\frac{y(x_1+1, ... , x_n)}{1-y(x_1+1, ... , x_n)}= e^{\\ln\\left(\\frac{y(x_1, ... , x_n)}{1-y(x_1, ... , x_n)}\\right)}\\cdot e^k$ <br/><br/> $\\frac{y(x_1+1, ... , x_n)}{1-y(x_1+1, ... , x_n)}= e^k\\cdot\\frac{y(x_1, ... , x_n)}{1-y(x_1, ... , x_n)}$\n",
    "\n",
    "That is, the odds ratio at $x_1+1$ has increased by a factor of $e^k$ relative to the odds ratio at $x_1$.\n",
    "\n",
    "For more on interpretation, see [this page](https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/how-to/binary-logistic-regression/interpret-the-results/all-statistics-and-graphs/coefficients/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# examine the intercept\n",
    "\n",
    "logreg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Interpretation:** For an 'al' value of 0, the log-odds of 'household' is -6.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert log-odds to probability\n",
    "\n",
    "logodds = logreg.intercept_\n",
    "odds = np.exp(logodds)\n",
    "prob = odds / (1 + odds)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# examine the coefficient for al\n",
    "\n",
    "list(zip(feature_cols, logreg.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Interpretation:** A 1 unit increase in 'al' is associated with a 3.12 unit increase in the log-odds of 'household'.\n",
    "\n",
    "Let's verify this as we change the aluminum content from 1 to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Prediction for al=1\n",
    "\n",
    "pred_al1 = logreg.predict_proba([[1]])\n",
    "pred_al1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Odds ratio for al=1\n",
    "\n",
    "odds_al1 = pred_al1[0][1] / pred_al1[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Prediction for al=2\n",
    "pred_al2 = logreg.predict_proba([[2]])\n",
    "pred_al2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Odds ratio for al=2\n",
    "\n",
    "odds_al2 = pred_al2[0][1] / pred_al2[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print((np.exp(logreg.coef_[0]) * odds_al1)[0])\n",
    "print(odds_al2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Use Coefficients to Generate Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# compute predicted log-odds for al=2 using the equation\n",
    "\n",
    "logodds = logreg.intercept_ + logreg.coef_[0] * 2\n",
    "logodds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# convert log-odds to odds\n",
    "\n",
    "odds = np.exp(logodds)\n",
    "odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# convert odds to probability\n",
    "\n",
    "prob = odds / (1 + odds)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# compute predicted probability for al=2 using the predict_proba method\n",
    "\n",
    "logreg.predict_proba(np.array([2]).reshape(1, 1))[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Split the data below into train and test, and then convert the y-values (`geysers.kind`) into 1's and 0's. Then use `sklearn` to build a logistic regression model of whether Old Faithful's eruption wait time is long or short, based on the duration of the eruption. Finally, find the points in the test set where the model's prediction differs from the true y-value. How many are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "geysers = sns.load_dataset('geyser', **{'usecols': ['duration', 'kind']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "geysers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Level Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## More Generalizations: Other Link Functions, Other Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Logistic regression's link function is the logit function, but different sorts of models use different link functions.\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function) has a nice table of generalized linear model types and their associated link functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
