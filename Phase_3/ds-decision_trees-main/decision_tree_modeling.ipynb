{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "SWBAT:\n",
    "\n",
    "- Describe the decision tree modeling algorithm;\n",
    "- Use attribute selection methods to build different trees;\n",
    "- Explain the pros and cons of decision trees;\n",
    "- Interpret the feature importances of a fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees at a High Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/titanic_tree.png' width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is a machine learning model that works by partitioning our sample space in a hierarchical way.\n",
    "\n",
    "How do we partition the space? The key idea is that some attributes provide more information than others when trying to make a decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trump lawsuit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/trump_train.csv',\n",
    "                 usecols=['dateFiled', 'type', 'issue'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning\n",
    "\n",
    "I partition my data by asking a question about the independent variables. The goal is to ask the right questions in the right order so that the resultant groups are \"pure\" with respect to the dependent variable. Let's say that the dependent variable in this case is whether the case is type 1 (criminal) or type 2 (civil).\n",
    "\n",
    "Suppose, for example, that I choose:\n",
    "\n",
    "#### Was the case filed before October 2018?\n",
    "\n",
    "This would divide my data into two groups:\n",
    "\n",
    "- Group 1 (filed before):\n",
    "\n",
    "data points: 0, 1, 4, 5\n",
    "\n",
    "- Group 2 (filed after):\n",
    "\n",
    "data points: 2, 3\n",
    "\n",
    "#### Key Question: How are the values of the target distributed in this group?\n",
    "\n",
    "In Group 1, I have: civil, criminal, civil, civil\n",
    "\n",
    "In Group 2, I have: criminal, civil\n",
    "\n",
    "This seems like an ok split:\n",
    "\n",
    "  - The first group has a majority of civil cases (3/4), so it suggests that earlier cases are unlikely to be criminal.\n",
    "  - But the second group has one of each type, so that doesn't give us much information.\n",
    "\n",
    "Would a different question split our data more effectively? Let's try:\n",
    "\n",
    "#### Does the case concern the Mueller investigation?\n",
    "\n",
    "Let's look at our splits:\n",
    "\n",
    "- Group 1 (Mueller cases):\n",
    "\n",
    "data points: 0, 1, 2\n",
    "\n",
    "- Group 2 (Other cases):\n",
    "\n",
    "data points: 3, 4, 5\n",
    "\n",
    "#### Key Question again: Distribution of target\n",
    "\n",
    "In Group 1, I have: civil, criminal, criminal\n",
    "\n",
    "In Group 2, I have: civil, civil, civil\n",
    "\n",
    "That seems better:\n",
    "\n",
    "  - Group 2 is a pure group! All cases outside the Mueller investigation were civil cases.\n",
    "  - Group 1 has two criminal cases and only one civil case.\n",
    "  \n",
    "A (very simple!) model that predicts \"criminal\" for Mueller cases and \"civil\" otherwise would correctly classify five of these six data points!\n",
    "\n",
    "But how would my partition be *best* split? And how do I really know that the second split is better than the first? Can I do better than intuition here?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy/Information Gain and Gini\n",
    "\n",
    "The goal is to have our ultimate classes be fully \"ordered\" (for a binary dependent variable, we'd have the 1's in one group and the 0's in the other). So one way to assess the value of a split is to measure how *disordered* our groups are, and there is a notion of *entropy* that measures precisely this.\n",
    "\n",
    "The entropy of the whole dataset is given by:\n",
    "\n",
    "$\\large E = -\\Sigma^n_i p_i\\log_2(p_i)$,\n",
    "\n",
    "where $p_i$ is the probability of belonging to the $i$th group, where $n$ is the number of groups (i.e. target values).\n",
    "\n",
    "**Entropy will always be between 0 and 1. The closer to 1, the more disordered your group.**\n",
    "\n",
    "<img src='./img/Entropy_mapped.png' width=600/>\n",
    "\n",
    "In the present case we have only two groups of interest: criminal and civil.\n",
    "\n",
    "Four out of six were civil and two out of six were criminal, so **these are the relevant probabilities** for our calculation of entropy.\n",
    "\n",
    "So our entropy for the sample above is:\n",
    "\n",
    "$-\\frac{2}{3}*\\log_2\\left(\\frac{2}{3}\\right) - \\frac{1}{3}*\\log_2\\left(\\frac{1}{3}\\right)$.\n",
    "\n",
    "Let's use ``numpy's`` `log2()` function to calculate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-(2/3) * np.log2(2/3) - (1/3) * np.log2(1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a high level of disorder!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy of a Split\n",
    "\n",
    "To calculate the entropy of a *split*, we're going to want to calculate the entropy of each of the groups made by the split, and then calculate a weighted average of those groups' entropies––weighted, that is, by the size of the groups. Let's calculate the entropy of the split produced by our \"was the case filed before October 2018?\" question:\n",
    "\n",
    "Group 1 (filed before): civil, criminal, civil, civil\n",
    "\n",
    "$E_{g1} = -\\frac{3}{4} * \\log_2(\\frac{3}{4}) - \\frac{1}{4} * \\log_2(\\frac{1}{4})$. \n",
    "\n",
    "Group 2 (filed after): criminal, civil\n",
    "\n",
    "$E_{g2} = -\\frac{1}{2} * \\log_2\\left(\\frac{1}{2}\\right) - \\frac{1}{2} * \\log_2\\left(\\frac{1}{2}\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_before = -(3/4)*np.log2(3/4) - (1/4)*np.log2(1/4)\n",
    "print(ent_before)\n",
    "\n",
    "ent_after = -(1/2)*np.log2(1/2) - (1/2)*np.log2(1/2)\n",
    "print(ent_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now weight those by the probability of each group, and sum them, to find the entropy of the split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_norm = (4/6) * ent_before\n",
    "after_norm = (2/6) * ent_after\n",
    "\n",
    "E_split_d = before_norm + after_norm\n",
    "E_split_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great. Compare that to the Mueller question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Group 1: civil, criminal, criminal\n",
    "\n",
    "ent_mueller = -(2/3)*np.log2(2/3) - (1/3)*np.log2(1/3)\n",
    "print(ent_mueller)\n",
    "\n",
    "# In Group 2: civil, civil, civil\n",
    "\n",
    "ent_other = -(3/3)*np.log2(3/3) # Pure group!\n",
    "print(ent_other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mueller_norm = ent_mueller * 3/6\n",
    "other_norm = ent_other * 3/6\n",
    "\n",
    "E_split_m = mueller_norm + other_norm\n",
    "E_split_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given split, the **information gain** is simply the entropy of the parent group less the entropy of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_entropy_sample = -(4/6)*np.log2(4/6) - (2/6)*np.log2(2/6)\n",
    "\n",
    "\n",
    "# Information gain: before or after October 2018\n",
    "\n",
    "ig_d = total_entropy_sample - E_split_d\n",
    "print(f\"Information gain for Oct. 2018 split: {ig_d}\")\n",
    "\n",
    "# Information gain: Mueller-related or not\n",
    "\n",
    "ig_m = total_entropy_sample - E_split_m\n",
    "print(f\"Information gain for Mueller split: {ig_m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given parent, then, we maximize our model's performance by *minimizing* the split's entropy.\n",
    "\n",
    "What we'd like to do then is:\n",
    "\n",
    "1. to look at the entropies of all possible splits, and\n",
    "2. to choose the split with the lowest entropy.\n",
    "\n",
    "In practice there are far too many splits for it to be practical for a person to calculate all these different entropies ...\n",
    "\n",
    "... but we can make computers do these calculations for us!\n",
    "\n",
    "Moreover, we can **iterate** this algorithm on the resultant groups until we reach pure groups!\n",
    "\n",
    "**Question**: Are we in fact guaranteed, proceeding in this way, to reach pure groups, no matter what our data looks like?\n",
    "\n",
    "**Observation**: This algorithm looks for the best split **locally**. There is no regard for how an overall tree might look. That's what makes this algorithm ***greedy***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Impurity\n",
    "\n",
    "An alternative metric to entropy comes from the work of Corrado Gini. The Gini Impurity is defined as:\n",
    "\n",
    "$\\large G = 1 - \\Sigma_ip_i^2$, or, equivalently, $\\large G = \\Sigma_ip_i(1-p_i)$.\n",
    "\n",
    "where, again, $p_i$ is the probability of belonging to the $i$th group.\n",
    "\n",
    "**Gini Impurity will always be between 0 and 0.5. The closer to 0.5, the more disordered your group.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "We can also use decision trees for regression problems. How does the regressor work? Information gain and Gini impurity make sense only for discrete classes. What we need for a continuous target is some measure of the *spread* of those values, and so the natural move here is to split our groups in ways that will most *reduce the standard deviation of those groups*. For more on this, see [here](https://www.saedsayad.com/decision_tree_reg.htm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will turn dates into numbers!\n",
    "\n",
    "df['dateFiled'] = pd.to_datetime(df['dateFiled']).map(lambda x: x.date().toordinal())\n",
    "\n",
    "# And this will convert the issue to numbers:\n",
    "\n",
    "df['issue'] = df['issue'].map(lambda x: 1 if x == 'Mueller investigation' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "X = df.drop('type', axis=1)\n",
    "y = df.type\n",
    "dtree = dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(dt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# April 7, 2018 is halfway between the points that differ in type\n",
    "\n",
    "datetime.fromordinal(736791)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Terminology related to Decision Trees\n",
    "\n",
    "- **Root Node:** Represents entire population or sample.\n",
    "- **Decision Node:** Node that is split.\n",
    "- **Leaf/ Terminal Node:** Node with no children.\n",
    "- **Pruning:** Removing nodes.\n",
    "- **Branch / Sub-Tree:** A sub-section of a decision tree.\n",
    "- **Parent and Child Node:** A node divided into sub-nodes is the parent; the sub-nodes are its children.\n",
    "\n",
    "<img src='./img/decision_leaf.webp' width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges with Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees are prone to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data/trump-lawsuits.csv',\n",
    "                     usecols=['dateFiled', 'type', 'issue']).iloc[[5, 17, 20, 25], :]\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on training data\n",
    "dt.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dates to numbers\n",
    "\n",
    "df_test['dateFiled'] = pd.to_datetime(df_test['dateFiled'])\\\n",
    ".map(lambda x: x.date().toordinal())\n",
    "\n",
    "# And this will convert the issue to numbers:\n",
    "\n",
    "df_test['issue'] = df_test['issue']\\\n",
    ".map(lambda x: 1 if x == 'Mueller investigation' else 0)\n",
    "\n",
    "X_test = df_test.drop('type', axis=1)\n",
    "y_test = df_test.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on test data\n",
    "dt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance with Decision Trees\n",
    "\n",
    "The CART algorithm will repeatedly partition data into smaller and smaller subsets until those final subsets are homogeneous in terms of the outcome variable. In practice this often means that the final subsets (known as the leaves of the tree) each consist of only one or a few data points. \n",
    "\n",
    "This tends to result in low-bias, high variance models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping Criterion - Pruning Parameters\n",
    "\n",
    "The recursive binary splitting procedure described above needs to know when to stop splitting as it works its way down the tree with the training data.\n",
    "\n",
    "**min_samples_leaf:**  The most common stopping procedure is to use a minimum count on the number of training instances assigned to each leaf node. If the count is less than some minimum then the split is not accepted and the node is taken as a final leaf node.\n",
    "\n",
    "**max_leaf_nodes:** \n",
    "Reduce the number of leaf nodes.\n",
    "\n",
    "**max_depth:**\n",
    "Reduce the depth of the tree to build a generalized tree.\n",
    "\n",
    "**min_impurity_split :**\n",
    "A node will split if its impurity is above the threshold, otherwise it will be a leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Importances\n",
    "\n",
    "The fitted tree has an attribute called `ct.feature_importances_`. What does this mean? Roughly, the importance (or \"Gini importance\") of a feature is a sort of weighted average of the impurity decrease at internal nodes that make use of the feature. The weighting comes from the number of samples that depend on the relevant nodes.\n",
    "\n",
    "> The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance. See [`sklearn`'s documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.feature_importances_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "dt.fit(X, y)\n",
    "\n",
    "for fi, feature in zip(dt.feature_importances_, X.columns):\n",
    "    print(fi, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on feature importances [here](https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "- The decision tree is a \"white-box\" type of ML algorithm. It shares internal decision-making logic, which is not available in the black-box type of algorithms such as Neural Network.\n",
    "- Its training time is faster compared to other algorithms such as neural networks.\n",
    "- The decision tree is a non-parametric method, which does not depend upon probability distribution assumptions.\n",
    "- Decision trees can handle high-dimensional data with good accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros\n",
    "\n",
    "Decision trees:\n",
    "- are easy to interpret and to visualize;\n",
    "- can easily capture non-linear patterns;\n",
    "- require little data preprocessing from the user. For example, there is no need to normalize columns;\n",
    "- can be used for feature engineering such as predicting missing values, suitable for variable selection;\n",
    "- make no assumptions about distribution, because of the non-parametric nature of the algorithm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cons\n",
    "\n",
    "Decision trees:\n",
    "- are sensitive to noisy data. This problem can be significantly ameliorated by ensemble methods.\n",
    "- produce high-bised models with imbalanced datasets (so it is recommended that you balance out the dataset before creating the decision tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
