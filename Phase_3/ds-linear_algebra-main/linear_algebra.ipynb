{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra for Data Scientists: Matrix Mathematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linalg](https://online-learning.harvard.edu/sites/default/files/styles/header/public/course/3295580_orig.jpg?itok=0Rdh9m8k)\n",
    "\n",
    "## Agenda\n",
    "\n",
    "SWBAT:\n",
    "\n",
    "- Use `numpy` to construct and manipulate scalars, vectors, and matrices;\n",
    "- Use `numpy` to construct and manipulate identity matrices and inverse matrices;\n",
    "- Use `numpy` to solve systems of equations;\n",
    "- Describe the matrix manipulations required to solve the best-fit line problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices are a fundamental aspect of data science models and problems, including image processing, deep learning, NLP, and PCA. You will encounter matrices *many* times in your career as a data scientist. Matrices are a fundamental tool in **linear algebra**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systems of Equations\n",
    "\n",
    "Linear algebra can be used to solve for multiple unknowns at once. Let's start with a one-variable \"system\" before moving on to two-, three-, or many-variable systems.\n",
    "\n",
    "Suppose we start with a one-variable system like $2X = 10$.\n",
    "\n",
    "How do we solve this?\n",
    "\n",
    "Now consider a two-variable system:\n",
    "\n",
    "$2X + 4Y = 10 \\\\\n",
    "X + 4Y = 7$\n",
    "\n",
    "### Solution through Substitution\n",
    "\n",
    "We _could_ solve this system by taking the first equation, solving it for X, and then plugging the result into the second:\n",
    "\n",
    "$2X + 4Y = 10$. <br/> Thus: $\\\\ 2X = 10 - 4Y \\\\ X = 5 - 2Y$.\n",
    "\n",
    "Plugging in to the second equation, we have:\n",
    "\n",
    "$5 - 2Y + 4Y = 7$. <br/> Thus: $\\\\ 5 + 2Y = 7 \\\\ 2Y = 2 \\\\ Y = 1$.\n",
    "\n",
    "Plugging this back into the first equation, we have:\n",
    "\n",
    "$2X + 4 = 10$.  <br/> Thus: $\\\\ 2X = 6 \\\\ X = 3$.\n",
    "\n",
    "And we have our solutions:  $X = 3, Y = 1$.\n",
    "\n",
    "But this is computationally *very slow*! There is a better way:\n",
    "\n",
    "### Solution through Elimination\n",
    "\n",
    "Much faster is to subtract the second equation from the first:\n",
    "\n",
    "If $2X + 4Y = 10$ and $X + 4Y = 7$,\n",
    "then $(2X - X) + (4Y - 4Y) = 10 - 7$, i.e. $X = 3$. Then I could subtract this ($X + 0Y = 3$) from $X + 4Y = 7$, yielding: $4Y = 4$, i.e. $Y = 1$.\n",
    "\n",
    "We can represent this in matrix form using the equations as our rows. The columns will correspond to the variables:\n",
    "\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "2 & 4 & 10 \\\\\n",
    "1 & 4 & 7\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\rightarrow \\begin{bmatrix}\n",
    "1 & 0 & 3 \\\\\n",
    "1 & 4 & 7\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\rightarrow \\begin{bmatrix}\n",
    "1 & 0 & 3 \\\\\n",
    "0 & 4 & 4\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\rightarrow \\begin{bmatrix}\n",
    "1 & 0 & 3 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "This is the matrix way of saying that X = 3 and that Y = 1.\n",
    "\n",
    "There are lots of strategies in linear algebra for \"reducing\" a matrix to a form where there are ones down the main diagonal and zeroes everywhere else (except the rightmost column), because such a matrix represents a list of \"already solved\" equations: <br/>\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1 & 0 & 0 & ... & 0 & b_1 \\\\\n",
    "0 & 1 & 0 & ... & 0 & b_2 \\\\\n",
    "... & ... & ... & ... & ... & ... \\\\\n",
    "... & ... & ... & ... & ... & ... \\\\\n",
    "... & ... & ... & ... & ... & ... \\\\\n",
    "0 & 0 & 0 & ... & 1 & b_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\; \\downarrow$\n",
    "\n",
    "$X_1 + 0X_2 + 0X_3 + ... + 0X_n = b_1 \\\\\n",
    "0X_1 + X_2 + 0X_3 + ... + 0X_n = b_2 \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "0X_1 + 0X_2 + ... + 0X_{n-1} + X_n = b_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Scalars to Vectors\n",
    "\n",
    "A _scalar_ has simply a single value. Any real number can be the value of a scalar.\n",
    "\n",
    "A _vector_ must be specified by _two_ parameters: magnitude and direction. In a Cartesian coordinate system, a vector $\\vec{v}$ will often be specified by the components defined by the coordinate system. In this way a vector can be embedded in a higher-dimensional space, which allows us to speak of vectors of any dimension (even though, as directed line segments, all vectors are, strictly speaking, one-dimensional).\n",
    "\n",
    "In a 2-d Cartesian space: <br/>\n",
    "- We can specify a vector $\\vec{v}$ as $v_x + v_y$.\n",
    "- The magnitude of $\\vec{v}$ is given by $||v|| = \\sqrt{v^2_x + v^2_y}$ <br/>\n",
    "- The direction of $\\vec{v}$ is given by $\\theta = tan^{-1}\\left(\\frac{v_y}{v_x}\\right)$\n",
    "\n",
    "For more on vectors, see this helpful [video](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Arithmetic\n",
    "\n",
    "#### Vector Addition\n",
    "\n",
    "Vector addition is simple: Just add the corresponding components together:\n",
    "\n",
    "$(8, 14) + (7, 6) = (15, 20)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the vectors (8, 14) and (7, 6). Let's try using Python\n",
    "# to add them together.\n",
    "\n",
    "vec_1 = (8, 14)\n",
    "vec_2 = (7, 6)\n",
    "\n",
    "vec_1 + vec_2 == (15, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Python is not particularly good for non-scalar arithmetic. Make a general practice of turning to `numpy` for mathematical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try this again, but this time we'll use NumPy arrays:\n",
    "\n",
    "vec_1, vec_2 = np.array([8, 14]), np.array([7, 6])\n",
    "vec_1 + vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact there are multiple ways of understanding the notion of vector multiplication. All are potentially useful, but the one that we'll likely be of most use is the *dot-product*, which is defined as follows:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "a & b \\\\\n",
    "\\end{bmatrix}\n",
    ". \n",
    "\\begin{bmatrix}\n",
    "c \\\\\n",
    "d\n",
    "\\end{bmatrix}\n",
    "=\n",
    "ac + bd\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot-product is the sum of the pariwise products of the vectors' entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out the different attributes and methods available to\n",
    "# a NumPy array.\n",
    "\n",
    "vec_1.\n",
    "\n",
    "# There are many options. Notice that one of these options is 'dot'.\n",
    "# This is our dot-product! So let's use the .dot() method to calculate\n",
    "# the dot-product of our two vectors:\n",
    "\n",
    "vec_1.dot(vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use '@'\n",
    "\n",
    "vec_1 @ vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher Dimensions: Matrices and Tensors\n",
    "\n",
    "For higher dimensions we can use _matrices_ to express ourselves. Suppose we had a two-variable system:\n",
    "\n",
    "\\begin{align}\n",
    "a_{1,1}x_1 + a_{1,2}x_2 = c_1 \\\\\n",
    "a_{2,1}x_1 + a_{2,2}x_2 = c_2\n",
    "\\end{align}\n",
    "\n",
    "Using matrices, we can write this as:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} \\\\\n",
    "a_{2,1} & a_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "c_2\n",
    "\\end{bmatrix}\n",
    "\\end{equation}$\n",
    "\n",
    "or\n",
    "\n",
    "$A\\vec{x} = \\vec{c}$,\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\vec{x}$ is the _vector_ $(x_1, x_2)$;\n",
    "- $\\vec{c}$ is the _vector_ $(c_1, c_2)$; and\n",
    "- $A$ is the _matrix_ of coefficients that describe our system:\n",
    "$\\begin{equation} A = \n",
    "\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} \\\\\n",
    "a_{2,1} & a_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition\n",
    "\n",
    "The addition of matrices is straightforward: Just add corresponding elements. In order to add two matrices $A$ and $B$, they must have the same number of rows and columns:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11} + b_{11} & a_{12} + b_{12} \\\\\n",
    "a_{21} + b_{21} & a_{22} + b_{22}\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[1, 2], [3, 4]]) + np.array([[4, 3], [2, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Ways to Multiply\n",
    "\n",
    "Just as there were different notions of \"multiplication\" for vectors, so too there are different notions of multiplication for matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dot-Product\n",
    "Very often when people talk about multiplying matrices they'll mean the dot-product:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} \\\\\n",
    "a_{2,1} & a_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "b_{1,1} & b_{1,2} \\\\\n",
    "b_{2,1} & b_{2,2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{1,1}\\times b_{1,1} + a_{1,2}\\times b_{2,1} & a_{1,1}\\times b_{1,2} + a_{1,2}\\times b_{2,2} \\\\\n",
    "a_{2,1}\\times b_{1,1} + a_{2,2}\\times b_{2,1} & a_{2,1}\\times b_{1,2} + a_{2,2}\\times b_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Take the entries in each *row* of the left matrix and multiply them, respectively, by the entries in each *column* of the right matrix, and then add them up. This is the product we calculated above with our two vectors!\n",
    "\n",
    "Note that matrix dot-multiplication is NOT commutative! In general, $AB \\neq BA$.\n",
    "\n",
    "##### A note about vectors and matrices\n",
    "\n",
    "Strictly speaking, this is true for vectors as well. Above, we multiplied the *row*-vector $(a, b)$ by the *column*-vector $(c, d)$. A row-vector is simply a matrix with only one row; a column-vector is simply a matrix with only one column. What would be the result of multiplying the column-vector $(c, d)$ on the left by the row-vector $(a, b)$ on the right?\n",
    "\n",
    "<details><summary>\n",
    "    Ans.:\n",
    "        </summary>\n",
    "    \\begin{equation}\n",
    "    \\begin{bmatrix}\n",
    "    c \\\\\n",
    "    d\n",
    "    \\end{bmatrix}\n",
    "    \\times\n",
    "    \\begin{bmatrix}\n",
    "    a & b\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "    ca & cb \\\\\n",
    "    da & db\n",
    "    \\end{bmatrix}\n",
    "    \\end{equation}\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "\n",
    "Illustrate this difference between $\\begin{bmatrix} a & b \\end{bmatrix}\\cdot\\begin{bmatrix} c \\\\ d \\end{bmatrix}$ and $\\begin{bmatrix} c \\\\ d \\end{bmatrix}\\cdot\\begin{bmatrix} a & b \\end{bmatrix}$ in Python for $a=2, b=3, c=4$, and $d=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Features of the Matrix Dot-Product\n",
    "\n",
    "Observe also that in order to be able to perform the dot product on two matrices A and B, **the number of columns of A must equal the number of rows of B**.\n",
    "\n",
    "Also, **the number of rows of the product matrix will equal the number of rows of A, and the number of columns of the product matrix will equal the number of columns of B**.\n",
    "\n",
    "In order to solve an equation like $A\\vec{x} = \\vec{c}$ for $\\vec{x}$, we can't very well divide $\\vec{c}$ by $A$! But there is a notion of matrix _inversion_ that is relevant here, which is analogous to multiplicative inversion. If we have an equation like $2x = 10$, we can simply multiply both sides by the multiplicative inverse of the coefficient of $x$, viz. $2^{-1}$. And here the point, of course, is that $2^{-1} \\times 2 = 1$.\n",
    "\n",
    "In the higher-dimensional case, what we can do is to left-multiply both sides by the _inverse matrix_ of A, denoted $A^{-1}$, and here the point is that the dot-product $A^{-1}A = I$, where $I$ is the **identity matrix** containing 1's along the main diagonal (upper-left to lower-right) and 0's everywhere else.\n",
    "\n",
    "The identity matrix is the multiplicative identity element for matrices in the sense that\n",
    "\n",
    "$AI = IA = A$ for any matrix $A$. For example:\n",
    "\n",
    "$\\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & j \\end{bmatrix}\\cdot\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & j \\end{bmatrix}$\n",
    "\n",
    "Thus when we have a matrix equation $A\\vec{x} = \\vec{c}$, we can calculate the solution by multiplying both sides by $A^{-1}$:\n",
    "\n",
    "$A^{-1}A\\vec{x} = A^{-1}\\vec{c}$; so <br/>\n",
    "$\\vec{x} = A^{-1}\\vec{c}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "\n",
    "You can produce the identity matrix in `numpy` by calling `np.eye()`.\n",
    "\n",
    "Using `numpy` arrays, dot-multiply the matrices\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "3 & 2 \\\\\n",
    "5 & 7\n",
    "\\end{bmatrix}$\n",
    "\n",
    "and\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "in the code-cell below. Remember that you need square brackets around the whole array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "\n",
    "You can produce the inverse of a matrix in `numpy` by calling `np.linalg.inv()`.\n",
    "\n",
    "Using `numpy` arrays, dot-multiply the matrices\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "3 & 2 \\\\\n",
    "5 & 7\n",
    "\\end{bmatrix}$\n",
    "\n",
    "and\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\frac{7}{11} & -\\frac{2}{11} \\\\\n",
    "-\\frac{5}{11} & \\frac{3}{11}\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "Sometimes you will encounter _tensors_ in your work. A tensor is to a matrix as a matrix is to a vector. A vector has one representational dimension and a matrix has two. If you need an object with three or more representational dimensions, you're talking about a tensor. A tensor has rows (that run from left to right), columns (that run from top to bottom), and _tubes_ (that run from front to back)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NumPy to Solve a System of Linear Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy's ```linalg``` module has a ```.solve()``` method that you can use to solve a system of linear equations!\n",
    "\n",
    "In particular, it will solve for the vector $\\vec{x}$ in the equation $A\\vec{x} = b$. You should know that, \"under the hood\", the ```.solve()``` method does NOT compute the inverse matrix $A^{-1}$. This is largely because of the enormous expense of directly computing a matrix inverse, which takes $\\mathcal{O}(n^3)$ time.\n",
    "\n",
    "Check out [this discussion](https://stackoverflow.com/questions/31256252/why-does-numpy-linalg-solve-offer-more-precise-matrix-inversions-than-numpy-li) on stackoverflow for more on the differences between using `.solve()` and `.inv()`.\n",
    "\n",
    "And check out the documentation for ```.solve()``` [here](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html).\n",
    "\n",
    "Suppose we have the following equations:\n",
    "\n",
    "$2x_1 - x_2 + 4x_3 + 6x_4 + 3x_5 = 3$\n",
    "\n",
    "$4x_1 + 7x_2 + x_3 + x_4 + 12x_5 = 15$\n",
    "\n",
    "$9x_1 + 14x_2 + 2x_3 + 2x_4 + 6x_5 = 20$\n",
    "\n",
    "$x_1 + x_2 + x_3 + 2x_4 + 17x_5 = 2$\n",
    "\n",
    "$-3x_1 - 2x_2 -6x_3 + 12x_4 - 5x_5 = -6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the .solve() method to solve this system of equations\n",
    "\n",
    "X = np.array([[2,  -1,  4,  6,  3],\n",
    "              [4,   7,  1,  1, 12],\n",
    "              [9,  14,  2,  2,  6],\n",
    "              [1,   1,  1,  2, 17],\n",
    "              [-3, -2, -6, 12, -5]])\n",
    "\n",
    "y = np.array([3, 15, 20, 2, -6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.solve(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we could just solve our matrix equation by calculating the inverse of our matrix $X$ and then multiplying by $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(X).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the time difference is striking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.linalg.inv(X).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.linalg.solve(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for a (tiny!) 5x5 matrix, the cost of computing the inverse directly is evident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical Data Science Problems\n",
    "\n",
    "Consider a typical dataset and the associated multiple linear regression problem. We have many observations (rows), each of which consists of a set of values both for the predictors (columns, i.e. the independent variables) and for the target (the dependent variable).\n",
    "\n",
    "We can think of the values of the independent variables as our matrix $A$ of coefficients and of the values of the dependent variable as our output vector $\\vec{c}$.\n",
    "\n",
    "The task here is, in effect, to solve for $\\vec{\\beta}$, where we have that $A\\vec{\\beta} = \\vec{c}$, except in general we'll have more rows than columns. This is why we won't in general be computing matrix inverses. (They're computationally expensive, anyway.) This is also why we have a problem requiring not a direct solution but rather an optimization--in our case, a best-fit line.\n",
    "\n",
    "Using $z$ for our independent variables and $y$ for our dependent variable, we have:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta_1\\begin{bmatrix}\n",
    "z_{1,1} \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "z_{m,1}\n",
    "\\end{bmatrix} +\n",
    "... + \\beta_n\\begin{bmatrix}\n",
    "z_{1,n} \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "z_{m,n}\n",
    "\\end{bmatrix} \\approx \\begin{bmatrix}\n",
    "y_1 \\\\\n",
    ".  \\\\\n",
    ".  \\\\\n",
    ".  \\\\\n",
    "y_m\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra Solves the Best-Fit Line Problem\n",
    "\n",
    "If we have a matrix of predictors $X$ and a target column $y$, we can express $\\hat{y}$, the best-fit line, as  follows:\n",
    "\n",
    "$\\large\\hat{y} = (X^TX)^{-1}X^Ty$.\n",
    "\n",
    "$(X^TX)^{-1}X^T$ is sometimes called the *pseudo-inverse* of $X$. We'll have more to say about this in a future lesson when we talk about the singular value decomposition.\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(list(zip(np.random.normal(size=10),\n",
    "                          np.array(np.random.normal(size=10, loc=2)))))\n",
    "target = np.array(np.random.exponential(size=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(preds.T.dot(preds)).dot(preds.T).dot(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression(fit_intercept=False).fit(preds, target).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
