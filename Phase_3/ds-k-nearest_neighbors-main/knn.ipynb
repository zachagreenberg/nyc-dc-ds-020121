{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, confusion_matrix,\\\n",
    "recall_score, precision_score, accuracy_score\n",
    "from src.confusion import plot_confusion_matrix\n",
    "from src.k_classify import predict_one\n",
    "from src.plot_train import *\n",
    "from src.euclid import *\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![wilson](img/wilson.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "SWBAT:\n",
    "\n",
    "- describe the $k$-nearest neighbors algorithm;\n",
    "- identify multiple common distance metrics;\n",
    "- tune $k$ appropriately in response to models with high bias or variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `sklearn.neighbors`\n",
    "\n",
    "$k$-Nearest Neighbors is a modeling technique that works for both regression and classification problems. Here we'll apply it to a version of the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('data/cleaned_titanic.csv')\n",
    "titanic = titanic.iloc[:, :-2]\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For visualization purposes, we will use only two features for our first model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic[['Age', 'Fare']]\n",
    "y = titanic['Survived']\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "This dataset of course presents a binary classification problem, with our target being the `Survived` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t, X_val, y_t, y_val = train_test_split(X_train, y_train,\n",
    "                                          random_state=42,\n",
    "                                          test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X_t, y_t)\n",
    "print(f\"training accuracy: {knn.score(X_t, y_t)}\")\n",
    "print(f\"validation accuracy: {knn.score(X_val, y_val)}\")\n",
    "\n",
    "y_hat = knn.predict(X_val)\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_val, y_hat), classes=['Perished', 'Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_viz = X_t.sample(15, random_state=40)\n",
    "y_for_viz = y_t[X_for_viz.index]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.scatterplot(X_for_viz['Age'], X_for_viz['Fare'], \n",
    "                hue=y_for_viz, palette={0: 'red', 1: 'green'}, \n",
    "                s=200, ax=ax)\n",
    "\n",
    "ax.set_xlim(0, 80)\n",
    "ax.set_ylim(0, 80)\n",
    "plt.legend()\n",
    "plt.title('Subsample of Training Data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $k$-NN algorithm works by simply storing the training set in memory, then measuring the distance from the training points to a new point.\n",
    "\n",
    "Let's drop a point from our validation set into the plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_viz = X_t.sample(15, random_state=40)\n",
    "y_for_viz = y_t[X_for_viz.index]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.scatterplot(X_for_viz['Age'], X_for_viz['Fare'],\n",
    "                hue=y_for_viz, palette={0: 'red', 1: 'green'},\n",
    "                s=200, ax=ax)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "#################^^^Old code^^^##############\n",
    "####################New code#################\n",
    "\n",
    "# Let's take one sample from our validation set and plot it\n",
    "new_x = pd.DataFrame(X_val.loc[484]).T\n",
    "new_y = y_val[new_x.index]\n",
    "\n",
    "sns.scatterplot(new_x['Age'], new_x['Fare'], color='blue',\n",
    "                s=200, ax=ax, label='New', marker='P')\n",
    "\n",
    "ax.set_xlim(0, 100)\n",
    "ax.set_ylim(0, 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, $k$-NN finds the $k$ nearest points. $k$ corresponds to the `n_neighbors` parameter defined when we instantiate the classifier object. **If $k$ = 1, then the prediction for a point will simply be the value of the target for the nearest point.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit our training data, then predict what our validation point will be based on the (one) closest neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_for_viz, y_for_viz)\n",
    "knn.predict(new_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When we raise the value of $k$, $k$-NN will act democratically: It will find the $k$ closest points, and take a vote based on the labels.**\n",
    "\n",
    "**Question**: How should $k$-NN work for a *regression* problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k=3$\n",
    "\n",
    "Let's raise $k$ to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn3 = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn3.fit(X_for_viz, y_for_viz)\n",
    "knn3.predict(new_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not easy to tell what which points are closest by eye.\n",
    "\n",
    "Let's update our plot to add indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_viz = X_t.sample(15, random_state=40)\n",
    "y_for_viz = y_t[X_for_viz.index]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.scatterplot(X_for_viz['Age'], X_for_viz['Fare'], hue=y_for_viz, \n",
    "                palette={0: 'red', 1: 'green'}, s=200, ax=ax)\n",
    "\n",
    "\n",
    "# Now let's take another sample\n",
    "\n",
    "# new_x = X_val.sample(1, random_state=33)\n",
    "new_x = pd.DataFrame(X_val.loc[484]).T\n",
    "new_x.columns = ['Age', 'Fare']\n",
    "new_y = y_val[new_x.index]\n",
    "\n",
    "print(new_x)\n",
    "sns.scatterplot(new_x['Age'], new_x['Fare'], color='blue', \n",
    "                s=200, ax=ax, label='New', marker='P')\n",
    "ax.set_xlim(0, 100)\n",
    "ax.set_ylim(0, 100)\n",
    "plt.legend()\n",
    "\n",
    "#################^^^Old code^^^##############\n",
    "####################New code#################\n",
    "\n",
    "# add annotations one by one with a loop\n",
    "for index in X_for_viz.index:\n",
    "    ax.text(X_for_viz.Age[index]+0.7, X_for_viz.Fare[index],\n",
    "            s=index, horizontalalignment='left', size='medium',\n",
    "            color='black', weight='semibold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `sklearn`'s NearestNeighors object to see the exact calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_viz = pd.merge(X_for_viz, y_for_viz, left_index=True, right_index=True)\n",
    "neighbor = NearestNeighbors(3)\n",
    "neighbor.fit(X_for_viz)\n",
    "nearest = neighbor.kneighbors(new_x)\n",
    "\n",
    "nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_viz.iloc[nearest[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(((29-24)**2 + (33-25.4667)**2)**0.5)\n",
    "print(((26-24)**2 + (16.1-25.4667)**2)**0.5)\n",
    "print(((20-24)**2 + (15.7417-25.4667)**2)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k=5$\n",
    "\n",
    "And with five neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_for_viz, y_for_viz)\n",
    "knn.predict(new_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's iterate through $k$, odd numbers 1 through 10, and see the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, 10, 2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_for_viz, y_for_viz)\n",
    "    print(knn.predict(new_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which models were correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "You may have suspected that we were leaving something out. For any distance-based algorithms, scaling is very important. Look at how the shape of the array changes before and after scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![non-normal](img/nonnormal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![normal](img/normalized.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our data_for_viz dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.25)\n",
    "X_t, X_val, y_t, y_val = train_test_split(X_train, y_train,\n",
    "                                          random_state=42,\n",
    "                                          test_size=0.25)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_ind = X_t.index\n",
    "X_col = X_t.columns\n",
    "\n",
    "X_t_s = pd.DataFrame(ss.fit_transform(X_t))\n",
    "X_t_s.index = X_ind\n",
    "X_t_s.columns = X_col\n",
    "\n",
    "X_v_ind = X_val.index\n",
    "X_val_s = pd.DataFrame(ss.transform(X_val))\n",
    "X_val_s.index = X_v_ind\n",
    "X_val_s.columns = X_col\n",
    "\n",
    "knn.fit(X_t_s, y_t)\n",
    "print(f\"training accuracy: {knn.score(X_t_s, y_t)}\")\n",
    "print(f\"Val accuracy: {knn.score(X_val_s, y_val)}\")\n",
    "\n",
    "y_hat = knn.predict(X_val_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The plot_train() function just does what we did above.\n",
    "\n",
    "plot_train(X_t, y_t, X_val, y_val)\n",
    "plot_train(X_t_s, y_t, X_val_s, y_val, -2, 2, text_pos=0.1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at how much that changes things.\n",
    "\n",
    "Look at points 166 and 150.  \n",
    "Look at the group 621, 143, and 191."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our classifier on scaled data and compare to unscaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.25)\n",
    "X_t, X_val, y_t, y_val = train_test_split(X_train, y_train,\n",
    "                                          random_state=42,\n",
    "                                          test_size=0.25)\n",
    "\n",
    "# The predict_one() function prints predictions on a given point\n",
    "# (#484) for k-nn models with k ranging from 1 to 10.\n",
    "\n",
    "predict_one(X_t, X_val, y_t, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MinMaxScaler()\n",
    "\n",
    "X_t_s = pd.DataFrame(mm.fit_transform(X_t))\n",
    "X_t_s.index = X_t.index\n",
    "X_t_s.columns = X_t.columns\n",
    "\n",
    "X_val_s = pd.DataFrame(mm.transform(X_val))\n",
    "X_val_s.index = X_val.index\n",
    "X_val_s.columns = X_val.columns\n",
    "\n",
    "\n",
    "predict_one(X_t_s, X_val_s, y_t, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Resources on Scaling\n",
    "\n",
    "https://sebastianraschka.com/Articles/2014_about_feature_scaling.html   \n",
    "http://datareality.blogspot.com/2016/11/scaling-normalizing-standardizing-which.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$ and the Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's slowly increase k and see what happens to our accuracy scores.\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "k_scores_train = {}\n",
    "k_scores_val = {}\n",
    "\n",
    "\n",
    "for k in range(1, 20):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    accuracy_score_t = []\n",
    "    accuracy_score_v = []\n",
    "    for train_ind, val_ind in kf.split(X_train, y_train):\n",
    "        \n",
    "        X_t, y_t = X_train.iloc[train_ind], y_train.iloc[train_ind] \n",
    "        X_v, y_v = X_train.iloc[val_ind], y_train.iloc[val_ind]\n",
    "        mm = MinMaxScaler()\n",
    "        \n",
    "        X_t_ind = X_t.index\n",
    "        X_v_ind = X_v.index\n",
    "        \n",
    "        X_t = pd.DataFrame(mm.fit_transform(X_t))\n",
    "        X_t.index = X_t_ind\n",
    "        X_v = pd.DataFrame(mm.transform(X_v))\n",
    "        X_v.index = X_v_ind\n",
    "        \n",
    "        knn.fit(X_t, y_t)\n",
    "        \n",
    "        y_pred_t = knn.predict(X_t)\n",
    "        y_pred_v = knn.predict(X_v)\n",
    "        \n",
    "        accuracy_score_t.append(accuracy_score(y_t, y_pred_t))\n",
    "        accuracy_score_v.append(accuracy_score(y_v, y_pred_v))\n",
    "        \n",
    "        \n",
    "    k_scores_train[k] = np.mean(accuracy_score_t)\n",
    "    k_scores_val[k] = np.mean(accuracy_score_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_scores_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "ax.plot(list(k_scores_train.keys()), list(k_scores_train.values()),\n",
    "        color='red', linestyle='dashed', marker='o',  \n",
    "         markerfacecolor='blue', markersize=10, label='Train')\n",
    "ax.plot(list(k_scores_val.keys()), list(k_scores_val.values()),\n",
    "        color='green', linestyle='dashed', marker='o',  \n",
    "         markerfacecolor='blue', markersize=10, label='Val')\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The relation between $k$ and bias/variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/K-NN_Neighborhood_Size_print.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MinMaxScaler()\n",
    "\n",
    "X_train_ind = X_train.index\n",
    "X_train = pd.DataFrame(mm.fit_transform(X_train))\n",
    "X_train.index = X_train_ind\n",
    "\n",
    "X_test_ind = X_test.index\n",
    "X_test =  pd.DataFrame(mm.transform(X_test))\n",
    "X_test.index = X_test_ind\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print(f\"training accuracy: {knn.score(X_train, y_train)}\")\n",
    "print(f\"Test accuracy: {knn.score(X_test, y_test)}\")\n",
    "\n",
    "y_hat = knn.predict(X_test)\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_test, y_hat), classes=['Perished', 'Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
