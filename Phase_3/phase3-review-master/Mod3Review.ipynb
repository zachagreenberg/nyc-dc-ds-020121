{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 Code Challenge Review\n",
    "\n",
    "Agenda:\n",
    "- Gradient Descent & Cost Function\n",
    "- Logistic Regression\n",
    "- Evaluation Metrics\n",
    "- Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Gradient Descent & Cost Function\n",
    "- What is a cost function? What are we trying to find?\n",
    "- How to use gd to find the lowest point? How does the gradient change as we get closer to the bottom?\n",
    "- What's the role learning rate play? How can learning rate affect your result? \n",
    "\n",
    "\n",
    "<p style='text-align:center;font-size:20px'>$ \\theta_j := \\theta_j - \\alpha * \\frac{\\partial J(\\theta)}{\\partial\\theta_i} $</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A cost function is an algorithm that helps us to determine the best fit line for a regression by minimizing the loss. We are systematically trying to find the line with the lowest amount of errors.\n",
    "\n",
    "> We can use gradient descent to find the lowest point because it takes into consideration the slope or derivative of a cost curve. We are ultimately trying to get the cost curve to descend to as close to 0 as possible. As the gradient gets closer to the bottom it approaches 0.\n",
    "\n",
    "> A learning rate is a constant that we establish to help us determine the appropriate step size as we try to find the minimum loss. Having a learning rate that is too large can make us overshoot and miss the minimum, whereas having a learning rate that is too small can stall our efforts to get to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "x = [1, 1, 2, 3, 4, 3, 4, 6, 4]\n",
    "y = [2, 1, 0.5, 1, 3, 3, 2, 5, 4]\n",
    "plt.figure(figsize = (8,8))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 = 0\n",
    "#beta_1 = [.25, .5, .75, .8, 1,]\n",
    "beta_1 = np.arange(0.25,3,0.1)\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "mses = []\n",
    "for t in beta_1:\n",
    "    line = beta_0 + (np.array(x)*t)\n",
    "    mse = round(mean_squared_error(y, line),3)\n",
    "    mses.append(mse)\n",
    "    ax.plot(x, line, label=f'{mse} {t}')\n",
    "ax.scatter(x,y)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the Cost Curve\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(beta_1, mses)\n",
    "ax.set_title('Cost Curve')\n",
    "ax.set_xlabel('beta 1')\n",
    "ax.set_ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For gradient descent, the questions are going to be mostly intuitive and written answers. You will need to be able to answer questions such as the 3 bullet points above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Logistic Regression \n",
    "- How does linear regression differ from logistic regression?\n",
    "- Why is logistic regression better at modeling a binary outcome?\n",
    "- What are some advantages and disadvantages of logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Linear Regression differs from Logistic Regression because the target variables are of different kinds. The target variable in a Linear Regression is continuous and the target variable in Logistic Regression is categorical. \n",
    "\n",
    "> A Logistic Regression is better at modeling a binary outcome because it utilizes the sigmoid function that creates an S shaped line that is better fit to the data than the line predicted from Linear Regression. \n",
    "\n",
    "> With Logistic Regression it can interpret the coefficients for feature importance by magnitude. A downside of this is that it is tough to interpret complex relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. Evaluation Metrics \n",
    "- What are precision and recall?\n",
    "- How to evaluate a logistic regression model?\n",
    "- What is roc auc curve?\n",
    "- What is class imbalance and how do we deal with it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Precision is the True Predicted Positives over the Total Positives Predicted. This is a good measure to use when you would concerned with the number of False Positives. A good example of this is crime conviction. You don't want to falsely accuse someone of a crime.\n",
    "\n",
    ">Recall is the True Predicted Positives over the Total True Positives. This is a good measure to use when you are concerned about the number of False Negatives. A good example of this is cancer detection. You don't want to tell someone they don't have cancer when they do.\n",
    "\n",
    ">There are other measures that you can use to evaluate Logistic Regression besides Precision and Recall, there is also Accuracy and F1-score. Which one you use is situational depending on the data and what you are trying to accomplish with it.\n",
    "\n",
    "> A ROC curve shows you all of the scenarios of the thresholds for the classification data. The thresholds determine at what point you consider something negative or positive. Again, what you choose is situational depending on your goal, however in general the best model is often the one closest to the top left corner because it maximizes the True Positives and minimizes the False Positives.\n",
    "\n",
    "> A class imbalance is when the classes you are trying to predict are uneven. This can bias your machine because of the lack of data on the unbalances side. An easy way to correct for this is Oversampling the minority class. This will create more data for that side and even out the two classes. Often a better way to do this is with SMOTE. This creates artificial data to balance it all out. This is a better option because replicating the data would lead to possible bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'confusion_matrix.png' width = 300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8076923076923077\n"
     ]
    }
   ],
   "source": [
    "### calculate precision here\n",
    "TP = 63\n",
    "TP_FP = (63 + 15)\n",
    "Precision = TP/TP_FP\n",
    "print(Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7411764705882353\n"
     ]
    }
   ],
   "source": [
    "### calculate recall here\n",
    "TP_FN = (63 + 22)\n",
    "Recall = TP/TP_FN\n",
    "print(Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7730061349693251\n"
     ]
    }
   ],
   "source": [
    "### calculate F1 score here\n",
    "F1 = (2 * Precision * Recall) / (Precision + Recall)\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain which model below has the best performance based on ROC-AUC curve? Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='roc_auc.png' width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model below, it depends on the measure that you are seeking to determine what your best model is. In general, for accuracy, the green model is the best. It is the closest to the top left corner maximizing True Positives and minimizing False Positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n",
    "<img src = 'imbalanced.png' wid = 300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### what problem would it cause? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine would lean towards the majority in an unbalanced dataset. It would have low predicting power for the target=1 result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How to remedy it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many solutions. We can resample by taking smaller samples of the majority class, taking larger samples (ie copies) of the minority class, or we can use SMOTE and Tomek Link. SMOTE creates new samples for the minority class and Tomek eliminates points from the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1 - Resampling\n",
    "<img src = 'resampling.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 2 - Smote\n",
    "<img src = 'smote.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 3 - Tomek Link \n",
    "<img src = 'tomek.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. Decision Trees\n",
    "- Build trees with the sklearn machine learning framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import the dataset and set up predictors and target\n",
    "import seaborn as sns\n",
    "titanic = sns.load_dataset('titanic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define x and y \n",
    "y = titanic['survived']\n",
    "X = titanic[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
    "     'adult_male']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pclass          0\n",
       "sex             0\n",
       "age           177\n",
       "sibsp           0\n",
       "parch           0\n",
       "fare            0\n",
       "adult_male      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/series.py:4517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().fillna(\n"
     ]
    }
   ],
   "source": [
    "# fill the age columns missing value with mean \n",
    "X['age'].fillna(X['age'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pclass        0\n",
       "sex           0\n",
       "age           0\n",
       "sibsp         0\n",
       "parch         0\n",
       "fare          0\n",
       "adult_male    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pclass          int64\n",
       "sex            object\n",
       "age           float64\n",
       "sibsp           int64\n",
       "parch           int64\n",
       "fare          float64\n",
       "adult_male       bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = X.select_dtypes(['object', 'category', 'bool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X = pd.get_dummies(X, columns = categories.columns, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 2, test_size = .2)\n",
    "\n",
    "#standardizing data\n",
    "scale = StandardScaler()\n",
    "\n",
    "X_train_scale = pd.DataFrame(scale.fit_transform(X_train), columns = X_train.columns, index = X_train.index)\n",
    "\n",
    "X_test_scale = pd.DataFrame(scale.transform(X_test), columns = X_test.columns, index = X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=23)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#creating an instance of DecisionTree\n",
    "decision_tree = DecisionTreeClassifier(random_state=23)\n",
    "\n",
    "#fitting the model\n",
    "decision_tree.fit(X_train_scale, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    537\n",
       "0    354\n",
       "Name: adult_male_True, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.adult_male_True.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.78991597 0.72995781 0.73417722]\n"
     ]
    }
   ],
   "source": [
    "# test the tree \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#performing cross validation\n",
    "cv_decision_tree = cross_val_score(decision_tree, X_train_scale, y_train, cv = 3)\n",
    "print(cv_decision_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting predictions\n",
    "pred = decision_tree.predict(X_test_scale)\n",
    "pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7988826815642458\n"
     ]
    }
   ],
   "source": [
    "# generate prediction and output metric (use accuracy)\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83       100\n",
      "           1       0.80      0.72      0.76        79\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.80      0.79      0.79       179\n",
      "weighted avg       0.80      0.80      0.80       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how did our tree do? did it perform well?\n",
    "The tree had 80 percent accuracy. There are no signs of overfitting as it actually performed better than the training set. With no pruning being done, we can see that the model was subject to high variance, as the cv scores were a bit varied and the test set in comparison to the three was also varied.\n",
    "\n",
    "## hint: think about baseline model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
