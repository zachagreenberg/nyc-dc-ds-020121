{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 Code Challenge Review\n",
    "\n",
    "Agenda:\n",
    "- Gradient Descent & Cost Function\n",
    "- Logistic Regression\n",
    "- Evaluation Metrics\n",
    "- Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Gradient Descent & Cost Function\n",
    "- What is a cost function? What are we trying to find?\n",
    "- How to use gd to find the lowest point? How does the gradient change as we get closer to the bottom?\n",
    "- What's the role learning rate play? How can learning rate affect your result? \n",
    "\n",
    "\n",
    "<p style='text-align:center;font-size:20px'>$ \\theta_j := \\theta_j - \\alpha * \\frac{\\partial J(\\theta)}{\\partial\\theta_i} $</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A cost function is an algorithm that helps us to determine the best fit line for a regression by minimizing the loss. We are systematically trying to find the line with the lowest amount of errors.\n",
    "\n",
    "> We can use gradient descent to find the lowest point because it takes into consideration the slope or derivative of a cost curve. We are ultimately trying to get the cost curve to descend to as close to 0 as possible. As the gradient gets closer to the bottom it approaches 0.\n",
    "\n",
    "> A learning rate is a constant that we establish to help us determine the appropriate step size as we try to find the minimum loss. Having a learning rate that is too large can make us overshoot and miss the minimum, whereas having a learning rate that is too small can stall our efforts to get to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "x = [1, 1, 2, 3, 4, 3, 4, 6, 4]\n",
    "y = [2, 1, 0.5, 1, 3, 3, 2, 5, 4]\n",
    "plt.figure(figsize = (8,8))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 = 0\n",
    "#beta_1 = [.25, .5, .75, .8, 1,]\n",
    "beta_1 = np.arange(0.25,3,0.1)\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "mses = []\n",
    "for t in beta_1:\n",
    "    line = beta_0 + (np.array(x)*t)\n",
    "    mse = round(mean_squared_error(y, line),3)\n",
    "    mses.append(mse)\n",
    "    ax.plot(x, line, label=f'{mse} {t}')\n",
    "ax.scatter(x,y)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the Cost Curve\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(beta_1, mses)\n",
    "ax.set_title('Cost Curve')\n",
    "ax.set_xlabel('beta 1')\n",
    "ax.set_ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For gradient descent, the questions are going to be mostly intuitive and written answers. You will need to be able to answer questions such as the 3 bullet points above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Logistic Regression \n",
    "- How does linear regression differ from logistic regression?\n",
    "- Why is logistic regression better at modeling a binary outcome?\n",
    "- What are some advantages and disadvantages of logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Linear Regression differs from Logistic Regression because the target variables are of different kinds. The target variable in a Linear Regression is continuous and the target variable in Logistic Regression is categorical. \n",
    "\n",
    "> A Logistic Regression is better at modeling a binary outcome because it utilizes the sigmoid function that creates an S shaped line that is better fit to the data than the line predicted from Linear Regression. \n",
    "\n",
    "> With Logistic Regression it can interpret the coefficients for feature importance by magnitude. A downside of this is that it is tough to interpret complex relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. Evaluation Metrics \n",
    "- What are precision and recall?\n",
    "- How to evaluate a logistic regression model?\n",
    "- What is roc auc curve?\n",
    "- What is class imbalance and how do we deal with it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Precision is the True Predicted Positives over the Total Positives Predicted. This is a good measure to use when you would concerned with the number of False Positives. A good example of this is crime conviction. You don't want to falsely accuse someone of a crime.\n",
    "\n",
    ">Recall is the True Predicted Positives over the Total True Positives. This is a good measure to use when you are concerned about the number of False Negatives. A good example of this is cancer detection. You don't want to tell someone they don't have cancer when they do.\n",
    "\n",
    ">There are other measures that you can use to evaluate Logistic Regression besides Precision and Recall, there is also Accuracy and F1-score. Which one you use is situational depending on the data and what you are trying to accomplish with it.\n",
    "\n",
    "> A ROC curve shows you all of the scenarios of the thresholds for the classification data. The thresholds determine at what point you consider something negative or positive. Again, what you choose is situational depending on your goal, however in general the best model is often the one closest to the top left corner because it maximizes the True Positives and minimizes the False Positives.\n",
    "\n",
    "> A class imbalance is when the classes you are trying to predict are uneven. This can bias your machine because of the lack of data on the unbalances side. An easy way to correct for this is Oversampling the minority class. This will create more data for that side and even out the two classes. Often a better way to do this is with SMOTE. This creates artificial data to balance it all out. This is a better option because replicating the data would lead to possible bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'confusion_matrix.png' width = 300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8076923076923077\n"
     ]
    }
   ],
   "source": [
    "### calculate precision here\n",
    "TP = 63\n",
    "TP_FP = (63 + 15)\n",
    "Precision = TP/TP_FP\n",
    "print(Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7411764705882353\n"
     ]
    }
   ],
   "source": [
    "### calculate recall here\n",
    "TP_FN = (63 + 22)\n",
    "Recall = TP/TP_FN\n",
    "print(Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7730061349693251\n"
     ]
    }
   ],
   "source": [
    "### calculate F1 score here\n",
    "F1 = (2 * Precision * Recall) / (Precision + Recall)\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain which model below has the best performance based on ROC-AUC curve? Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='roc_auc.png' width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model below, it depends on the measure that you are seeking to determine what your best model is. In general, for accuracy, the green model is the best. It is the closest to the top left corner maximizing True Positives and minimizing False Positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n",
    "<img src = 'imbalanced.png' wid = 300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### what problem would it cause? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine would lean towards the majority in an unbalanced dataset. It would have low predicting power for the target=1 result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How to remedy it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many solutions. We can resample by taking smaller samples of the majority class, taking larger samples (ie copies) of the minority class, or we can use SMOTE. This creates new samples, which is good because it can help avoid bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1 - Resampling\n",
    "<img src = 'resampling.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 2 - Smote\n",
    "<img src = 'smote.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 3 - Tomek Link \n",
    "<img src = 'tomek.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. Decision Trees\n",
    "- Build trees with the sklearn machine learning framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import the dataset and set up predictors and target\n",
    "import seaborn as sns\n",
    "titanic = sns.load_dataset('titanic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define x and y \n",
    "y = titanic['survived']\n",
    "X = titanic[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
    "     'adult_male']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived         0\n",
       "pclass           0\n",
       "sex              0\n",
       "age            177\n",
       "sibsp            0\n",
       "parch            0\n",
       "fare             0\n",
       "embarked         2\n",
       "class            0\n",
       "who              0\n",
       "adult_male       0\n",
       "deck           688\n",
       "embark_town      2\n",
       "alive            0\n",
       "alone            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fill the age columns missing value with mean \n",
    "titanic['age'].fillna(titanic['age'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived         0\n",
       "pclass           0\n",
       "sex              0\n",
       "age              0\n",
       "sibsp            0\n",
       "parch            0\n",
       "fare             0\n",
       "embarked         2\n",
       "class            0\n",
       "who              0\n",
       "adult_male       0\n",
       "deck           688\n",
       "embark_town      2\n",
       "alive            0\n",
       "alone            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived          int64\n",
       "pclass            int64\n",
       "sex              object\n",
       "age             float64\n",
       "sibsp             int64\n",
       "parch             int64\n",
       "fare            float64\n",
       "embarked         object\n",
       "class          category\n",
       "who              object\n",
       "adult_male         bool\n",
       "deck           category\n",
       "embark_town      object\n",
       "alive            object\n",
       "alone              bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = titanic.select_dtypes(['object', 'category', 'bool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "titanic = pd.get_dummies(titanic, columns = categories.columns, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [890, 891]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7b628a22682f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2125\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2127\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \"\"\"\n\u001b[1;32m    291\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    256\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [890, 891]"
     ]
    }
   ],
   "source": [
    "y = titanic['survived']\n",
    "X = titanic[1:]\n",
    "\n",
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 2, test_size = .2)\n",
    "\n",
    "scale = StandardScaler()\n",
    "\n",
    "X_train_scale = pd.DataFrame(scale.fit_transform(X_train), columns = X_train.columns, index = X_train.index)\n",
    "\n",
    "X_test_scale = pd.DataFrame(scale.transform(X_test), columns = X_test.columns, index = X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the tree \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate prediction and output metric (use accuracy)\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did our tree do? did it perform well?\n",
    "\n",
    "## hint: think about baseline model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
